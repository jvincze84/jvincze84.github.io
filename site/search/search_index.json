{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome To My Blog \u00b6 This site is the newver version of https://blog.vinczejanos.info . All conent have been migrated to this new site. Migrate From Ghost To MKDocs \u00b6 Maybe a bit strange that I use MKDocs for my Blog, but it was neccessary to replace my old Ghost site. I really liked the Ghost platform when it was at 1.x version, but the new editor is not my taste, so I started to find a new platform. I need a simple and easy to use markdown based platform, so the widely used Joomla, Drupal, Wordpress could not come into the picture. And to be honest I don't need a complete CMS system for this little site. After some hours of researching I decided to use https://www.mkdocs.org to migrate my conetnt into it. I know this is not a typical Blog platform, but after some hours I think it can fulfill all my needs. Old Content \u00b6 All of old conents can be found here under the \" Old Blog Conetent \" menu. But please aware that these conents are relally old, and I keep them only for examples. I think most of the tutorials could not be followed step by step, because a lot of things has been changed since these posts were written.","title":"Welcome To My Blog"},{"location":"#welcome-to-my-blog","text":"This site is the newver version of https://blog.vinczejanos.info . All conent have been migrated to this new site.","title":"Welcome To My Blog"},{"location":"#migrate-from-ghost-to-mkdocs","text":"Maybe a bit strange that I use MKDocs for my Blog, but it was neccessary to replace my old Ghost site. I really liked the Ghost platform when it was at 1.x version, but the new editor is not my taste, so I started to find a new platform. I need a simple and easy to use markdown based platform, so the widely used Joomla, Drupal, Wordpress could not come into the picture. And to be honest I don't need a complete CMS system for this little site. After some hours of researching I decided to use https://www.mkdocs.org to migrate my conetnt into it. I know this is not a typical Blog platform, but after some hours I think it can fulfill all my needs.","title":"Migrate From Ghost To MKDocs"},{"location":"#old-content","text":"All of old conents can be found here under the \" Old Blog Conetent \" menu. But please aware that these conents are relally old, and I keep them only for examples. I think most of the tutorials could not be followed step by step, because a lot of things has been changed since these posts were written.","title":"Old Content"},{"location":"How_to_use_MKdocs/","text":"How to use MKdocs? \u00b6 TL;DR \u00b6 As wrote on the index page, I changed from Ghos Blog to MKDocs. This was necessary because my Ghost version was really outdated and I could not get used to its new editor. I have some essential requirement against the platform I use: Should be easy to use with native MarkDown support Should not have billions of features I won't use ever (to avoid unnecessary system load) Must have integrated very good search engine Look-And-Feel must be easily customised without plugins or addons Since I post a lot of code bloks, code highlighting is must. It should be achieved natively or using prism.js Source code of the contens (*.md) shold be stored in a Git repository. What was the alternatives? \u00b6 CMS Systems Drupal, WordPress, Joomla : All of these are CMS systems, and too robust for my purpose. I needed a light-weight system. Static Site Generators and Jekyll, Gatsby.js, Scully, MKDocs, etc. : They are way more closer to my expectations. And how I chose up MKDocs over the rest? My selection process was really simple. I gave a try all of them, this means I spent 2 hours to try each of them. The winner was with which I could get closer to my expectations in 2 hours. And of course which was closer to my taste in coding, manageability and flexibility. Now It seems MKDocs does exactly what I need. Probably most of the other static website gernerator would have been perfect for me, but after 2 hours of using I found it rellay comfortable for me. And I did not regret my choice. All of my old contents are migrated to this site, and meantime I did some customisation on the the theme and the site. And this is the main topic of this post: How I migrated the content and how I use MKDocs? Prepare MKDocs Docker Image \u00b6 If you blinks at the official MKDocs installtion page you can see that the installation should be done by run python pip install command. I don't like to install various python packages on my computer, because sooner or later I'm going to stuck in failed requirements. So at the first step I had to make decision what to use: python virtualenv or Docker. Of course I chose Docker. I do know that \"Material for MkDocs\" have official Docker image, but I like to use Docker images was built by my own. Every time I build a new Docker image from scratch I learn something or make my knowledge deeper about Dockerfiles, so it's absolutely worth it. Dockerfile & Build \u00b6 FROM python:3-alpine --> Using the official python image. ARG USER=1001 --> Default user id. If you don't specify another when building the container (see below) ENTRYPOINT --> The default command to run when the container starts. Build command docker build -t exmaple-mkdocs:v1 --build-arg=USER=$(id -u) . Important Docker container will be built using your local user id. This will help you to avoid permission deined when mounting local directory inside the container. Usage Examples \u00b6 Get help docker run -it exmaple-mkdocs:v1 --help Create New Site mkdir /tmp/example/ docker run -it -v /tmp/example:/build exmaple-mkdocs:v1 new /build This command will create the following initial files (inside the /tmp/example directory on you local system: . ./docs ./docs/index.md ./mkdocs.yml Info With -v parameter you can mount one of your local directory inside the container (bind mount). So the running process iside the container will see your local direcotry /tmp/example as bild . ( -v /HOST-DIR:/CONTAINER-DIR ). If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host (default location: /var/lib/docker/volumes ). Get into the container docker run -it --entrypoint=/bin/bash exmaple-mkdocs:v1 Run the builtin development server docker run -it -p 8789:8000 \\ -v /tmp/example:/build exmaple-mkdocs:v1 \\ serve --dev-addr 0.0.0.0:8000 --config-file /build/mkdocs.yml This command may need some explanation: docker run -it --> Run the container in interacrive mode and allocate a pseudo-tty. -p 8789:8000 --> Publish container port 8000 on the host port 8789 . This means process binding the port 8000 inside the container will be published on the local port 8789. serve --dev-addr 0.0.0.0:8000 --config-file /build/mkdocs.yml --> Arguments of the mkdocs command. (ENTRYPOINT) The bind port (8000) must be the same as specified at -p parameter. Since we bind mounted the /tmp/example local directory into the container's /build we can acces mkdocs.yml inside the container as /build/mkdocs.yml Now you can access your newly created site at http://localhost:8789 or http://[your machine ip address]:8789. Every modification inside the /tmp/example directory immediately take effetcs, so you don't need to restart the container, your browser will refresh the page automatically. But be aware that if you make systax error in the mkdocs.yml the container will exit and you need to manually retart it. Configure \u00b6 mkdocs.yml \u00b6 Configuring your MKDocs intstance basically means editing mkdocs.yml . You can see my current configuration below: I think there is nothing special in this configuration, but could be a good example. Every part of this file is very well documented on the officail Material and MKDocs website: https://www.mkdocs.org/user-guide/ https://squidfunk.github.io/mkdocs-material/ Info All the items in the nav section is relative to the docs directory. Example: old/Iptables_Examples.md is located at /tmp/example/docs/old/Iptables_Examples.md I think the only thing to metion is my extra.css file. extra.css \u00b6 .md-grid { max-width: initial; } pre[class*=\"language-\"] { max-height: 32em !important; } .md-clipboard { display: none !important; } .md-typeset pre>code { overflow: unset !important; padding: unset !important; } I know using !important is not the best things to do, but I'm not a web developer and I needed a 'quick and dirty solution'. Maybe later, if I have more time I will customise the mkdocs theme and leave !important . Info More about !important : https://stackoverflow.com/questions/9245353/what-does-important-mean-in-css . \"Using !important has its purposes (though I struggle to think of them), but it's much like using a nuclear explosion to stop the foxes killing your chickens; yes, the foxes will be killed, but so will the chickens. And the neighbourhood.\" md-grid (Material Theme) Reference: Content area width The width of the content area is set so the length of each line doesn't exceed 80-100 characters, depending on the width of the characters. While this is a reasonable default, as longer lines tend to be harder to read, it may be desirable to increase the overall width of the content area, or even make it stretch to the entire available space. This can easily be achieved with an additional stylesheet and a few lines of CSS: .md-grid { max-width: initial; } pre[class*=\"language-\"] (prismjs) Add vertical scroll bar when code block cointans more than 32 lines. md-clipboard (Material Theme) This section disables the theme built in \"copy to clipoad\" funcion, it's neccessary if you wish to use the prismjs \"copy to clipboard\" method. md-typeset pre>code (Material Theme) Some functions of prismj won't work properly without this modification, for example line numbering. Example: <pre class=\"line-numbers language-docker\" data-src=\"/files/Dockerfile\"></pre> Screenshot: You can see that the line number from the left hand side of the lines are missing. Build the site \u00b6 If you are done writing your docs, and nav section is properly configured in mkdocs.yml it's time to build your site. I will show three methods to publis the site. Important mkdocs serve is absolutely not suitable for production. It's only purpose to help you developing the site, and watch realtime your modification. Build Site & Own Web Server \u00b6 If you already have a web servers you can simply copy your content to the DocmentRoot. You can simply build your site: docker run -it \\ -v /tmp/example:/build exmaple-mkdocs:v1 \\ build This command will put your static html site into the /tmp/example/site directory on your host machine. Or you can sepcify where to store the generated content: docker run -it \\ -v /tmp/example:/build exmaple-mkdocs:v1 \\ -v /var/www/html/mysite:/site \\ build --site-dir /site Build Site & Nginx with Docer \u00b6 This method almost the same as the previous one, except that here we are using another Docker container to server our page. First build your site with buld command. Server your page with Nginx First start your container in the foreground to check if everything is fine: docker run -it --rm \\ --name mkdocs \\ -v [PATH TO YOUR SITE DIR]:/usr/share/nginx/html:ro \\ -p 8087:80 \\ nginx:latest If you can access your site on the host port 8087, you should stop the container (ctrl+c) and start again in detached mode: docker run -d \\ --restart alaway \\ --name mkdocs \\ -v [PATH TO YOUR SITE DIR]:/usr/share/nginx/html:ro \\ -p 8087:80 \\ nginx:latest More abaout Nginx container image: https://hub.docker.com/_/nginx GitHub Pages \u00b6 This is the method I use. I'm not borering with own web server instead I use github pages: https://pages.github.com For this you need a free Github registration. 1. Create a repository The name of the repository must be [your username] .github.io and must be public. In my case: 2. Push Github does not support user/pass uath anymore, so you need to create an auth token. Go to settings : Developer settings : And Personal access tokens : Finally click on the Generate new token , select the permissions you need and generate the token. Push your mkdocs root dir: ls -al total 40 drwxr-xr-x 7 root root 4096 Oct 9 14:34 . drwx------ 22 root root 4096 Oct 9 14:33 .. drwxr-xr-x 6 root root 4096 Oct 9 14:33 cinder -rw-r--r-- 1 root root 162 Oct 9 14:33 docker.cmd drwxr-xr-x 8 root root 4096 Oct 9 14:33 docs drwxr-xr-x 8 root root 4096 Oct 9 14:35 .git -rw-r--r-- 1 root root 4471 Oct 9 14:33 mkdocs.yml drwxr-xr-x 2 root root 4096 Oct 9 14:33 overrides drwxr-xr-x 11 root root 4096 Oct 9 14:33 site git init git add --all git commit -m 'Initial release' git remote add origin https://github.com/jvincze84/test-delete.git git push -u origin master 3. Push gh-pages clear docker run -it -v /root/test-delete/:/usr/src/mkdocs/build example-mkdocs:v2 gh-deploy INFO - Cleaning site directory INFO - Building documentation to directory: /usr/src/mkdocs/build/site INFO - Documentation built in 1.04 seconds WARNING - Version check skipped: No version specified in previous deployment. INFO - Copying '/usr/src/mkdocs/build/site' to 'gh-pages' branch and pushing to GitHub. Username for 'https://github.com': jvincze84 Password for 'https://jvincze84@github.com': Enumerating objects: 219, done. Counting objects: 100% (219/219), done. Delta compression using up to 8 threads Compressing objects: 100% (109/109), done. Writing objects: 100% (219/219), 14.91 MiB | 9.50 MiB/s, done. Total 219 (delta 72), reused 219 (delta 72), pack-reused 0 remote: Resolving deltas: 100% (72/72), done. remote: remote: Create a pull request for 'gh-pages' on GitHub by visiting: remote: https://github.com/jvincze84/test-delete/pull/new/gh-pages remote: To https://github.com/jvincze84/test-delete.git * [new branch] gh-pages -> gh-pages INFO - Based on your CNAME file, your documentation should be available shortly at: http://readthedocs.vinczejanos.info INFO - NOTE: Your DNS records must be configured appropriately for your CNAME URL to work. We are almost done. Go back to Github, and set up the newly created \"gh-pages\" branch for pages: 4. Custom Domain Github publishes your content to \"https://[username].github.io\" (example: jvincze84.github.io). If you want to use your custom domain, put a file into the docs folder with name CNAME : cat docs/CNAME readthedocs.vinczejanos.info But first you need to create a CNAME DNS record which points to \"[username].github.io\". Godaddy example: Warning Do not modify your custom domain directly on github.com; mkdocs gh-deploy will overwrite your config. Update The Site \u00b6 If you want to update a page or add new pages you can follow these steps: Clone your repository git clone https://github.com/jvincze84/jvincze84.github.io Cloning into 'jvincze84.github.io'... remote: Enumerating objects: 850, done. remote: Counting objects: 100% (850/850), done. remote: Compressing objects: 100% (292/292), done. remote: Total 850 (delta 351), reused 820 (delta 327), pack-reused 0 Receiving objects: 100% (850/850), 16.09 MiB | 10.59 MiB/s, done. Resolving deltas: 100% (351/351), done. Info If you want to save your git credetials run this command: git config --global credential.helper store After you have done the neccessary modification (add/change page) push your changes: Warning If you add a new page (.md) don't forget to add it to the nav section in mkdocs.yml push to git git add docs/How_to_use_MKdocs.md mkdocs.yml git commit -m 'Add new page : docs/How_to_use_MKdocs.md' master 0949efa] Add new page : docs/How_to_use_MKdocs.md 1 file changed, 25 insertions(+), 9 deletions(-) git push Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 4 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 993 bytes | 993.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To https://github.com/jvincze84/jvincze84.github.io a9585e6..0949efa master -> master gh-deploy Finally apply the modification on Github Pages docker run -it --rm \\ -v /home/mkdocs/Documents/mkdocs/jvincze84.github.io/:/usr/src/mkdocs/build \\ mkdocs:1 gh-deploy","title":"How To Use MKDocs with Docker?"},{"location":"How_to_use_MKdocs/#how-to-use-mkdocs","text":"","title":"How to use MKdocs?"},{"location":"How_to_use_MKdocs/#tldr","text":"As wrote on the index page, I changed from Ghos Blog to MKDocs. This was necessary because my Ghost version was really outdated and I could not get used to its new editor. I have some essential requirement against the platform I use: Should be easy to use with native MarkDown support Should not have billions of features I won't use ever (to avoid unnecessary system load) Must have integrated very good search engine Look-And-Feel must be easily customised without plugins or addons Since I post a lot of code bloks, code highlighting is must. It should be achieved natively or using prism.js Source code of the contens (*.md) shold be stored in a Git repository.","title":"TL;DR"},{"location":"How_to_use_MKdocs/#what-was-the-alternatives","text":"CMS Systems Drupal, WordPress, Joomla : All of these are CMS systems, and too robust for my purpose. I needed a light-weight system. Static Site Generators and Jekyll, Gatsby.js, Scully, MKDocs, etc. : They are way more closer to my expectations. And how I chose up MKDocs over the rest? My selection process was really simple. I gave a try all of them, this means I spent 2 hours to try each of them. The winner was with which I could get closer to my expectations in 2 hours. And of course which was closer to my taste in coding, manageability and flexibility. Now It seems MKDocs does exactly what I need. Probably most of the other static website gernerator would have been perfect for me, but after 2 hours of using I found it rellay comfortable for me. And I did not regret my choice. All of my old contents are migrated to this site, and meantime I did some customisation on the the theme and the site. And this is the main topic of this post: How I migrated the content and how I use MKDocs?","title":"What was the alternatives?"},{"location":"How_to_use_MKdocs/#prepare-mkdocs-docker-image","text":"If you blinks at the official MKDocs installtion page you can see that the installation should be done by run python pip install command. I don't like to install various python packages on my computer, because sooner or later I'm going to stuck in failed requirements. So at the first step I had to make decision what to use: python virtualenv or Docker. Of course I chose Docker. I do know that \"Material for MkDocs\" have official Docker image, but I like to use Docker images was built by my own. Every time I build a new Docker image from scratch I learn something or make my knowledge deeper about Dockerfiles, so it's absolutely worth it.","title":"Prepare MKDocs Docker Image"},{"location":"How_to_use_MKdocs/#dockerfile-build","text":"FROM python:3-alpine --> Using the official python image. ARG USER=1001 --> Default user id. If you don't specify another when building the container (see below) ENTRYPOINT --> The default command to run when the container starts. Build command docker build -t exmaple-mkdocs:v1 --build-arg=USER=$(id -u) . Important Docker container will be built using your local user id. This will help you to avoid permission deined when mounting local directory inside the container.","title":"Dockerfile &amp; Build"},{"location":"How_to_use_MKdocs/#usage-examples","text":"Get help docker run -it exmaple-mkdocs:v1 --help Create New Site mkdir /tmp/example/ docker run -it -v /tmp/example:/build exmaple-mkdocs:v1 new /build This command will create the following initial files (inside the /tmp/example directory on you local system: . ./docs ./docs/index.md ./mkdocs.yml Info With -v parameter you can mount one of your local directory inside the container (bind mount). So the running process iside the container will see your local direcotry /tmp/example as bild . ( -v /HOST-DIR:/CONTAINER-DIR ). If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host (default location: /var/lib/docker/volumes ). Get into the container docker run -it --entrypoint=/bin/bash exmaple-mkdocs:v1 Run the builtin development server docker run -it -p 8789:8000 \\ -v /tmp/example:/build exmaple-mkdocs:v1 \\ serve --dev-addr 0.0.0.0:8000 --config-file /build/mkdocs.yml This command may need some explanation: docker run -it --> Run the container in interacrive mode and allocate a pseudo-tty. -p 8789:8000 --> Publish container port 8000 on the host port 8789 . This means process binding the port 8000 inside the container will be published on the local port 8789. serve --dev-addr 0.0.0.0:8000 --config-file /build/mkdocs.yml --> Arguments of the mkdocs command. (ENTRYPOINT) The bind port (8000) must be the same as specified at -p parameter. Since we bind mounted the /tmp/example local directory into the container's /build we can acces mkdocs.yml inside the container as /build/mkdocs.yml Now you can access your newly created site at http://localhost:8789 or http://[your machine ip address]:8789. Every modification inside the /tmp/example directory immediately take effetcs, so you don't need to restart the container, your browser will refresh the page automatically. But be aware that if you make systax error in the mkdocs.yml the container will exit and you need to manually retart it.","title":"Usage Examples"},{"location":"How_to_use_MKdocs/#configure","text":"","title":"Configure"},{"location":"How_to_use_MKdocs/#mkdocsyml","text":"Configuring your MKDocs intstance basically means editing mkdocs.yml . You can see my current configuration below: I think there is nothing special in this configuration, but could be a good example. Every part of this file is very well documented on the officail Material and MKDocs website: https://www.mkdocs.org/user-guide/ https://squidfunk.github.io/mkdocs-material/ Info All the items in the nav section is relative to the docs directory. Example: old/Iptables_Examples.md is located at /tmp/example/docs/old/Iptables_Examples.md I think the only thing to metion is my extra.css file.","title":"mkdocs.yml"},{"location":"How_to_use_MKdocs/#extracss","text":".md-grid { max-width: initial; } pre[class*=\"language-\"] { max-height: 32em !important; } .md-clipboard { display: none !important; } .md-typeset pre>code { overflow: unset !important; padding: unset !important; } I know using !important is not the best things to do, but I'm not a web developer and I needed a 'quick and dirty solution'. Maybe later, if I have more time I will customise the mkdocs theme and leave !important . Info More about !important : https://stackoverflow.com/questions/9245353/what-does-important-mean-in-css . \"Using !important has its purposes (though I struggle to think of them), but it's much like using a nuclear explosion to stop the foxes killing your chickens; yes, the foxes will be killed, but so will the chickens. And the neighbourhood.\" md-grid (Material Theme) Reference: Content area width The width of the content area is set so the length of each line doesn't exceed 80-100 characters, depending on the width of the characters. While this is a reasonable default, as longer lines tend to be harder to read, it may be desirable to increase the overall width of the content area, or even make it stretch to the entire available space. This can easily be achieved with an additional stylesheet and a few lines of CSS: .md-grid { max-width: initial; } pre[class*=\"language-\"] (prismjs) Add vertical scroll bar when code block cointans more than 32 lines. md-clipboard (Material Theme) This section disables the theme built in \"copy to clipoad\" funcion, it's neccessary if you wish to use the prismjs \"copy to clipboard\" method. md-typeset pre>code (Material Theme) Some functions of prismj won't work properly without this modification, for example line numbering. Example: <pre class=\"line-numbers language-docker\" data-src=\"/files/Dockerfile\"></pre> Screenshot: You can see that the line number from the left hand side of the lines are missing.","title":"extra.css"},{"location":"How_to_use_MKdocs/#build-the-site","text":"If you are done writing your docs, and nav section is properly configured in mkdocs.yml it's time to build your site. I will show three methods to publis the site. Important mkdocs serve is absolutely not suitable for production. It's only purpose to help you developing the site, and watch realtime your modification.","title":"Build the site"},{"location":"How_to_use_MKdocs/#build-site-own-web-server","text":"If you already have a web servers you can simply copy your content to the DocmentRoot. You can simply build your site: docker run -it \\ -v /tmp/example:/build exmaple-mkdocs:v1 \\ build This command will put your static html site into the /tmp/example/site directory on your host machine. Or you can sepcify where to store the generated content: docker run -it \\ -v /tmp/example:/build exmaple-mkdocs:v1 \\ -v /var/www/html/mysite:/site \\ build --site-dir /site","title":"Build Site &amp; Own Web Server"},{"location":"How_to_use_MKdocs/#build-site-nginx-with-docer","text":"This method almost the same as the previous one, except that here we are using another Docker container to server our page. First build your site with buld command. Server your page with Nginx First start your container in the foreground to check if everything is fine: docker run -it --rm \\ --name mkdocs \\ -v [PATH TO YOUR SITE DIR]:/usr/share/nginx/html:ro \\ -p 8087:80 \\ nginx:latest If you can access your site on the host port 8087, you should stop the container (ctrl+c) and start again in detached mode: docker run -d \\ --restart alaway \\ --name mkdocs \\ -v [PATH TO YOUR SITE DIR]:/usr/share/nginx/html:ro \\ -p 8087:80 \\ nginx:latest More abaout Nginx container image: https://hub.docker.com/_/nginx","title":"Build Site &amp; Nginx with Docer"},{"location":"How_to_use_MKdocs/#github-pages","text":"This is the method I use. I'm not borering with own web server instead I use github pages: https://pages.github.com For this you need a free Github registration. 1. Create a repository The name of the repository must be [your username] .github.io and must be public. In my case: 2. Push Github does not support user/pass uath anymore, so you need to create an auth token. Go to settings : Developer settings : And Personal access tokens : Finally click on the Generate new token , select the permissions you need and generate the token. Push your mkdocs root dir: ls -al total 40 drwxr-xr-x 7 root root 4096 Oct 9 14:34 . drwx------ 22 root root 4096 Oct 9 14:33 .. drwxr-xr-x 6 root root 4096 Oct 9 14:33 cinder -rw-r--r-- 1 root root 162 Oct 9 14:33 docker.cmd drwxr-xr-x 8 root root 4096 Oct 9 14:33 docs drwxr-xr-x 8 root root 4096 Oct 9 14:35 .git -rw-r--r-- 1 root root 4471 Oct 9 14:33 mkdocs.yml drwxr-xr-x 2 root root 4096 Oct 9 14:33 overrides drwxr-xr-x 11 root root 4096 Oct 9 14:33 site git init git add --all git commit -m 'Initial release' git remote add origin https://github.com/jvincze84/test-delete.git git push -u origin master 3. Push gh-pages clear docker run -it -v /root/test-delete/:/usr/src/mkdocs/build example-mkdocs:v2 gh-deploy INFO - Cleaning site directory INFO - Building documentation to directory: /usr/src/mkdocs/build/site INFO - Documentation built in 1.04 seconds WARNING - Version check skipped: No version specified in previous deployment. INFO - Copying '/usr/src/mkdocs/build/site' to 'gh-pages' branch and pushing to GitHub. Username for 'https://github.com': jvincze84 Password for 'https://jvincze84@github.com': Enumerating objects: 219, done. Counting objects: 100% (219/219), done. Delta compression using up to 8 threads Compressing objects: 100% (109/109), done. Writing objects: 100% (219/219), 14.91 MiB | 9.50 MiB/s, done. Total 219 (delta 72), reused 219 (delta 72), pack-reused 0 remote: Resolving deltas: 100% (72/72), done. remote: remote: Create a pull request for 'gh-pages' on GitHub by visiting: remote: https://github.com/jvincze84/test-delete/pull/new/gh-pages remote: To https://github.com/jvincze84/test-delete.git * [new branch] gh-pages -> gh-pages INFO - Based on your CNAME file, your documentation should be available shortly at: http://readthedocs.vinczejanos.info INFO - NOTE: Your DNS records must be configured appropriately for your CNAME URL to work. We are almost done. Go back to Github, and set up the newly created \"gh-pages\" branch for pages: 4. Custom Domain Github publishes your content to \"https://[username].github.io\" (example: jvincze84.github.io). If you want to use your custom domain, put a file into the docs folder with name CNAME : cat docs/CNAME readthedocs.vinczejanos.info But first you need to create a CNAME DNS record which points to \"[username].github.io\". Godaddy example: Warning Do not modify your custom domain directly on github.com; mkdocs gh-deploy will overwrite your config.","title":"GitHub Pages"},{"location":"How_to_use_MKdocs/#update-the-site","text":"If you want to update a page or add new pages you can follow these steps: Clone your repository git clone https://github.com/jvincze84/jvincze84.github.io Cloning into 'jvincze84.github.io'... remote: Enumerating objects: 850, done. remote: Counting objects: 100% (850/850), done. remote: Compressing objects: 100% (292/292), done. remote: Total 850 (delta 351), reused 820 (delta 327), pack-reused 0 Receiving objects: 100% (850/850), 16.09 MiB | 10.59 MiB/s, done. Resolving deltas: 100% (351/351), done. Info If you want to save your git credetials run this command: git config --global credential.helper store After you have done the neccessary modification (add/change page) push your changes: Warning If you add a new page (.md) don't forget to add it to the nav section in mkdocs.yml push to git git add docs/How_to_use_MKdocs.md mkdocs.yml git commit -m 'Add new page : docs/How_to_use_MKdocs.md' master 0949efa] Add new page : docs/How_to_use_MKdocs.md 1 file changed, 25 insertions(+), 9 deletions(-) git push Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 4 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 993 bytes | 993.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0) remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To https://github.com/jvincze84/jvincze84.github.io a9585e6..0949efa master -> master gh-deploy Finally apply the modification on Github Pages docker run -it --rm \\ -v /home/mkdocs/Documents/mkdocs/jvincze84.github.io/:/usr/src/mkdocs/build \\ mkdocs:1 gh-deploy","title":"Update The Site"},{"location":"Tips_And_Tricks/","text":"Tips And Tricks \u00b6 In this page I will share you some random tips and tricks I use in my daily life, and I will update this post frequently. Some of them will be commented, but not all. Generate random strings \u00b6 tr -cd '[:alnum:]' < /dev/./urandom | fold -w12 | head -n4 You can use these string for example as random generated passwords. Run simple python http server \u00b6 `nohup python -m SimpleHTTPServer 8888 >>../access.log &` This will create a very simple http server on port 8888. Useful for shearing files quickly and easily over http protocol. Poor man's VPN with sshuttle \u00b6 sshuttle -r [USER]@[HOSTNAME] 0.0.0.0/0 --dns -v Split MP3 into equal time length slices \u00b6 Install required package: apt-get install poc-streamer Split into 3mins slices: mp3splt -t 3.00 [MP3 FILE] -o @n Output will be: 01.mp3, 02.mp3, 04.mp3 . . . . Convert Video to 1280X (lower quality) \u00b6 It us useful when you want to convert your video files to lower quality. Command: avconv -y -i $MOVIE -vf \"scale=1280:trunc(ow/a/2)*2\" -vcodec libx264 -acodec libmp3lame $NEWNAME Where: -y overwrite output files -i input files -vf scale=1280:trunc(ow/a/2)*2\" --> Keep original ratio, and avoid \"height not divisible by 2\" error message. -vcodec libx264 --> video codec -acodec libmp3lame --> audio codec You can replace \"1280\" to any other values which is divisible by 2. (640,480,etc...) LUA - delay function \u00b6 Sometimes you have to use delay before the next function, and in this situation can be useful this little function. function sleepAndContinueWithCommand(command) print(\"sleepAndContinueWithCommand Function\") tmr.alarm(6,2000,tmr.ALARM_SINGLE, function() command() end) How to call it? sleepAndContinueWithCommand(FUNCTION_NAME) Modify Post Width of Ghost Blog \u00b6 Change max-width in this section /* Every post, on every page, gets this style on its <article> tag */ .post { position: relative; width: 80%; max-width: 1000px; margin: 4rem auto; padding-bottom: 4rem; border-bottom: #EBF2F6 1px solid; word-wrap: break-word; } Limit OprengePI CPU cores (enable/disable cores) \u00b6 echo 1 >/sys/devices/system/cpu/cpu0/online echo 0 >/sys/devices/system/cpu/cpu1/online echo 0 >/sys/devices/system/cpu/cpu2/online echo 0 >/sys/devices/system/cpu/cpu3/online Check CPU temperature: /sys/devices/virtual/thermal/thermal_zone0/temp tar from/to remote machine \u00b6 tar from remote machine: ssh root@172.16.0.240 \"tar cfz - /etc /opt\" >output.tar.gz tar to remote machine: tar cvf - *.sh | ssh vinyo@172.16.0.240 \"cat >~/test.tar.gz\" untar from remote machine ssh vinyo@172.16.0.240 \"cat ~/test.tar.gz\" | tar xv #OR ssh vinyo@172.16.0.240 \"cat ~/test.tar.gz\" | tar xvf - untar to remote machine cat test.tar.gz | ssh 172.16.0.250 \"cd /home/vinyo/temp ; tar xv\" #OR cat test.tar.gz | ssh 172.16.0.250 \"cd /home/vinyo/temp ; tar xvf -\" Raspberry - prevent to sleep Wifi \u00b6 Find your Wifi chip module: ls -la /sys/module/ Mine is: /sys/module/8189es/ Check power management status: cat /sys/module/8189es/parameters/rtw_power_mgnt If it is equal to 0 then you are OK. If not, create a file something like this: cat /etc/modprobe.d/8189es.conf options 8189es rtw_power_mgnt=0 rtw_enusbss=0 Restart. Rename OpenVZ container \u00b6 vzctl set [CTID] --hostname [NEW HOSTNAME] --save Unzip Each .zip File To Separate Directory \u00b6 You have to be in the directory which contains the .zip files. for Z in $( find . -type f -name '*.zip') ; do B=$( basename $Z ) ; F=${B%.*} ; mkdir $F ; unzip $Z -d $F ; done Read Parameter File In Linux Shell (bash) \u00b6 while IFS='=' read -r key value do echo \"... $key='$value'\" eval \"$key='$value'\" done < $PARAM_FILE Where: $PARAM_FILE --> File which contains the paramter and values. IFS='=' Internal Field Separator. Parameter names and Values are separated by = sign. Example: appname=weblogic eval \"$key='$value'\" --> This will create system environment. Some Linux Console Fun \u00b6 Try them out, if you are brave enough. :) apt-get moo apt-get install sl apt-get install furtune apt-get install cowsay apt-get install figlet Examples: sl fortune | cowsay figlet \"Hello\" Run command without X display \u00b6 This method can be useful when you want to run a command (which needs X11 display), but X11 display isn't running on your system. Typical error message: failed to commit changes to dconf: Cannot autolaunch D-Bus without X11 $DISPLAY The solution is: xvfb-run xvfb-run - run specified X client or command in a virtual X server environment Install: apt-cache search xvfb-run xvfb - Virtual Framebuffer 'fake' X server apt-get install xvfb How to use it? xvfb-run --server-args=\"-screen 0, 1024x768x24\" /usr/bin/ssconvert --export-type=Gnumeric_Excel:excel_dsf 20161105_090001.html 20161105_090001.xls Or from a shell script: xvfb-run --server-args=\"-screen 0, 1024x768x24\" /usr/bin/wkhtmltopdf $* There are 2 typical command I'm using with xvfb-run: ssconvert and wkhtmltopdf Simple Image Viewer For Linux \u00b6 feh -- image viewer and cataloguer Failed To Install Cisco AnyConnect on Xubuntu 16.04 \u00b6 Error message: Failed to start vpnagentd.service: Unit vpnagentd.service not found. Solution: apt install network-manager-openconnect systemctl daemon-reload Then restart the install. REFERENCE: https://technicalsanctuary.wordpress.com/2016/05/28/installing-cisco-anyconnect-vpn-on-ubuntu-16-04/ Configure Extra Mouse Buttons Under Linux \u00b6 This solution is tested on Linux Mint (Sarah) and Xubuntu 16.04. So I have a Logitech M505 mouse and I love using the vertical scroll button to minimize and maximize the active window. I have never used these buttons according to its original function, they were always configured to minimize and maximize Window. What you need to install? \u00b6 xbindkeys xvkbd xdotool wmctrl Install them with one command: sudo apt install xbindkeys xvkbd xdotool wmctrl Create sample configuration file \u00b6 xbindkeys -d > ~/.xbindkeysrc This will create a sample configuration file in your home directory. Please remove the \"Examples of commands:\" section from this file to avoid furtherer conflicts. \"xbindkeys_show\" control+shift + q Determine the ID of the buttons you want to use \u00b6 It is very simple. Just run xev command. For example my left button code (button 1): ButtonPress event, serial 37, synthetic NO, window 0x4800001, root 0xc5, subw 0x0, time 7138218, (103,85), root:(965,1564), state 0x10, button 1, same_screen YES ButtonRelease event, serial 37, synthetic NO, window 0x4800001, root 0xc5, subw 0x0, time 7138264, (103,85), root:(965,1564), state 0x110, button 1, same_screen YES Example with grep to easier determine button ID: xev | egrep -o 'button [0-9]{1,2}' Configure .xbindkeysrc \u00b6 So in my case I want to configure only two buttons: Left Scrolling for minimize window Right Scrolling for maximize window I had to add only these two section to .xbindkeysrc: Minimize (Button 11) \"xdotool getactivewindow windowminimize\" b:11 Maximize (button 12) \"wmctrl -r :ACTIVE: -b toggle,maximized_vert,maximized_horz\" b:12 As you can see we had to use two different command: xdotool and wmctrl . If you want to use your buttons for any other activity I'm pretty sure that after some googleing you will find your solution. Case Insensitive Search In Oracle DB \u00b6 alter session set NLS_COMP=ANSI; alter session set NLS_SORT=BINARY_CI; REFERENCE: http://stackoverflow.com/questions/1031844/oracle-db-how-can-i-write-query-ignoring-case Change Date & Timestamp Format In Oracle DB \u00b6 alter session set NLS_DATE_FORMAT='yyyy-mm-dd HH24:mi:ss'; ALTER SESSION SET NLS_TIMESTAMP_FORMAT='yyyy-mm-dd HH24:mi:ss'; Redirect all output (stderr, stdout) to a file \u00b6 #!/bin/bash LOG=\"[LOG file location]\" exec >> $LOG 2>&1 ... ... ... Redirect nohup output \u00b6 You can redirect stdout and stderr to differrent files or into the same file. Examples: nohup ./program >stdout.log 2>stderr.log nohup ./progrem >stoutAndStderr.log 2>&1 abbreviated syntax nohup command > output-$(date +%Y%m%d_%H%M%S).log & Example startup script for OpenHAB: cat start-daemon.sh #!/bin/sh ... ... ... echo Launching the openHAB runtime... nohup java \\ -Dosgi.clean=true \\ -Declipse.ignoreApp=true \\ -Dosgi.noShutdown=true \\ -Djetty.port=$HTTP_PORT \\ -Djetty.port.ssl=$HTTPS_PORT \\ -Djetty.home=. \\ -Dlogback.configurationFile=configurations/logback.xml \\ -Dfelix.fileinstall.dir=addons -Dfelix.fileinstall.filter=.*\\\\.jar \\ -Djava.library.path=lib \\ -Djava.security.auth.login.config=/opt/openhab/runtime/distribution-1.8.3-runtime/etc/login.conf \\ -Dorg.quartz.properties=./etc/quartz.properties \\ -Dequinox.ds.block_timeout=240000 \\ -Dequinox.scr.waitTimeOnBlock=60000 \\ -Dfelix.fileinstall.active.level=4 \\ -Djava.awt.headless=true \\ -jar $cp $* \\ -console 9898 >nohup-$(date +%Y%m%d_%H%M%S).out &","title":"Tips And Tricks"},{"location":"Tips_And_Tricks/#tips-and-tricks","text":"In this page I will share you some random tips and tricks I use in my daily life, and I will update this post frequently. Some of them will be commented, but not all.","title":"Tips And Tricks"},{"location":"Tips_And_Tricks/#generate-random-strings","text":"tr -cd '[:alnum:]' < /dev/./urandom | fold -w12 | head -n4 You can use these string for example as random generated passwords.","title":"Generate random strings"},{"location":"Tips_And_Tricks/#run-simple-python-http-server","text":"`nohup python -m SimpleHTTPServer 8888 >>../access.log &` This will create a very simple http server on port 8888. Useful for shearing files quickly and easily over http protocol.","title":"Run simple python http server"},{"location":"Tips_And_Tricks/#poor-mans-vpn-with-sshuttle","text":"sshuttle -r [USER]@[HOSTNAME] 0.0.0.0/0 --dns -v","title":"Poor man's VPN with sshuttle"},{"location":"Tips_And_Tricks/#split-mp3-into-equal-time-length-slices","text":"Install required package: apt-get install poc-streamer Split into 3mins slices: mp3splt -t 3.00 [MP3 FILE] -o @n Output will be: 01.mp3, 02.mp3, 04.mp3 . . . .","title":"Split MP3 into equal time length slices"},{"location":"Tips_And_Tricks/#convert-video-to-1280x-lower-quality","text":"It us useful when you want to convert your video files to lower quality. Command: avconv -y -i $MOVIE -vf \"scale=1280:trunc(ow/a/2)*2\" -vcodec libx264 -acodec libmp3lame $NEWNAME Where: -y overwrite output files -i input files -vf scale=1280:trunc(ow/a/2)*2\" --> Keep original ratio, and avoid \"height not divisible by 2\" error message. -vcodec libx264 --> video codec -acodec libmp3lame --> audio codec You can replace \"1280\" to any other values which is divisible by 2. (640,480,etc...)","title":"Convert  Video to 1280X (lower quality)"},{"location":"Tips_And_Tricks/#lua-delay-function","text":"Sometimes you have to use delay before the next function, and in this situation can be useful this little function. function sleepAndContinueWithCommand(command) print(\"sleepAndContinueWithCommand Function\") tmr.alarm(6,2000,tmr.ALARM_SINGLE, function() command() end) How to call it? sleepAndContinueWithCommand(FUNCTION_NAME)","title":"LUA - delay function"},{"location":"Tips_And_Tricks/#modify-post-width-of-ghost-blog","text":"Change max-width in this section /* Every post, on every page, gets this style on its <article> tag */ .post { position: relative; width: 80%; max-width: 1000px; margin: 4rem auto; padding-bottom: 4rem; border-bottom: #EBF2F6 1px solid; word-wrap: break-word; }","title":"Modify Post Width of Ghost Blog"},{"location":"Tips_And_Tricks/#limit-oprengepi-cpu-cores-enabledisable-cores","text":"echo 1 >/sys/devices/system/cpu/cpu0/online echo 0 >/sys/devices/system/cpu/cpu1/online echo 0 >/sys/devices/system/cpu/cpu2/online echo 0 >/sys/devices/system/cpu/cpu3/online Check CPU temperature: /sys/devices/virtual/thermal/thermal_zone0/temp","title":"Limit OprengePI CPU cores (enable/disable cores)"},{"location":"Tips_And_Tricks/#tar-fromto-remote-machine","text":"tar from remote machine: ssh root@172.16.0.240 \"tar cfz - /etc /opt\" >output.tar.gz tar to remote machine: tar cvf - *.sh | ssh vinyo@172.16.0.240 \"cat >~/test.tar.gz\" untar from remote machine ssh vinyo@172.16.0.240 \"cat ~/test.tar.gz\" | tar xv #OR ssh vinyo@172.16.0.240 \"cat ~/test.tar.gz\" | tar xvf - untar to remote machine cat test.tar.gz | ssh 172.16.0.250 \"cd /home/vinyo/temp ; tar xv\" #OR cat test.tar.gz | ssh 172.16.0.250 \"cd /home/vinyo/temp ; tar xvf -\"","title":"tar from/to remote machine"},{"location":"Tips_And_Tricks/#raspberry-prevent-to-sleep-wifi","text":"Find your Wifi chip module: ls -la /sys/module/ Mine is: /sys/module/8189es/ Check power management status: cat /sys/module/8189es/parameters/rtw_power_mgnt If it is equal to 0 then you are OK. If not, create a file something like this: cat /etc/modprobe.d/8189es.conf options 8189es rtw_power_mgnt=0 rtw_enusbss=0 Restart.","title":"Raspberry - prevent to sleep Wifi"},{"location":"Tips_And_Tricks/#rename-openvz-container","text":"vzctl set [CTID] --hostname [NEW HOSTNAME] --save","title":"Rename OpenVZ container"},{"location":"Tips_And_Tricks/#unzip-each-zip-file-to-separate-directory","text":"You have to be in the directory which contains the .zip files. for Z in $( find . -type f -name '*.zip') ; do B=$( basename $Z ) ; F=${B%.*} ; mkdir $F ; unzip $Z -d $F ; done","title":"Unzip Each .zip File To Separate Directory"},{"location":"Tips_And_Tricks/#read-parameter-file-in-linux-shell-bash","text":"while IFS='=' read -r key value do echo \"... $key='$value'\" eval \"$key='$value'\" done < $PARAM_FILE Where: $PARAM_FILE --> File which contains the paramter and values. IFS='=' Internal Field Separator. Parameter names and Values are separated by = sign. Example: appname=weblogic eval \"$key='$value'\" --> This will create system environment.","title":"Read Parameter File In Linux Shell (bash)"},{"location":"Tips_And_Tricks/#some-linux-console-fun","text":"Try them out, if you are brave enough. :) apt-get moo apt-get install sl apt-get install furtune apt-get install cowsay apt-get install figlet Examples: sl fortune | cowsay figlet \"Hello\"","title":"Some Linux Console Fun"},{"location":"Tips_And_Tricks/#run-command-without-x-display","text":"This method can be useful when you want to run a command (which needs X11 display), but X11 display isn't running on your system. Typical error message: failed to commit changes to dconf: Cannot autolaunch D-Bus without X11 $DISPLAY The solution is: xvfb-run xvfb-run - run specified X client or command in a virtual X server environment Install: apt-cache search xvfb-run xvfb - Virtual Framebuffer 'fake' X server apt-get install xvfb How to use it? xvfb-run --server-args=\"-screen 0, 1024x768x24\" /usr/bin/ssconvert --export-type=Gnumeric_Excel:excel_dsf 20161105_090001.html 20161105_090001.xls Or from a shell script: xvfb-run --server-args=\"-screen 0, 1024x768x24\" /usr/bin/wkhtmltopdf $* There are 2 typical command I'm using with xvfb-run: ssconvert and wkhtmltopdf","title":"Run command without X display"},{"location":"Tips_And_Tricks/#simple-image-viewer-for-linux","text":"feh -- image viewer and cataloguer","title":"Simple Image Viewer For Linux"},{"location":"Tips_And_Tricks/#failed-to-install-cisco-anyconnect-on-xubuntu-1604","text":"Error message: Failed to start vpnagentd.service: Unit vpnagentd.service not found. Solution: apt install network-manager-openconnect systemctl daemon-reload Then restart the install. REFERENCE: https://technicalsanctuary.wordpress.com/2016/05/28/installing-cisco-anyconnect-vpn-on-ubuntu-16-04/","title":"Failed To Install Cisco AnyConnect on Xubuntu 16.04"},{"location":"Tips_And_Tricks/#configure-extra-mouse-buttons-under-linux","text":"This solution is tested on Linux Mint (Sarah) and Xubuntu 16.04. So I have a Logitech M505 mouse and I love using the vertical scroll button to minimize and maximize the active window. I have never used these buttons according to its original function, they were always configured to minimize and maximize Window.","title":"Configure Extra Mouse Buttons Under Linux"},{"location":"Tips_And_Tricks/#what-you-need-to-install","text":"xbindkeys xvkbd xdotool wmctrl Install them with one command: sudo apt install xbindkeys xvkbd xdotool wmctrl","title":"What you need to install?"},{"location":"Tips_And_Tricks/#create-sample-configuration-file","text":"xbindkeys -d > ~/.xbindkeysrc This will create a sample configuration file in your home directory. Please remove the \"Examples of commands:\" section from this file to avoid furtherer conflicts. \"xbindkeys_show\" control+shift + q","title":"Create sample configuration file"},{"location":"Tips_And_Tricks/#determine-the-id-of-the-buttons-you-want-to-use","text":"It is very simple. Just run xev command. For example my left button code (button 1): ButtonPress event, serial 37, synthetic NO, window 0x4800001, root 0xc5, subw 0x0, time 7138218, (103,85), root:(965,1564), state 0x10, button 1, same_screen YES ButtonRelease event, serial 37, synthetic NO, window 0x4800001, root 0xc5, subw 0x0, time 7138264, (103,85), root:(965,1564), state 0x110, button 1, same_screen YES Example with grep to easier determine button ID: xev | egrep -o 'button [0-9]{1,2}'","title":"Determine the ID of the buttons you want to use"},{"location":"Tips_And_Tricks/#configure-xbindkeysrc","text":"So in my case I want to configure only two buttons: Left Scrolling for minimize window Right Scrolling for maximize window I had to add only these two section to .xbindkeysrc: Minimize (Button 11) \"xdotool getactivewindow windowminimize\" b:11 Maximize (button 12) \"wmctrl -r :ACTIVE: -b toggle,maximized_vert,maximized_horz\" b:12 As you can see we had to use two different command: xdotool and wmctrl . If you want to use your buttons for any other activity I'm pretty sure that after some googleing you will find your solution.","title":"Configure .xbindkeysrc"},{"location":"Tips_And_Tricks/#case-insensitive-search-in-oracle-db","text":"alter session set NLS_COMP=ANSI; alter session set NLS_SORT=BINARY_CI; REFERENCE: http://stackoverflow.com/questions/1031844/oracle-db-how-can-i-write-query-ignoring-case","title":"Case Insensitive Search In Oracle DB"},{"location":"Tips_And_Tricks/#change-date-timestamp-format-in-oracle-db","text":"alter session set NLS_DATE_FORMAT='yyyy-mm-dd HH24:mi:ss'; ALTER SESSION SET NLS_TIMESTAMP_FORMAT='yyyy-mm-dd HH24:mi:ss';","title":"Change Date &amp; Timestamp Format In Oracle DB"},{"location":"Tips_And_Tricks/#redirect-all-output-stderr-stdout-to-a-file","text":"#!/bin/bash LOG=\"[LOG file location]\" exec >> $LOG 2>&1 ... ... ...","title":"Redirect all output (stderr, stdout) to a file"},{"location":"Tips_And_Tricks/#redirect-nohup-output","text":"You can redirect stdout and stderr to differrent files or into the same file. Examples: nohup ./program >stdout.log 2>stderr.log nohup ./progrem >stoutAndStderr.log 2>&1 abbreviated syntax nohup command > output-$(date +%Y%m%d_%H%M%S).log & Example startup script for OpenHAB: cat start-daemon.sh #!/bin/sh ... ... ... echo Launching the openHAB runtime... nohup java \\ -Dosgi.clean=true \\ -Declipse.ignoreApp=true \\ -Dosgi.noShutdown=true \\ -Djetty.port=$HTTP_PORT \\ -Djetty.port.ssl=$HTTPS_PORT \\ -Djetty.home=. \\ -Dlogback.configurationFile=configurations/logback.xml \\ -Dfelix.fileinstall.dir=addons -Dfelix.fileinstall.filter=.*\\\\.jar \\ -Djava.library.path=lib \\ -Djava.security.auth.login.config=/opt/openhab/runtime/distribution-1.8.3-runtime/etc/login.conf \\ -Dorg.quartz.properties=./etc/quartz.properties \\ -Dequinox.ds.block_timeout=240000 \\ -Dequinox.scr.waitTimeOnBlock=60000 \\ -Dfelix.fileinstall.active.level=4 \\ -Djava.awt.headless=true \\ -jar $cp $* \\ -console 9898 >nohup-$(date +%Y%m%d_%H%M%S).out &","title":"Redirect nohup output"},{"location":"kub/","text":"INSTALL \u00b6 Important: kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. If you do not, there is a risk of a version skew occurring that can lead to unexpected, buggy behaviour. However, one minor version skew between the kubelet and the control plane is supported, but the kubelet version may never exceed the API server version. For example, kubelets running 1.7.0 should be fully compatible with a 1.8.0 API server, but not vice versa. Add Repository \u00b6 sudo apt-get update && sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update Check Versions \u00b6 Lehets\u00e9ges parancsok: apt-cache madison kubectl apt-cache madison kubectl kubectl: Installed: (none) Candidate: 1.17.3-00 Version table: 1.17.3-00 500 500 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages 1.17.2-00 500 500 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages 1.17.1-00 500 500 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages apt list kubectl Listing... Done kubectl/kubernetes-xenial 1.17.3-00 amd64 N: There are 143 additional versions. Please use the '-a' switch to see them. apt list kubectl kubelet kubeadm Listing... Done kubeadm/kubernetes-xenial 1.17.3-00 amd64 kubectl/kubernetes-xenial 1.17.3-00 amd64 kubelet/kubernetes-xenial 1.17.3-00 amd64 Ellen\u0151rz\u00e9s telep\u00edt\u00e9s el\u0151tt: apt-get install -s kubelet kubeadm kubectl cgroup \u00b6 Check: docker info | grep -i cgroup Cgroup Driver: cgroupfs Modify Config : cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\", \"live-restore\": true } ReCheck docker info | grep -i cgroup Cgroup Driver: systemd /etc/containerd/config.toml/config.toml (ez m\u00e9gsem kell) root@docker:/home/vinyo# cat /etc/containerd/config.toml | grep cg #plugins.cri.systemd_cgroup = true Kubelet start script \u00b6 cat kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/home/ After=docker.service [Service] ExecStart=/usr/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target Aadmin \u00b6 https://github.com/cloudnativelabs/kube-router/blob/master/docs/kubeadm.md KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml https://cloudblue.freshdesk.com/support/solutions/articles/44001886522 1 node(s) had taints that the pod didn't tolerate. \u00b6 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling <unknown> default-scheduler 0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate. Warning FailedScheduling <unknown> default-scheduler 0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate. Megolds\u00e1s: kubectl taint nodes docker.loc node-role.kubernetes.io/master- vsyscall \u00b6 GRUB_CMDLINE_LINUX_DEFAULT=\"quiet vsyscall=emulate\" https://einsteinathome.org/content/vsyscall-now-disabled-latest-linux-distros","title":"INSTALL"},{"location":"kub/#install","text":"Important: kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control plane you want kubeadm to install for you. If you do not, there is a risk of a version skew occurring that can lead to unexpected, buggy behaviour. However, one minor version skew between the kubelet and the control plane is supported, but the kubelet version may never exceed the API server version. For example, kubelets running 1.7.0 should be fully compatible with a 1.8.0 API server, but not vice versa.","title":"INSTALL"},{"location":"kub/#add-repository","text":"sudo apt-get update && sudo apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list sudo apt-get update","title":"Add Repository"},{"location":"kub/#check-versions","text":"Lehets\u00e9ges parancsok: apt-cache madison kubectl apt-cache madison kubectl kubectl: Installed: (none) Candidate: 1.17.3-00 Version table: 1.17.3-00 500 500 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages 1.17.2-00 500 500 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages 1.17.1-00 500 500 https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages apt list kubectl Listing... Done kubectl/kubernetes-xenial 1.17.3-00 amd64 N: There are 143 additional versions. Please use the '-a' switch to see them. apt list kubectl kubelet kubeadm Listing... Done kubeadm/kubernetes-xenial 1.17.3-00 amd64 kubectl/kubernetes-xenial 1.17.3-00 amd64 kubelet/kubernetes-xenial 1.17.3-00 amd64 Ellen\u0151rz\u00e9s telep\u00edt\u00e9s el\u0151tt: apt-get install -s kubelet kubeadm kubectl","title":"Check Versions"},{"location":"kub/#cgroup","text":"Check: docker info | grep -i cgroup Cgroup Driver: cgroupfs Modify Config : cat /etc/docker/daemon.json { \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\", \"live-restore\": true } ReCheck docker info | grep -i cgroup Cgroup Driver: systemd /etc/containerd/config.toml/config.toml (ez m\u00e9gsem kell) root@docker:/home/vinyo# cat /etc/containerd/config.toml | grep cg #plugins.cri.systemd_cgroup = true","title":"cgroup"},{"location":"kub/#kubelet-start-script","text":"cat kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/home/ After=docker.service [Service] ExecStart=/usr/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target","title":"Kubelet start script"},{"location":"kub/#aadmin","text":"https://github.com/cloudnativelabs/kube-router/blob/master/docs/kubeadm.md KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yaml https://cloudblue.freshdesk.com/support/solutions/articles/44001886522","title":"Aadmin"},{"location":"kub/#1-nodes-had-taints-that-the-pod-didnt-tolerate","text":"Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling <unknown> default-scheduler 0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate. Warning FailedScheduling <unknown> default-scheduler 0/1 nodes are available: 1 node(s) had taints that the pod didn't tolerate. Megolds\u00e1s: kubectl taint nodes docker.loc node-role.kubernetes.io/master-","title":"1 node(s) had taints that the pod didn't tolerate."},{"location":"kub/#vsyscall","text":"GRUB_CMDLINE_LINUX_DEFAULT=\"quiet vsyscall=emulate\" https://einsteinathome.org/content/vsyscall-now-disabled-latest-linux-distros","title":"vsyscall"},{"location":"proba/","text":"Useful \u00b6 echo \"################ health ################\" curl \"http://$(hostname -f):9200/_cat/health?v\" echo \"################ Allocation ################\" curl \"http://$(hostname -f):9200/_cat/allocation?v\" echo \"################ indices ################\" curl \"http://$(hostname -f):9200/_cat/indices?v\" echo \"################ shards ################\" curl \"http://$(hostname -f):9200/_cat/shards?v\" echo \"################ nodes ################\" curl \"http://$(hostname -f):9200/_cat/nodes?v\" Template / Settings \u00b6 Change Settings of an index \u00b6 curl -X PUT \"localhost:9200/bitbucket-search/_settings?pretty\" -H 'Content-Type: application/json' -d' { \"index\" : { \"number_of_replicas\" : 0 } } ' Get Templates \u00b6 curl -s -X GET \"http://itdevokdtstw1.vodafone.hu:30918/_index_template?pretty\" | jq -r \".index_templates[].name\" Get One Template \u00b6 Command: \u00b6 curl -s -X GET \"http://itdevokdtstw1.vodafone.hu:30918/_index_template/logstash?pretty\" Output: \u00b6 { \"index_templates\" : [ { \"name\" : \"logstash\", \"index_template\" : { \"index_patterns\" : [ \"logstash-*\" ], \"template\" : { \"settings\" : { \"index\" : { \"lifecycle\" : { \"name\" : \"logstash\", \"rollover_alias\" : \"logstash\" }, \"number_of_shards\" : \"3\", \"number_of_replicas\" : \"0\" } } }, \"composed_of\" : [ ] } } ] } Create New & Check \u00b6 Command: \u00b6 curl -X PUT \"http://itdevokdtstw1.vodafone.hu:30918/_index_template/notused?pretty\" -H 'Content-Type: application/json' -d' Response: \u00b6 { \"index_patterns\" : [ \"notused-*\" ], \"template\" : { \"settings\" : { \"index\" : { \"number_of_shards\" : \"3\", \"number_of_replicas\" : \"0\" } } }, \"composed_of\" : [ ] } ' Command: \u00b6 curl -s -X GET \"http://itdevokdtstw1.vodafone.hu:30918/_index_template/notused?pretty\" Response: \u00b6 { \"index_templates\" : [ { \"name\" : \"notused\", \"index_template\" : { \"index_patterns\" : [ \"notused-*\" ], \"template\" : { \"settings\" : { \"index\" : { \"number_of_shards\" : \"3\", \"number_of_replicas\" : \"0\" } } }, \"composed_of\" : [ ] } } ] } console.log('Hello, world!'); int main() { return 0; }","title":"Useful"},{"location":"proba/#useful","text":"echo \"################ health ################\" curl \"http://$(hostname -f):9200/_cat/health?v\" echo \"################ Allocation ################\" curl \"http://$(hostname -f):9200/_cat/allocation?v\" echo \"################ indices ################\" curl \"http://$(hostname -f):9200/_cat/indices?v\" echo \"################ shards ################\" curl \"http://$(hostname -f):9200/_cat/shards?v\" echo \"################ nodes ################\" curl \"http://$(hostname -f):9200/_cat/nodes?v\"","title":"Useful"},{"location":"proba/#template-settings","text":"","title":"Template / Settings"},{"location":"proba/#change-settings-of-an-index","text":"curl -X PUT \"localhost:9200/bitbucket-search/_settings?pretty\" -H 'Content-Type: application/json' -d' { \"index\" : { \"number_of_replicas\" : 0 } } '","title":"Change Settings of an index"},{"location":"proba/#get-templates","text":"curl -s -X GET \"http://itdevokdtstw1.vodafone.hu:30918/_index_template?pretty\" | jq -r \".index_templates[].name\"","title":"Get Templates"},{"location":"proba/#get-one-template","text":"","title":"Get One Template"},{"location":"proba/#command","text":"curl -s -X GET \"http://itdevokdtstw1.vodafone.hu:30918/_index_template/logstash?pretty\"","title":"Command:"},{"location":"proba/#output","text":"{ \"index_templates\" : [ { \"name\" : \"logstash\", \"index_template\" : { \"index_patterns\" : [ \"logstash-*\" ], \"template\" : { \"settings\" : { \"index\" : { \"lifecycle\" : { \"name\" : \"logstash\", \"rollover_alias\" : \"logstash\" }, \"number_of_shards\" : \"3\", \"number_of_replicas\" : \"0\" } } }, \"composed_of\" : [ ] } } ] }","title":"Output:"},{"location":"proba/#create-new-check","text":"","title":"Create New &amp; Check"},{"location":"proba/#command_1","text":"curl -X PUT \"http://itdevokdtstw1.vodafone.hu:30918/_index_template/notused?pretty\" -H 'Content-Type: application/json' -d'","title":"Command:"},{"location":"proba/#response","text":"{ \"index_patterns\" : [ \"notused-*\" ], \"template\" : { \"settings\" : { \"index\" : { \"number_of_shards\" : \"3\", \"number_of_replicas\" : \"0\" } } }, \"composed_of\" : [ ] } '","title":"Response:"},{"location":"proba/#command_2","text":"curl -s -X GET \"http://itdevokdtstw1.vodafone.hu:30918/_index_template/notused?pretty\"","title":"Command:"},{"location":"proba/#response_1","text":"{ \"index_templates\" : [ { \"name\" : \"notused\", \"index_template\" : { \"index_patterns\" : [ \"notused-*\" ], \"template\" : { \"settings\" : { \"index\" : { \"number_of_shards\" : \"3\", \"number_of_replicas\" : \"0\" } } }, \"composed_of\" : [ ] } } ] } console.log('Hello, world!'); int main() { return 0; }","title":"Response:"},{"location":"multi/diff/","text":"Diff Files \u00b6 Proba \u00b6 Only in ghost-current/content/themes/casper/assets/built: prism.css Only in ghost-current/content/themes/casper/assets/built: prism.css.map Only in ghost-current/content/themes/casper/assets/css: prism.css diff -u -r ghost-compare/content/themes/casper/assets/css/screen.css ghost-current/content/themes/casper/assets/css/screen.css --- ghost-compare/content/themes/casper/assets/css/screen.css 2017-12-07 13:30:00.000000000 +0100 +++ ghost-current/content/themes/casper/assets/css/screen.css 2017-12-12 14:08:19.307838832 +0100 @@ -71,7 +71,7 @@ /* Centered content container blocks */ .inner { margin: 0 auto; - max-width: 1040px; + max-width: none; width: 100%; } @@ -670,7 +670,7 @@ display: flex; flex-direction: column; align-items: center; - max-width: 920px; + max-width: none; } .post-full-content h1, Only in ghost-current/content/themes/casper/assets: ghostHunter Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter.js Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter.min.js Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter-nodependency.js Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter-nodependency.min.js Only in ghost-current/content/themes/casper/assets/js: lunr.js Only in ghost-current/content/themes/casper/assets/js: lunr.min.js Only in ghost-current/content/themes/casper/assets/js: prism.js diff -u -r ghost-compare/content/themes/casper/default.hbs ghost-current/content/themes/casper/default.hbs --- ghost-compare/content/themes/casper/default.hbs 2017-12-07 13:30:00.000000000 +0100 +++ ghost-current/content/themes/casper/default.hbs 2017-12-12 12:46:55.127683665 +0100 @@ -13,6 +13,11 @@ {{!-- Styles'n'Scripts --}} <link rel=\"stylesheet\" type=\"text/css\" href=\"{{asset \"built/screen.css\"}}\" /> + <script src=\"{{asset \"js/prism.js\"}}\"></script> + <script src=\"{{asset \"js/jquery.ghostHunter.min.js\"}}\"></script> + <script src=\"{{asset \"js/lunr.js\"}}\"></script> + + <link rel=\"stylesheet\" type=\"text/css\" href=\"{{asset \"css/prism.css\"}}\" /> {{!-- This tag outputs SEO meta+structured data and other important settings --}} {{ghost_head}} Only in ghost-current/content/themes/casper/: node_modules diff -u -r ghost-compare/content/themes/casper/partials/floating-header.hbs ghost-current/content/themes/casper/partials/floating-header.hbs --- ghost-compare/content/themes/casper/partials/floating-header.hbs 2017-12-07 13:30:00.000000000 +0100 +++ ghost-current/content/themes/casper/partials/floating-header.hbs 2017-12-12 12:37:05.767664941 +0100 @@ -9,17 +9,6 @@ </div> <span class=\"floating-header-divider\">&mdash;</span> <div class=\"floating-header-title\">{{title}}</div> - <div class=\"floating-header-share\"> - <div class=\"floating-header-share-label\">Share this {{> \"icons/point\"}}</div> - <a class=\"floating-header-share-tw\" href=\"https://twitter.com/share?text={{encode title}}&amp;url={{url absolute=\"true\"}}\" - onclick=\"window.open(this.href, 'share-twitter', 'width=550,height=235');return false;\"> - {{> \"icons/twitter\"}} - </a> - <a class=\"floating-header-share-fb\" href=\"https://www.facebook.com/sharer/sharer.php?u={{url absolute=\"true\"}}\" - onclick=\"window.open(this.href, 'share-facebook','width=580,height=296');return false;\"> - {{> \"icons/facebook\"}} - </a> - </div> <progress class=\"progress\" value=\"0\"> <div class=\"progress-container\"> <span class=\"progress-bar\"></span> Only in ghost-current/content/themes/casper/partials: floating-header.hbs-bck New Search page \u00b6 Ketto \u00b6 <!DOCTYPE html> <!--[if lt IE 7]> <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]--> <!--[if IE 7]> <html class=\"no-js lt-ie9 lt-ie8\"> <![endif]--> <!--[if IE 8]> <html class=\"no-js lt-ie9\"> <![endif]--> <!--[if gt IE 8]><!--> <html class=\"no-js\"> <!--<![endif]--> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"> <title>ghostHunter</title> <meta name=\"description\" content=\"\"> <meta name=\"viewport\" content=\"width=device-width\"> </head> <body> <form> <input id=\"search-field\" /> <input type=\"submit\" value=\"search\"> <input type=\"button\" value=\"clear\" onclick=\"clearResults();\" /> </form> <hr /> <section id=\"results\"></section> <script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.1/jquery.min.js\"></script> <script src=\"../assets/js/jquery.ghostHunter.js\"></script> <script> var searchField = $(\"#search-field\").ghostHunter({ results : \"#results\", rss : \"rss.xml\", //Enable the \"search as you type\" by uncommenting the following line //onKeyUp : true, result_template: \"<a href='{{link}}'><p><h2>{{title}}</h2><h4>{{pubDate}}</h4>{{description}}</p></a>\" }); function clearResults() { searchField.clear(); } </script> </body> </html> Compile CSS (Gulp) \u00b6 Screen \u00b6 Build \u00b6 Itt kell kiadni a parancsokat: ... /ghost-current/content/themes/casper Ez a theme root dir. npm install gulp Egy \u00b6 fdsfds Ketto \u00b6 fsdf Ketto \u00b6 dfsdf Harom \u00b6 fsdfsd harom \u00b6 sfsdfsf ketto \u00b6 fsdfs egy \u00b6 fsdf","title":"Diff Files"},{"location":"multi/diff/#diff-files","text":"","title":"Diff Files"},{"location":"multi/diff/#proba","text":"Only in ghost-current/content/themes/casper/assets/built: prism.css Only in ghost-current/content/themes/casper/assets/built: prism.css.map Only in ghost-current/content/themes/casper/assets/css: prism.css diff -u -r ghost-compare/content/themes/casper/assets/css/screen.css ghost-current/content/themes/casper/assets/css/screen.css --- ghost-compare/content/themes/casper/assets/css/screen.css 2017-12-07 13:30:00.000000000 +0100 +++ ghost-current/content/themes/casper/assets/css/screen.css 2017-12-12 14:08:19.307838832 +0100 @@ -71,7 +71,7 @@ /* Centered content container blocks */ .inner { margin: 0 auto; - max-width: 1040px; + max-width: none; width: 100%; } @@ -670,7 +670,7 @@ display: flex; flex-direction: column; align-items: center; - max-width: 920px; + max-width: none; } .post-full-content h1, Only in ghost-current/content/themes/casper/assets: ghostHunter Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter.js Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter.min.js Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter-nodependency.js Only in ghost-current/content/themes/casper/assets/js: jquery.ghostHunter-nodependency.min.js Only in ghost-current/content/themes/casper/assets/js: lunr.js Only in ghost-current/content/themes/casper/assets/js: lunr.min.js Only in ghost-current/content/themes/casper/assets/js: prism.js diff -u -r ghost-compare/content/themes/casper/default.hbs ghost-current/content/themes/casper/default.hbs --- ghost-compare/content/themes/casper/default.hbs 2017-12-07 13:30:00.000000000 +0100 +++ ghost-current/content/themes/casper/default.hbs 2017-12-12 12:46:55.127683665 +0100 @@ -13,6 +13,11 @@ {{!-- Styles'n'Scripts --}} <link rel=\"stylesheet\" type=\"text/css\" href=\"{{asset \"built/screen.css\"}}\" /> + <script src=\"{{asset \"js/prism.js\"}}\"></script> + <script src=\"{{asset \"js/jquery.ghostHunter.min.js\"}}\"></script> + <script src=\"{{asset \"js/lunr.js\"}}\"></script> + + <link rel=\"stylesheet\" type=\"text/css\" href=\"{{asset \"css/prism.css\"}}\" /> {{!-- This tag outputs SEO meta+structured data and other important settings --}} {{ghost_head}} Only in ghost-current/content/themes/casper/: node_modules diff -u -r ghost-compare/content/themes/casper/partials/floating-header.hbs ghost-current/content/themes/casper/partials/floating-header.hbs --- ghost-compare/content/themes/casper/partials/floating-header.hbs 2017-12-07 13:30:00.000000000 +0100 +++ ghost-current/content/themes/casper/partials/floating-header.hbs 2017-12-12 12:37:05.767664941 +0100 @@ -9,17 +9,6 @@ </div> <span class=\"floating-header-divider\">&mdash;</span> <div class=\"floating-header-title\">{{title}}</div> - <div class=\"floating-header-share\"> - <div class=\"floating-header-share-label\">Share this {{> \"icons/point\"}}</div> - <a class=\"floating-header-share-tw\" href=\"https://twitter.com/share?text={{encode title}}&amp;url={{url absolute=\"true\"}}\" - onclick=\"window.open(this.href, 'share-twitter', 'width=550,height=235');return false;\"> - {{> \"icons/twitter\"}} - </a> - <a class=\"floating-header-share-fb\" href=\"https://www.facebook.com/sharer/sharer.php?u={{url absolute=\"true\"}}\" - onclick=\"window.open(this.href, 'share-facebook','width=580,height=296');return false;\"> - {{> \"icons/facebook\"}} - </a> - </div> <progress class=\"progress\" value=\"0\"> <div class=\"progress-container\"> <span class=\"progress-bar\"></span> Only in ghost-current/content/themes/casper/partials: floating-header.hbs-bck","title":"Proba"},{"location":"multi/diff/#new-search-page","text":"","title":"New Search page"},{"location":"multi/diff/#ketto","text":"<!DOCTYPE html> <!--[if lt IE 7]> <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]--> <!--[if IE 7]> <html class=\"no-js lt-ie9 lt-ie8\"> <![endif]--> <!--[if IE 8]> <html class=\"no-js lt-ie9\"> <![endif]--> <!--[if gt IE 8]><!--> <html class=\"no-js\"> <!--<![endif]--> <head> <meta charset=\"utf-8\"> <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"> <title>ghostHunter</title> <meta name=\"description\" content=\"\"> <meta name=\"viewport\" content=\"width=device-width\"> </head> <body> <form> <input id=\"search-field\" /> <input type=\"submit\" value=\"search\"> <input type=\"button\" value=\"clear\" onclick=\"clearResults();\" /> </form> <hr /> <section id=\"results\"></section> <script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.1/jquery.min.js\"></script> <script src=\"../assets/js/jquery.ghostHunter.js\"></script> <script> var searchField = $(\"#search-field\").ghostHunter({ results : \"#results\", rss : \"rss.xml\", //Enable the \"search as you type\" by uncommenting the following line //onKeyUp : true, result_template: \"<a href='{{link}}'><p><h2>{{title}}</h2><h4>{{pubDate}}</h4>{{description}}</p></a>\" }); function clearResults() { searchField.clear(); } </script> </body> </html>","title":"Ketto"},{"location":"multi/diff/#compile-css-gulp","text":"","title":"Compile CSS (Gulp)"},{"location":"multi/diff/#screen","text":"","title":"Screen"},{"location":"multi/diff/#build","text":"Itt kell kiadni a parancsokat: ... /ghost-current/content/themes/casper Ez a theme root dir. npm install gulp","title":"Build"},{"location":"multi/diff/#egy","text":"fdsfds","title":"Egy"},{"location":"multi/diff/#ketto_1","text":"fsdf","title":"Ketto"},{"location":"multi/diff/#ketto_2","text":"dfsdf","title":"Ketto"},{"location":"multi/diff/#harom","text":"fsdfsd","title":"Harom"},{"location":"multi/diff/#harom_1","text":"sfsdfsf","title":"harom"},{"location":"multi/diff/#ketto_3","text":"fsdfs","title":"ketto"},{"location":"multi/diff/#egy_1","text":"fsdf","title":"egy"},{"location":"multi/okd/","text":"Role & ServiceAccount \u00b6 ClusterRole \u00b6 El\u0151sz\u00f6r csin\u00e1l\u00e1lni kell egy ClusterRole-t, amit majd a user-hez rendel\u00fcnk. Ments\u00fck le az al\u00e1bbit egy file-ba. (pl.: 01-create-cluster-role.yaml ) kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: descheduler-cluster-role rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\", \"delete\"] - apiGroups: [\"\"] resources: [\"pods/eviction\"] verbs: [\"create\"] Majd futtasuk az oc create parancsot: oc create -f 01-create-cluster-role.yaml clusterrole.rbac.authorization.k8s.io/descheduler-cluster-role created Ellen\u0151rz\u00e9s: oc get clusterrole | grep desc descheduler-cluster-role User (service account) \u00b6 A JOB-nak a helye a RedHat le\u00edr\u00e1sa alapj\u00e1n a kube-system -ben van. oc create sa descheduler-sa -n kube-system Ezzel megvan a kis descheduler-sa service account-unk. Cluster Role Binding \u00b6 User m\u00e1r van, ClusterRole is van. M\u00e1r csak valahogyan \u00f6ssze kellene rendelni a kett\u0151t. oc create clusterrolebinding descheduler-cluster-role-binding \\ --clusterrole=descheduler-cluster-role \\ --serviceaccount=kube-system:descheduler-sa descheduler-cluster-role-binding : A ClusterRoleBinding neve. descheduler-cluster-role : A cluster role neve amit a yaml-ben adtunk meg ( name: descheduler-cluster-role ). kube-system:descheduler-sa : A namespace \u00e9s a kett\u0151spont ut\u00e1n a user amit el\u0151bb csin\u00e1ltunk. Futtat\u00e1s: oc create clusterrolebinding descheduler-cluster-role-binding \\ > --clusterrole=descheduler-cluster-role \\ > --serviceaccount=kube-system:descheduler-sa clusterrolebinding.rbac.authorization.k8s.io/descheduler-cluster-role-binding created Ellen\u0151rz\u00e9s: oc describe clusterrolebinding/descheduler-cluster-role-binding Name: descheduler-cluster-role-binding Created: Less than a second ago Labels: <none> Annotations: <none> Role: /descheduler-cluster-role Users: <none> Groups: <none> ServiceAccounts: kube-system/descheduler-sa Subjects: <none> Verbs Non-Resource URLs Resource Names API Groups Resources [get list watch] [] [] [] [nodes] [delete get list watch] [] [] [] [pods] [create] [] [] [] [pods/eviction] Teh\u00e1t eddig minden ok. :) Create Config Map(s) \u00b6 A majdani job(ok) configmap(ek)b\u0151l fogj\u00e1k kiolvasni, hogy mit csin\u00e1ljanak. Itt van egy komplett p\u00e9lda: apiVersion: \"descheduler/v1alpha1\" kind: \"DeschedulerPolicy\" strategies: \"RemoveDuplicates\": enabled: false \"LowNodeUtilization\": enabled: true params: nodeResourceUtilizationThresholds: thresholds: \"cpu\" : 20 \"memory\": 20 \"pods\": 20 targetThresholds: \"cpu\" : 50 \"memory\": 50 \"pods\": 50 numberOfNodes: 3 \"RemovePodsViolatingInterPodAntiAffinity\": enabled: true ``` **H\u00e1rom strat\u00e9gia van:** * Remove duplicate pods (*RemoveDuplicates*) * Move pods to underutilized nodes (*LowNodeUtilization*) * Remove pods that violate anti-affinity rules (*RemovePodsViolatingInterPodAntiAffinity*). Ez mind a h\u00e1rom fel van sorolva a fenti p\u00e9ld\u00e1ban. ## Config Map l\u00e9trehoz\u00e1sa ```bash oc create configmap descheduler-policy-configmap-allinone \\ -n kube-system --from-file=05-allinone-configmap.yaml (Nekem a file neve: 05-allinone-configmap.yaml ) Crete (run) Job \u00b6 Job defin\u00edci\u00f3: apiVersion: batch/v1 kind: Job metadata: name: descheduler-job-03 namespace: kube-system spec: parallelism: 1 completions: 1 template: metadata: name: descheduler-pod annotations: scheduler.alpha.kubernetes.io/critical-pod: \"true\" spec: containers: - name: descheduler image: itdevarepotst.vodafone.hu/docker-remote-quay/openshift/origin-descheduler:v3.11 volumeMounts: - mountPath: /policy-dir name: policy-volume command: - \"/bin/sh\" - \"-ec\" - | /bin/descheduler --policy-config-file /policy-dir/05-allinone-configmap.yaml restartPolicy: \"Never\" serviceAccountName: descheduler-sa volumes: - name: policy-volume configMap: name: descheduler-policy-configmap-allinone Egy kis magyar\u00e1zkod\u00e1s: * scheduler.alpha.kubernetes.io/critical-pod: \"true\" It is important to note that there are a number of core components, such as DNS, that are critical to a fully functional cluster, but, run on a regular cluster node rather than the master. A cluster may stop working properly if the component is evicted. To prevent the descheduler from removing these pods, configure the pod as a critical pod by adding the scheduler.alpha.kubernetes.io/critical-pod annotation to the pod specification. image: itdevarepotst.vodafone.hu/docker-remote-quay/openshift/origin-descheduler:v3.11 \u00c9n itt tal\u00e1ltam meg a descheduler docker image-t. Sereg\u00e9ly D\u00e1vid megtal\u00e1lta egy m\u00e1sik helyen is: https://microbadger.com/images/openshift/origin-descheduler:v3.11.0 Val\u00f3sz\u00edn\u00e1leg m\u00e9g t\u00f6bb helyen is meg lehet tal\u00e1lni. :) A teszt Artifactory-ban a docker-remote-quay egy remote repo, ami ide mutat: https://quay.io name: descheduler-policy-configmap-lownodeutilization \u00e9s --policy-config-file /policy-dir/04-test-configmap.yaml Ezek mindkettetten a Configmap-b\u0151l j\u00f6nnek. Itt, \u00edgy lehet megtal\u00e1lni (Name, Data) : Name: descheduler-policy-configmap-allinone Namespace: kube-system Labels: <none> Annotations: <none> Data ==== 05-allinone-configmap.yaml: ---- apiVersion: \"descheduler/v1alpha1\" kind: \"DeschedulerPolicy\" strategies: \"RemoveDuplicates\": enabled: false \"LowNodeUtilization\": enabled: true params: nodeResourceUtilizationThresholds: thresholds: \"cpu\" : 20 \"memory\": 20 \"pods\": 20 targetThresholds: \"cpu\" : 50 \"memory\": 50 \"pods\": 50 numberOfNodes: 3 \"RemovePodsViolatingInterPodAntiAffinity\": enabled: true Events: <none> Run Job: Ezt m\u00e1r a legegyszer\u0171bb r\u00e9sze, vagy oc apply vagy oc create parancs. oc apply -f 03-CreateJob.yaml Ellen\u0151rz\u00e9s: oc get job NAME DESIRED SUCCESSFUL AGE descheduler-job-03 1 1 9s Ez a szerencs\u00e9tlen sajnos semmi logot nem \u00edr: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc logs job/descheduler-job-03 janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc logs pod/descheduler-job-03-8f5hb janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ \u00cdgy ellen\u0151rizni csak a describe vagy oc get events paranccsal lehet. \u00dajrafuttatni a job-ot \u00edgy lehet: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc delete -f 03-CreateJob.yaml job.batch \"descheduler-job-03\" deleted janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc apply -f 03-CreateJob.yaml job.batch/descheduler-job-03 created janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ Vagy a job defin\u00edci\u00f3s file-be \u00e1t\u00edrod a nev\u00e9t: apiVersion: batch/v1 kind: Job metadata: name: descheduler-job-03 namespace: kube-system spec: Pl. descheduler-job-03 --> descheduler-job-04 \u00c9s \u00edgy m\u00e1r k\u00e9t job lesz SUCCESSFUL state-ben: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get jobs NAME DESIRED SUCCESSFUL AGE descheduler-job-03 1 1 1m descheduler-job-04 1 1 26s Tesztel\u00e9s \u00b6 Kettes l\u00e1bat kiszedtem a forgalomb\u00f3l: oc adm manage-node itdevkubtstapp2.vodafone.hu --schedulable=false \u00c9s evaku\u00e1ltam: oc adm manage-node itdevkubtstapp2.vodafone.hu --evacuate Parancs kimenete: evaculate.txt \u00cdgy gyakorlatilag nem maradt rajta POD: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE app-storage glusterfs-storage-nzshm 0/1 Pending 0 3s <none> itdevkubtstapp2.vodafone.hu <none> application-monitoring prom-0 3/4 CrashLoopBackOff 64 5h 192.168.1.103 itdevkubtstapp2.vodafone.hu <none> ent-arch-dbms mysql-master-6-r57qm 0/1 Terminating 327 1d 192.168.1.98 itdevkubtstapp2.vodafone.hu <none> microserviceproject microservice-kitchen-sin-10-fcj2q 1/1 Terminating 0 1d 192.168.1.39 itdevkubtstapp2.vodafone.hu <none> openshift-logging logging-fluentd-g455f 0/1 ContainerCreating 0 31s <none> itdevkubtstapp2.vodafone.hu <none> openshift-monitoring node-exporter-zb7cv 0/2 ContainerCreating 0 29s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> openshift-node sync-c6smk 0/1 ContainerCreating 0 39s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> openshift-sdn ovs-pfcvz 0/1 ContainerCreating 0 34s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> openshift-sdn sdn-nrd5j 0/1 ContainerCreating 0 33s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> Ezut\u00e1n vissza\u00e1ll\u00edtottam a \"schedulable\" flag-et: oc adm manage-node itdevkubtstapp2.vodafone.hu --schedulable=true Egy\u00e9bk\u00e9nt ellen\u0151rizni \u00edgy (is) lehet: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc describe node/itdevkubtstapp2.vodafone.hu | grep Unschedulable Unschedulable: true \u00c9rdemes figyelni mert ellent\u00e9tes a k\u00e9t cuccos: schedulable <--> Unschedulable Ezzel a be\u00e1ll\u00edt\u00e1ssal teszteltem: apiVersion: \"descheduler/v1alpha1\" kind: \"DeschedulerPolicy\" strategies: \"RemoveDuplicates\": enabled: false \"LowNodeUtilization\": enabled: true params: nodeResourceUtilizationThresholds: thresholds: \"pods\": 20 targetThresholds: \"pods\": 50 \"RemovePodsViolatingInterPodAntiAffinity\": enabled: true \u00c9s itt l\u00e1tni, hogy mi t\u00f6rt\u00e9nt: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu | wc -l 15 janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc delete cm/descheduler-policy-configmap-allinone configmap \"descheduler-policy-configmap-allinone\" deleted janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc create configmap descheduler-policy-configmap-allinone -n kube-system --from-file=05-allinone-configmap.yaml configmap/descheduler-policy-configmap-allinone created janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc delete -f 03-CreateJob.yaml job.batch \"descheduler-job-04\" deleted janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc apply -f 03-CreateJob.yaml job.batch/descheduler-job-04 created janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu | wc -l 21 janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu | wc -l 21 L\u00e1tni, hogy a job fut\u00e1sa el\u0151tt csak ~15 POD volt rajta, ut\u00e1na pedig ~21 darab.","title":"Role & ServiceAccount"},{"location":"multi/okd/#role-serviceaccount","text":"","title":"Role &amp; ServiceAccount"},{"location":"multi/okd/#clusterrole","text":"El\u0151sz\u00f6r csin\u00e1l\u00e1lni kell egy ClusterRole-t, amit majd a user-hez rendel\u00fcnk. Ments\u00fck le az al\u00e1bbit egy file-ba. (pl.: 01-create-cluster-role.yaml ) kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: descheduler-cluster-role rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"watch\", \"list\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\", \"delete\"] - apiGroups: [\"\"] resources: [\"pods/eviction\"] verbs: [\"create\"] Majd futtasuk az oc create parancsot: oc create -f 01-create-cluster-role.yaml clusterrole.rbac.authorization.k8s.io/descheduler-cluster-role created Ellen\u0151rz\u00e9s: oc get clusterrole | grep desc descheduler-cluster-role","title":"ClusterRole"},{"location":"multi/okd/#user-service-account","text":"A JOB-nak a helye a RedHat le\u00edr\u00e1sa alapj\u00e1n a kube-system -ben van. oc create sa descheduler-sa -n kube-system Ezzel megvan a kis descheduler-sa service account-unk.","title":"User (service account)"},{"location":"multi/okd/#cluster-role-binding","text":"User m\u00e1r van, ClusterRole is van. M\u00e1r csak valahogyan \u00f6ssze kellene rendelni a kett\u0151t. oc create clusterrolebinding descheduler-cluster-role-binding \\ --clusterrole=descheduler-cluster-role \\ --serviceaccount=kube-system:descheduler-sa descheduler-cluster-role-binding : A ClusterRoleBinding neve. descheduler-cluster-role : A cluster role neve amit a yaml-ben adtunk meg ( name: descheduler-cluster-role ). kube-system:descheduler-sa : A namespace \u00e9s a kett\u0151spont ut\u00e1n a user amit el\u0151bb csin\u00e1ltunk. Futtat\u00e1s: oc create clusterrolebinding descheduler-cluster-role-binding \\ > --clusterrole=descheduler-cluster-role \\ > --serviceaccount=kube-system:descheduler-sa clusterrolebinding.rbac.authorization.k8s.io/descheduler-cluster-role-binding created Ellen\u0151rz\u00e9s: oc describe clusterrolebinding/descheduler-cluster-role-binding Name: descheduler-cluster-role-binding Created: Less than a second ago Labels: <none> Annotations: <none> Role: /descheduler-cluster-role Users: <none> Groups: <none> ServiceAccounts: kube-system/descheduler-sa Subjects: <none> Verbs Non-Resource URLs Resource Names API Groups Resources [get list watch] [] [] [] [nodes] [delete get list watch] [] [] [] [pods] [create] [] [] [] [pods/eviction] Teh\u00e1t eddig minden ok. :)","title":"Cluster Role Binding"},{"location":"multi/okd/#create-config-maps","text":"A majdani job(ok) configmap(ek)b\u0151l fogj\u00e1k kiolvasni, hogy mit csin\u00e1ljanak. Itt van egy komplett p\u00e9lda: apiVersion: \"descheduler/v1alpha1\" kind: \"DeschedulerPolicy\" strategies: \"RemoveDuplicates\": enabled: false \"LowNodeUtilization\": enabled: true params: nodeResourceUtilizationThresholds: thresholds: \"cpu\" : 20 \"memory\": 20 \"pods\": 20 targetThresholds: \"cpu\" : 50 \"memory\": 50 \"pods\": 50 numberOfNodes: 3 \"RemovePodsViolatingInterPodAntiAffinity\": enabled: true ``` **H\u00e1rom strat\u00e9gia van:** * Remove duplicate pods (*RemoveDuplicates*) * Move pods to underutilized nodes (*LowNodeUtilization*) * Remove pods that violate anti-affinity rules (*RemovePodsViolatingInterPodAntiAffinity*). Ez mind a h\u00e1rom fel van sorolva a fenti p\u00e9ld\u00e1ban. ## Config Map l\u00e9trehoz\u00e1sa ```bash oc create configmap descheduler-policy-configmap-allinone \\ -n kube-system --from-file=05-allinone-configmap.yaml (Nekem a file neve: 05-allinone-configmap.yaml )","title":"Create Config Map(s)"},{"location":"multi/okd/#crete-run-job","text":"Job defin\u00edci\u00f3: apiVersion: batch/v1 kind: Job metadata: name: descheduler-job-03 namespace: kube-system spec: parallelism: 1 completions: 1 template: metadata: name: descheduler-pod annotations: scheduler.alpha.kubernetes.io/critical-pod: \"true\" spec: containers: - name: descheduler image: itdevarepotst.vodafone.hu/docker-remote-quay/openshift/origin-descheduler:v3.11 volumeMounts: - mountPath: /policy-dir name: policy-volume command: - \"/bin/sh\" - \"-ec\" - | /bin/descheduler --policy-config-file /policy-dir/05-allinone-configmap.yaml restartPolicy: \"Never\" serviceAccountName: descheduler-sa volumes: - name: policy-volume configMap: name: descheduler-policy-configmap-allinone Egy kis magyar\u00e1zkod\u00e1s: * scheduler.alpha.kubernetes.io/critical-pod: \"true\" It is important to note that there are a number of core components, such as DNS, that are critical to a fully functional cluster, but, run on a regular cluster node rather than the master. A cluster may stop working properly if the component is evicted. To prevent the descheduler from removing these pods, configure the pod as a critical pod by adding the scheduler.alpha.kubernetes.io/critical-pod annotation to the pod specification. image: itdevarepotst.vodafone.hu/docker-remote-quay/openshift/origin-descheduler:v3.11 \u00c9n itt tal\u00e1ltam meg a descheduler docker image-t. Sereg\u00e9ly D\u00e1vid megtal\u00e1lta egy m\u00e1sik helyen is: https://microbadger.com/images/openshift/origin-descheduler:v3.11.0 Val\u00f3sz\u00edn\u00e1leg m\u00e9g t\u00f6bb helyen is meg lehet tal\u00e1lni. :) A teszt Artifactory-ban a docker-remote-quay egy remote repo, ami ide mutat: https://quay.io name: descheduler-policy-configmap-lownodeutilization \u00e9s --policy-config-file /policy-dir/04-test-configmap.yaml Ezek mindkettetten a Configmap-b\u0151l j\u00f6nnek. Itt, \u00edgy lehet megtal\u00e1lni (Name, Data) : Name: descheduler-policy-configmap-allinone Namespace: kube-system Labels: <none> Annotations: <none> Data ==== 05-allinone-configmap.yaml: ---- apiVersion: \"descheduler/v1alpha1\" kind: \"DeschedulerPolicy\" strategies: \"RemoveDuplicates\": enabled: false \"LowNodeUtilization\": enabled: true params: nodeResourceUtilizationThresholds: thresholds: \"cpu\" : 20 \"memory\": 20 \"pods\": 20 targetThresholds: \"cpu\" : 50 \"memory\": 50 \"pods\": 50 numberOfNodes: 3 \"RemovePodsViolatingInterPodAntiAffinity\": enabled: true Events: <none> Run Job: Ezt m\u00e1r a legegyszer\u0171bb r\u00e9sze, vagy oc apply vagy oc create parancs. oc apply -f 03-CreateJob.yaml Ellen\u0151rz\u00e9s: oc get job NAME DESIRED SUCCESSFUL AGE descheduler-job-03 1 1 9s Ez a szerencs\u00e9tlen sajnos semmi logot nem \u00edr: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc logs job/descheduler-job-03 janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc logs pod/descheduler-job-03-8f5hb janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ \u00cdgy ellen\u0151rizni csak a describe vagy oc get events paranccsal lehet. \u00dajrafuttatni a job-ot \u00edgy lehet: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc delete -f 03-CreateJob.yaml job.batch \"descheduler-job-03\" deleted janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc apply -f 03-CreateJob.yaml job.batch/descheduler-job-03 created janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ Vagy a job defin\u00edci\u00f3s file-be \u00e1t\u00edrod a nev\u00e9t: apiVersion: batch/v1 kind: Job metadata: name: descheduler-job-03 namespace: kube-system spec: Pl. descheduler-job-03 --> descheduler-job-04 \u00c9s \u00edgy m\u00e1r k\u00e9t job lesz SUCCESSFUL state-ben: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get jobs NAME DESIRED SUCCESSFUL AGE descheduler-job-03 1 1 1m descheduler-job-04 1 1 26s","title":"Crete (run) Job"},{"location":"multi/okd/#teszteles","text":"Kettes l\u00e1bat kiszedtem a forgalomb\u00f3l: oc adm manage-node itdevkubtstapp2.vodafone.hu --schedulable=false \u00c9s evaku\u00e1ltam: oc adm manage-node itdevkubtstapp2.vodafone.hu --evacuate Parancs kimenete: evaculate.txt \u00cdgy gyakorlatilag nem maradt rajta POD: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE app-storage glusterfs-storage-nzshm 0/1 Pending 0 3s <none> itdevkubtstapp2.vodafone.hu <none> application-monitoring prom-0 3/4 CrashLoopBackOff 64 5h 192.168.1.103 itdevkubtstapp2.vodafone.hu <none> ent-arch-dbms mysql-master-6-r57qm 0/1 Terminating 327 1d 192.168.1.98 itdevkubtstapp2.vodafone.hu <none> microserviceproject microservice-kitchen-sin-10-fcj2q 1/1 Terminating 0 1d 192.168.1.39 itdevkubtstapp2.vodafone.hu <none> openshift-logging logging-fluentd-g455f 0/1 ContainerCreating 0 31s <none> itdevkubtstapp2.vodafone.hu <none> openshift-monitoring node-exporter-zb7cv 0/2 ContainerCreating 0 29s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> openshift-node sync-c6smk 0/1 ContainerCreating 0 39s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> openshift-sdn ovs-pfcvz 0/1 ContainerCreating 0 34s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> openshift-sdn sdn-nrd5j 0/1 ContainerCreating 0 33s 172.17.64.45 itdevkubtstapp2.vodafone.hu <none> Ezut\u00e1n vissza\u00e1ll\u00edtottam a \"schedulable\" flag-et: oc adm manage-node itdevkubtstapp2.vodafone.hu --schedulable=true Egy\u00e9bk\u00e9nt ellen\u0151rizni \u00edgy (is) lehet: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc describe node/itdevkubtstapp2.vodafone.hu | grep Unschedulable Unschedulable: true \u00c9rdemes figyelni mert ellent\u00e9tes a k\u00e9t cuccos: schedulable <--> Unschedulable Ezzel a be\u00e1ll\u00edt\u00e1ssal teszteltem: apiVersion: \"descheduler/v1alpha1\" kind: \"DeschedulerPolicy\" strategies: \"RemoveDuplicates\": enabled: false \"LowNodeUtilization\": enabled: true params: nodeResourceUtilizationThresholds: thresholds: \"pods\": 20 targetThresholds: \"pods\": 50 \"RemovePodsViolatingInterPodAntiAffinity\": enabled: true \u00c9s itt l\u00e1tni, hogy mi t\u00f6rt\u00e9nt: janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu | wc -l 15 janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc delete cm/descheduler-policy-configmap-allinone configmap \"descheduler-policy-configmap-allinone\" deleted janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc create configmap descheduler-policy-configmap-allinone -n kube-system --from-file=05-allinone-configmap.yaml configmap/descheduler-policy-configmap-allinone created janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc delete -f 03-CreateJob.yaml job.batch \"descheduler-job-04\" deleted janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc apply -f 03-CreateJob.yaml job.batch/descheduler-job-04 created janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu | wc -l 21 janos.vincze@MCC013625:/storage/janos.vincze/x-Temp/20200317/Guide$ oc get pods --all-namespaces -o wide --field-selector=spec.nodeName=itdevkubtstapp2.vodafone.hu | wc -l 21 L\u00e1tni, hogy a job fut\u00e1sa el\u0151tt csak ~15 POD volt rajta, ut\u00e1na pedig ~21 darab.","title":"Tesztel\u00e9s"},{"location":"old/Collect_Network_Statistic_With_Telegraf_Vnstat/","text":"Collect Network Statistic With Telegraf & VNSTAT \u00b6 Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! I use Telegraf on various hosts without any problem, but in some cases I'm facing issues using sysstat plugin on Orange PI zeros. One of the most important thing for me to collect network bandwidth statistic. For this sysstat plugin is perfect, but how to achieve this without it? I was thinking a bit, and found out that with exec plugin and vnstat I can gather information about bandwidth. Here is the configuration: [[inputs.exec]] commands = [ \"/usr/bin/vnstat -i eth0 -tr --short --json\", \"/usr/bin/vnstat -i tun0 -tr --short --json\" ] timeout = \"10s\" name_suffix = \"_vnstat\" data_format = \"json\" json_name_key=\"vnstat\" tag_keys= [\"interface\"] References: Input Data Formats JSON Example InfluxDB Commands \u00b6 List avaiable hosts: SHOW TAG VALUES ON telegraf from \"system\" WITH KEY = \"host\" Show MEASUREMENTS SHOW MEASUREMENTS WITH MEASUREMENT =~ /exec.*/ Output: name: measurements name ---- exec_vnstat List Series: SHOW SERIES ON telegraf FROM exec_vnstat; Output: key --- exec_vnstat,dc=barber,host=*****-opi0,interface=eth0,rack=opi0 exec_vnstat,dc=barber,host=*****-opi0,interface=tun0,rack=opi0","title":"Collect Network Statistic With Telegraf & VNSTAT"},{"location":"old/Collect_Network_Statistic_With_Telegraf_Vnstat/#collect-network-statistic-with-telegraf-vnstat","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! I use Telegraf on various hosts without any problem, but in some cases I'm facing issues using sysstat plugin on Orange PI zeros. One of the most important thing for me to collect network bandwidth statistic. For this sysstat plugin is perfect, but how to achieve this without it? I was thinking a bit, and found out that with exec plugin and vnstat I can gather information about bandwidth. Here is the configuration: [[inputs.exec]] commands = [ \"/usr/bin/vnstat -i eth0 -tr --short --json\", \"/usr/bin/vnstat -i tun0 -tr --short --json\" ] timeout = \"10s\" name_suffix = \"_vnstat\" data_format = \"json\" json_name_key=\"vnstat\" tag_keys= [\"interface\"] References: Input Data Formats JSON","title":"Collect Network Statistic With Telegraf &amp; VNSTAT"},{"location":"old/Collect_Network_Statistic_With_Telegraf_Vnstat/#example-influxdb-commands","text":"List avaiable hosts: SHOW TAG VALUES ON telegraf from \"system\" WITH KEY = \"host\" Show MEASUREMENTS SHOW MEASUREMENTS WITH MEASUREMENT =~ /exec.*/ Output: name: measurements name ---- exec_vnstat List Series: SHOW SERIES ON telegraf FROM exec_vnstat; Output: key --- exec_vnstat,dc=barber,host=*****-opi0,interface=eth0,rack=opi0 exec_vnstat,dc=barber,host=*****-opi0,interface=tun0,rack=opi0","title":"Example InfluxDB Commands"},{"location":"old/Iptables_Examples/","text":"Iptables Examples \u00b6 Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Clear All Rules \u00b6 The following commands will completely clear all your rules (and ACCEPT everything). iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT iptables -t nat -F iptables -t mangle -F iptables -F iptables -X Simple NAT \u00b6 Situation: There are some ISPs in Hungary (and all over the world) which use Carrier-grade NAT and this \"feature\" makes my life harder. Devices and Networks: Router IP address: 192.168.100.1 , Network: 192.168.100.0/24 NVR (Network Video Recorder) It connects to the ISP's router. IP address: 192.168.100.4 But it has its own network for IP cameras. (3 eth port and WiFi) IP address: 172.20.18.4 , Network: 172.20.18.0/24 Orange PI zero eth0 - Connected to the router. ( 192.168.100.230 ) tun0 - Connected to the OpenVPN server. ( 10.50.0.230 ) wlan0 - Connected directly to the NVR over WiFi. ( 172.20.18.0.6 ) For example 3 IP cameras: 172.20.18.3 172.20.18.4 172.20.18.5 Picture: Mission: Access the NVR and the cameras over the VPN network. So there are three different network: 192.168.100.0/24 10.50.0.0./16 172.20.18.0/24 First we have to determine on which ports the NVR listens. nmap 192.168.100.4 Starting Nmap 7.40 ( https://nmap.org ) at 2018-11-07 10:30 UTC Nmap scan report for 192.168.100.4 Host is up (0.0015s latency). Not shown: 995 closed ports PORT STATE SERVICE 53/tcp open domain 80/tcp open http 554/tcp open rtsp 5000/tcp open upnp 8888/tcp open sun-answerbook MAC Address: 08:EA:40:56:95:EB (Shenzhen Bilian Electronicltd) Nmap done: 1 IP address (1 host up) scanned in 2.34 seconds Solution: iptables -A FORWARD -i tun0 -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 192.168.100.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 192.168.100.4:554 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 8888 -j DNAT --to 192.168.100.4:8888 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 5000 -j DNAT --to 192.168.100.4:5000 It is very important to enable IP(v4) forwarding. We can enable it temporary: # check: cat /proc/sys/net/ipv4/ip_forward echo -n 1 >/proc/sys/net/ipv4/ip_forward Or permanently, by adding the following line to /etc/sysctl.conf file: net.ipv4.ip_forward=1 The whole script: #!/bin/bash IFS=' ' iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT iptables -t nat -F iptables -t mangle -F iptables -F iptables -X echo -n 1 > /proc/sys/net/ipv4/ip_forward iptables -A FORWARD -i tun0 -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 192.168.100.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 192.168.100.4:554 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 8888 -j DNAT --to 192.168.100.4:8888 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 5000 -j DNAT --to 192.168.100.4:5000 Important This will enable all traffic, and completely turn the firewall off. NOT recommended if you do not have any other firewall in your network and/or your device has public IP address. And what about the cameras? Rules for a camera: iptables -A FORWARD -i tun0 -j ACCEPT iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 172.20.18.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 172.20.18.4:554 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34567 -j DNAT --to 172.20.18.4:34567 I have never tried but it may be possible to configure iptables to access all cameras without reconfigure our rules for each camera. Example: iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 172.20.18.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 81 -j DNAT --to 172.20.18.3:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 82 -j DNAT --to 172.20.18.5:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34567 -j DNAT --to 172.20.18.4:34567 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34568 -j DNAT --to 172.20.18.3:34568 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34569 -j DNAT --to 172.20.18.5:34569 In order to work these rule perfectly the listen ports have to be (re)configured on the IP camera side as well. But in my scenario it is not so important, because I don't need frequently access directly the cameras. Permanently Save Iptables rules (on debian(-like) OS) \u00b6 Save the current configuration iptables-save > /etc/iptables.rules Restore the saved configuration from file iptables-restore < /etc/iptables.rules For apply iptables rules on startup use this little script: cat <<EOF>/etc/network/if-pre-up.d/firewall #!/bin/bash /sbin/iptables-restore < /etc/iptables.rules EOF chmod +x /etc/network/if-pre-up.d/firewall Port Forwarding To Another Host \u00b6 My scenario: I wanted to access RTSP port (554) of multiple IP cameras over one host (Gateway). Host IP address: 172.16.0.230 IP adress of cameras: 172.19.1.1 - 172.19.1.8 RTSP listen port: 554 Rules: iptables -t nat -A PREROUTING -p tcp --dport 5541 -j DNAT --to-destination 172.19.1.1:554 iptables -t nat -A POSTROUTING -p tcp -d 172.19.1.1 --dport 554 -j SNAT --to-source 172.16.0.230 iptables -t nat -A PREROUTING -p tcp --dport 5542 -j DNAT --to-destination 172.19.1.2:554 iptables -t nat -A POSTROUTING -p tcp -d 172.19.1.2 --dport 554 -j SNAT --to-source 172.16.0.230 iptables -t nat -A PREROUTING -p tcp --dport 5543 -j DNAT --to-destination 172.19.1.3:554 iptables -t nat -A POSTROUTING -p tcp -d 172.19.1.3 --dport 554 -j SNAT --to-source 172.16.0.230 etc. The script with traffic monitoring: #!/bin/bash IFS=' ' iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT iptables -t nat -F iptables -t mangle -F iptables -F iptables -X echo -n 1 > /proc/sys/net/ipv4/ip_forward iptables -A INPUT -p tcp -m state --state NEW -j LOG --log-prefix \"INPUT: \" iptables -A OUTPUT -p tcp -m state --state NEW -j LOG --log-prefix \"OUTPUT: \" iptables -t nat -A PREROUTING -p tcp -j LOG --log-prefix \"PREROUTING: \" iptables -t nat -A POSTROUTING -p tcp -j LOG --log-prefix \"POSTROUTING: \" ### ## Traffic ### iptables -N trafficmon iptables -A FORWARD -p tcp --sport 554 -m comment --comment \"ALL\" -j trafficmon LOCAL_IP=\"172.16.0.230\" LISTEN=1554 # NAME | IP | PORT | URL CAMS[0]=\"Pool|172.19.1.1|554|ucast/11\" CAMS[1]=\"Garden|172.19.1.2|554|11\" CAMS[2]=\"Garage|172.19.1.3|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[3]=\"Workshop|172.19.1.4|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[4]=\"Backyard|172.19.1.5|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[5]=\"Gate|172.19.1.6|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[6]=\"Street|172.19.1.7|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[7]=\"Corridor|172.19.1.8|554|11\" for CAM in ${CAMS[@]} do NAME=$( echo $CAM | cut -f 1 -d\"|\" ) IPADDR=$( echo $CAM | cut -f 2 -d\"|\" ) PORT=$( echo $CAM | cut -f 3 -d\"|\" ) URL=$( echo $CAM | cut -f 4 -d\"|\" ) echo \"URL : rtsp://$LOCAL_IP:$LISTEN/$URL (### $NAME ###)\" iptables -t nat -A PREROUTING -p tcp --dport $LISTEN -j DNAT --to-destination $IPADDR:$PORT -m comment --comment \"$NAME\" iptables -t nat -A POSTROUTING -p tcp -d $IPADDR --dport $PORT -j SNAT --to-source $LOCAL_IP -m comment --comment \"$NAME\" iptables -A FORWARD -p tcp --sport 554 --source $IPADDR -m comment --comment \"$NAME\" -j trafficmon (( LISTEN ++ )) done cat <<EOF Please Run iptables-save > /etc/iptables.rules to make rules permanent! EOF IPTABLES quick commands & Cheat Sheets \u00b6 manage chain: # iptables -N new_chain // create a chain # iptables -E new_chain old_chain // edit a chain # iptables -X old_chain // delete a chain redirecting packet to a user chain: # iptables -A INPUT -p icmp -j new_chain listing rules: # iptables -L // list all rules of all tables # iptables -L -v // display rules and their counters # iptables -L -t nat // display rules for a specific tables # iptables -L -n --line-numbers // listing rules with line number for all tables # iptables -L INPUT -n --line-numbers // listing rules with line number for specific table manage rules: # iptables -A chain // append rules to the bottom of the chain # iptables -I chain [rulenum] // insert in chain as rulenum (default at the top or 1) # iptables -R chain rulenum // replace rules with rules specified for the rulnum # iptables -D chain rulenum // delete rules matching rulenum (default 1) # iptables -D chain // delete matching rules change default policy: # iptables -P chain target // change policy on chain to target # iptables -P INPUT DROP // change INPUT table policy to DROP # iptables -P OUTPUT DROP // change OUTPUT chain policy to DROP # iptables -P FORWARD DROP // change FORWARD chain policy to DROP Reference: iptables-quick-command-list/","title":"Iptables Examples"},{"location":"old/Iptables_Examples/#iptables-examples","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!","title":"Iptables Examples"},{"location":"old/Iptables_Examples/#clear-all-rules","text":"The following commands will completely clear all your rules (and ACCEPT everything). iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT iptables -t nat -F iptables -t mangle -F iptables -F iptables -X","title":"Clear All Rules"},{"location":"old/Iptables_Examples/#simple-nat","text":"Situation: There are some ISPs in Hungary (and all over the world) which use Carrier-grade NAT and this \"feature\" makes my life harder. Devices and Networks: Router IP address: 192.168.100.1 , Network: 192.168.100.0/24 NVR (Network Video Recorder) It connects to the ISP's router. IP address: 192.168.100.4 But it has its own network for IP cameras. (3 eth port and WiFi) IP address: 172.20.18.4 , Network: 172.20.18.0/24 Orange PI zero eth0 - Connected to the router. ( 192.168.100.230 ) tun0 - Connected to the OpenVPN server. ( 10.50.0.230 ) wlan0 - Connected directly to the NVR over WiFi. ( 172.20.18.0.6 ) For example 3 IP cameras: 172.20.18.3 172.20.18.4 172.20.18.5 Picture: Mission: Access the NVR and the cameras over the VPN network. So there are three different network: 192.168.100.0/24 10.50.0.0./16 172.20.18.0/24 First we have to determine on which ports the NVR listens. nmap 192.168.100.4 Starting Nmap 7.40 ( https://nmap.org ) at 2018-11-07 10:30 UTC Nmap scan report for 192.168.100.4 Host is up (0.0015s latency). Not shown: 995 closed ports PORT STATE SERVICE 53/tcp open domain 80/tcp open http 554/tcp open rtsp 5000/tcp open upnp 8888/tcp open sun-answerbook MAC Address: 08:EA:40:56:95:EB (Shenzhen Bilian Electronicltd) Nmap done: 1 IP address (1 host up) scanned in 2.34 seconds Solution: iptables -A FORWARD -i tun0 -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 192.168.100.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 192.168.100.4:554 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 8888 -j DNAT --to 192.168.100.4:8888 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 5000 -j DNAT --to 192.168.100.4:5000 It is very important to enable IP(v4) forwarding. We can enable it temporary: # check: cat /proc/sys/net/ipv4/ip_forward echo -n 1 >/proc/sys/net/ipv4/ip_forward Or permanently, by adding the following line to /etc/sysctl.conf file: net.ipv4.ip_forward=1 The whole script: #!/bin/bash IFS=' ' iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT iptables -t nat -F iptables -t mangle -F iptables -F iptables -X echo -n 1 > /proc/sys/net/ipv4/ip_forward iptables -A FORWARD -i tun0 -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 192.168.100.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 192.168.100.4:554 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 8888 -j DNAT --to 192.168.100.4:8888 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 5000 -j DNAT --to 192.168.100.4:5000 Important This will enable all traffic, and completely turn the firewall off. NOT recommended if you do not have any other firewall in your network and/or your device has public IP address. And what about the cameras? Rules for a camera: iptables -A FORWARD -i tun0 -j ACCEPT iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 172.20.18.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 172.20.18.4:554 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34567 -j DNAT --to 172.20.18.4:34567 I have never tried but it may be possible to configure iptables to access all cameras without reconfigure our rules for each camera. Example: iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 172.20.18.4:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 81 -j DNAT --to 172.20.18.3:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 82 -j DNAT --to 172.20.18.5:80 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34567 -j DNAT --to 172.20.18.4:34567 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34568 -j DNAT --to 172.20.18.3:34568 iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34569 -j DNAT --to 172.20.18.5:34569 In order to work these rule perfectly the listen ports have to be (re)configured on the IP camera side as well. But in my scenario it is not so important, because I don't need frequently access directly the cameras.","title":"Simple NAT"},{"location":"old/Iptables_Examples/#permanently-save-iptables-rules-on-debian-like-os","text":"Save the current configuration iptables-save > /etc/iptables.rules Restore the saved configuration from file iptables-restore < /etc/iptables.rules For apply iptables rules on startup use this little script: cat <<EOF>/etc/network/if-pre-up.d/firewall #!/bin/bash /sbin/iptables-restore < /etc/iptables.rules EOF chmod +x /etc/network/if-pre-up.d/firewall","title":"Permanently Save Iptables rules (on debian(-like) OS)"},{"location":"old/Iptables_Examples/#port-forwarding-to-another-host","text":"My scenario: I wanted to access RTSP port (554) of multiple IP cameras over one host (Gateway). Host IP address: 172.16.0.230 IP adress of cameras: 172.19.1.1 - 172.19.1.8 RTSP listen port: 554 Rules: iptables -t nat -A PREROUTING -p tcp --dport 5541 -j DNAT --to-destination 172.19.1.1:554 iptables -t nat -A POSTROUTING -p tcp -d 172.19.1.1 --dport 554 -j SNAT --to-source 172.16.0.230 iptables -t nat -A PREROUTING -p tcp --dport 5542 -j DNAT --to-destination 172.19.1.2:554 iptables -t nat -A POSTROUTING -p tcp -d 172.19.1.2 --dport 554 -j SNAT --to-source 172.16.0.230 iptables -t nat -A PREROUTING -p tcp --dport 5543 -j DNAT --to-destination 172.19.1.3:554 iptables -t nat -A POSTROUTING -p tcp -d 172.19.1.3 --dport 554 -j SNAT --to-source 172.16.0.230 etc. The script with traffic monitoring: #!/bin/bash IFS=' ' iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT iptables -t nat -F iptables -t mangle -F iptables -F iptables -X echo -n 1 > /proc/sys/net/ipv4/ip_forward iptables -A INPUT -p tcp -m state --state NEW -j LOG --log-prefix \"INPUT: \" iptables -A OUTPUT -p tcp -m state --state NEW -j LOG --log-prefix \"OUTPUT: \" iptables -t nat -A PREROUTING -p tcp -j LOG --log-prefix \"PREROUTING: \" iptables -t nat -A POSTROUTING -p tcp -j LOG --log-prefix \"POSTROUTING: \" ### ## Traffic ### iptables -N trafficmon iptables -A FORWARD -p tcp --sport 554 -m comment --comment \"ALL\" -j trafficmon LOCAL_IP=\"172.16.0.230\" LISTEN=1554 # NAME | IP | PORT | URL CAMS[0]=\"Pool|172.19.1.1|554|ucast/11\" CAMS[1]=\"Garden|172.19.1.2|554|11\" CAMS[2]=\"Garage|172.19.1.3|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[3]=\"Workshop|172.19.1.4|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[4]=\"Backyard|172.19.1.5|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[5]=\"Gate|172.19.1.6|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[6]=\"Street|172.19.1.7|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\" CAMS[7]=\"Corridor|172.19.1.8|554|11\" for CAM in ${CAMS[@]} do NAME=$( echo $CAM | cut -f 1 -d\"|\" ) IPADDR=$( echo $CAM | cut -f 2 -d\"|\" ) PORT=$( echo $CAM | cut -f 3 -d\"|\" ) URL=$( echo $CAM | cut -f 4 -d\"|\" ) echo \"URL : rtsp://$LOCAL_IP:$LISTEN/$URL (### $NAME ###)\" iptables -t nat -A PREROUTING -p tcp --dport $LISTEN -j DNAT --to-destination $IPADDR:$PORT -m comment --comment \"$NAME\" iptables -t nat -A POSTROUTING -p tcp -d $IPADDR --dport $PORT -j SNAT --to-source $LOCAL_IP -m comment --comment \"$NAME\" iptables -A FORWARD -p tcp --sport 554 --source $IPADDR -m comment --comment \"$NAME\" -j trafficmon (( LISTEN ++ )) done cat <<EOF Please Run iptables-save > /etc/iptables.rules to make rules permanent! EOF","title":"Port Forwarding To Another Host"},{"location":"old/Iptables_Examples/#iptables-quick-commands-cheat-sheets","text":"manage chain: # iptables -N new_chain // create a chain # iptables -E new_chain old_chain // edit a chain # iptables -X old_chain // delete a chain redirecting packet to a user chain: # iptables -A INPUT -p icmp -j new_chain listing rules: # iptables -L // list all rules of all tables # iptables -L -v // display rules and their counters # iptables -L -t nat // display rules for a specific tables # iptables -L -n --line-numbers // listing rules with line number for all tables # iptables -L INPUT -n --line-numbers // listing rules with line number for specific table manage rules: # iptables -A chain // append rules to the bottom of the chain # iptables -I chain [rulenum] // insert in chain as rulenum (default at the top or 1) # iptables -R chain rulenum // replace rules with rules specified for the rulnum # iptables -D chain rulenum // delete rules matching rulenum (default 1) # iptables -D chain // delete matching rules change default policy: # iptables -P chain target // change policy on chain to target # iptables -P INPUT DROP // change INPUT table policy to DROP # iptables -P OUTPUT DROP // change OUTPUT chain policy to DROP # iptables -P FORWARD DROP // change FORWARD chain policy to DROP Reference: iptables-quick-command-list/","title":"IPTABLES quick commands &amp; Cheat Sheets"},{"location":"old/Nokia_6120c_%28bb5%29_Forgotten_Security_Code/","text":"Nokia 6120c (BB5) Forgotten Security Code \u00b6 Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Maybe this post is a bit outdated, but still can be useful whose wants to bring back an old Nokia phone to life. My story is very simple and usual. I've forgotten my security code of an old Nokia 6120c. The key problem was that I locked my phone, and could not turn it on. It always asked for the security code, and after 5 tries locked for 5 minutes, thus brute force would have been a bit time-consuming. Oh! Why I wanted to use such an old phone? I had to carry my iPhone to the service and was thinking about going back in time and using one of my really old phone during I'm waiting to get my iPhone back, so I chose my old Nokia 6120c. I had started googling and found some article about how to unlock BB5 phones. Unfortunately there are a lot of article which can lead you to the wrong way. For example generating master unlock code is not possible for bb5 phones, and please aware of downloading software from unknown sources. NOTE: Resetting your phone won't set your security code to its original (12345)! By the way, you can hard reset your phone by following these easy steps: Turn OFF your phone. Press and hold \"Green phone\" + \"3\" + \"*\" button. Press and hold the power button while the phone is turned on. After displaying the \"Nokia\" message and the phone turned on (or asking for security code, if it is locked) you can release the mentioned button in the 2. step. REFERENCE Link: http://forum.gsmhosting.com/vbb/f299/get-your-phone-lock-code-security-code-without-reset-format-nokia-bb5-phones-teste-667948/ It the post linked below you can find everything you need to successfully retrieve your security code. Here I want to give you a \"real\" step by step guide with screenshots and examples. What will you need? A locked phone with forgotten security code (You don't really need this, you can do it as hobby, as well :) ) An USB cable An ~5K resistor Installed NSS ( Nemesis Service Suite 1.0.38.15) During the install please choose the \"Virtual USB device\" . Download link: Installed \" Nokia Suite \" or \" Nokia Connectivity Cable Driver \" (Optional) Charger First you have to connect the 5K resistor between BSI (middle) and GNS (\"-\") pins of the battery connector. DO NOT short circuit your battery by connecting the \"+\" and \"-\" to each other even through the resistor! \u00b6 Next turn your phone on. You have to see something like this: After that you should connect the phone to your computer with the mini USB cable and start NSS. Click on the magnifier button on the top right corner: Click on the Device Info, then the Scan button: Finally click on the \"Permanent Memory\" tab, and \"Read\" button: As the last step a .pm file will be created in this directory: \"c:\\Program Files (x86)\\NSS\\Backup\\pm\\\". The file name have to be the imei number of your phone. Example: \"358640012938846.pm\" Look for the line beginning with \"5=\", in my case this is the 218. line. Example: 5=38343634350000000000 Remove every second \"3\" digit and the trailing zeros, the remaining is your security code ( 84645 ). That's all! :)","title":"Nokia 6120c (BB5) Forgotten Security Code"},{"location":"old/Nokia_6120c_%28bb5%29_Forgotten_Security_Code/#nokia-6120c-bb5-forgotten-security-code","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Maybe this post is a bit outdated, but still can be useful whose wants to bring back an old Nokia phone to life. My story is very simple and usual. I've forgotten my security code of an old Nokia 6120c. The key problem was that I locked my phone, and could not turn it on. It always asked for the security code, and after 5 tries locked for 5 minutes, thus brute force would have been a bit time-consuming. Oh! Why I wanted to use such an old phone? I had to carry my iPhone to the service and was thinking about going back in time and using one of my really old phone during I'm waiting to get my iPhone back, so I chose my old Nokia 6120c. I had started googling and found some article about how to unlock BB5 phones. Unfortunately there are a lot of article which can lead you to the wrong way. For example generating master unlock code is not possible for bb5 phones, and please aware of downloading software from unknown sources. NOTE: Resetting your phone won't set your security code to its original (12345)! By the way, you can hard reset your phone by following these easy steps: Turn OFF your phone. Press and hold \"Green phone\" + \"3\" + \"*\" button. Press and hold the power button while the phone is turned on. After displaying the \"Nokia\" message and the phone turned on (or asking for security code, if it is locked) you can release the mentioned button in the 2. step. REFERENCE Link: http://forum.gsmhosting.com/vbb/f299/get-your-phone-lock-code-security-code-without-reset-format-nokia-bb5-phones-teste-667948/ It the post linked below you can find everything you need to successfully retrieve your security code. Here I want to give you a \"real\" step by step guide with screenshots and examples. What will you need? A locked phone with forgotten security code (You don't really need this, you can do it as hobby, as well :) ) An USB cable An ~5K resistor Installed NSS ( Nemesis Service Suite 1.0.38.15) During the install please choose the \"Virtual USB device\" . Download link: Installed \" Nokia Suite \" or \" Nokia Connectivity Cable Driver \" (Optional) Charger First you have to connect the 5K resistor between BSI (middle) and GNS (\"-\") pins of the battery connector. DO NOT short circuit your battery by connecting the \"+\" and \"-\" to each other even through the resistor!","title":"Nokia 6120c (BB5) Forgotten Security Code"},{"location":"old/Nokia_6120c_%28bb5%29_Forgotten_Security_Code/#_1","text":"Next turn your phone on. You have to see something like this: After that you should connect the phone to your computer with the mini USB cable and start NSS. Click on the magnifier button on the top right corner: Click on the Device Info, then the Scan button: Finally click on the \"Permanent Memory\" tab, and \"Read\" button: As the last step a .pm file will be created in this directory: \"c:\\Program Files (x86)\\NSS\\Backup\\pm\\\". The file name have to be the imei number of your phone. Example: \"358640012938846.pm\" Look for the line beginning with \"5=\", in my case this is the 218. line. Example: 5=38343634350000000000 Remove every second \"3\" digit and the trailing zeros, the remaining is your security code ( 84645 ). That's all! :)","title":""},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/","text":"Sonoff Relays With OpenHab And Tasmota Firmware \u00b6 Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! TL;DR \u00b6 I'm using OpenHab to control some Lights and equipment in my house and garden. I used to develop my own circuits using ESPs, relays, power supplies etc., but some months ago I found the Sonoff products, which offer the same or more functions than my own devices. Just for comparison here is my 4ch relay with ESP07: And the 4CH relay from Sonoff: However, all my own relays and sensors work fine, I had to develop a firmware for them. Since I'm not a not developer it was a big challenge for me, even with the LUA FW. So I decided to order some Sonoff products and tried them. For the very first time I tried with its factory firmware, but it has a lot of limitations: It cannot be used with OpenHab. This is a show stopper disadvantage for me. It has no web interface. Very limited scheduler. (No sunset / sunrise option, etc) Scenes cannot be shared. Uses the sonoff own cloud infrastructure. That's why nobody knows what kind of information is sent to the sonoff servers. (The big brother is always watching you. :) ) I started Googling and found the Tasmota firmware easily, which is for Sonoff products. It is available on Github: Sonoff-Tasmota There are a lot of excellent articles on the Internet which are about How to update your sonoff product with Tasmota firmware, so mine is something like \"Yet, another article about....\" :) So, you need: One supported sonoff product. (You can find the supported device list on the official Tasmota Github page.) 4 solder-able pins. (For GND, RxD, TxD and Vcc) Soldering Station. (And willingness to solder of course.) USB to TTL converter. A PC or notebook. Screwdriver. Before we start, let's say some words about Sonoffs. I'v ordered 4 different types of relays: S2 smart Socket, POW (One ch relay with power monitoring capability), Sonoff basic and Sonoff 4ch. All of them are working perfectly with Tasmota. You can visit the sonoff store on Aliexpress and I bet you will be surprised at how cheap these things are. At the moment the cheapest relay is the sonoff basic, it costs only 5$, and you can get a full system for switching things around your house, garden, holiday house even if you are far away. What about the quality? So-so. For this price I think the quality is acceptable, but not the best. For example the product sheet of the Basic says that the maximum load is 2200W. Seeing the soldering inside the cover I don't think it can bear 2.2kW. But from the other hand there is not a lot of equipment which we want to control over the internet and consumes 2.2kW. In my case I want to control some lights, fans and my water pump with these relays, so sonoff relays are perfectly suitable for my goal. 1. Soldering \u00b6 As I wrote above I have 4 types of Sonoff relays. Thanks to God the most expensive one (4ch) have pre-soldered pins on it. Helpful links: Hardware-Preparation Sonoff-4CH-and-4CH-Pro Please be aware (4ch) that \"The printed labels on the PCB for Rx and Tx are incorrectly swapped as can be seen on the image.\" 1.1. Sonoff POW \u00b6 1.2. Sonoff S20 (Socket) \u00b6 1.3. Basic \u00b6 The GPIO14 can be used for example to connect DHT22 sensor (or any other sensor supported by Tasmota) to the sonoff. I will assist you later how to configure Tasmota to be able to use additional sensor. The button on sonoffs are always connected to GPIO0, except when the board has more then one button (eg.: 4ch), in this case the 1st button is connected to GPIO0. 2. Upload The Firmware \u00b6 To be honest this is the easiest step. Danger Do not connect AC power and the serial connection at the same time! Connection Matrix: USB 2 TTL <--> Sonoff GND <--> GND TxD <--> RxD RxD <--> TxD VCC <--> VCC In order to put your device into FW upgrade mode, press and hold the button on it while connecting to VCC. You can connect all the wires and press the button when you connect the USB2TTL to your computer, or leave the VCC (only the vcc) disconnected, connect the USB2TTL to the computer, press and hold the button and connect the VCC. Useful links: upgrading-sonoff-stock-firmware-to-sonoff-tasmota-usb-to-serial-and-ota-update-methods https://github.com/arendst/Sonoff-Tasmota/wiki/Esptool 2.1. (Optional) Take Backup \u00b6 Taking backup is always optional, but essential. If you skip this step, you lose the possibility to restore the factory firmware. (It is not 100% true because I will make the stock firmware available here.) You will need Esptool to achieve this step. If you don't know how to install it, please take a look at this page: esptool For Windows users maybe the \"ESP8266Flasher\" could be an option ( nodemcu-flasher ). esptool.py --port /dev/ttyUSB0 read_flash 0x00000 0x100000 image1M.bin esptool.py v2.3.1 Connecting.... Detecting chip type... ESP8266 Chip is ESP8266EX Features: WiFi Uploading stub... Running stub... Stub running... 1048576 (100 %) 1048576 (100 %) Read 1048576 bytes at 0x0 in 94.9 seconds (88.4 kbit/s)... Hard resetting via RTS pin... Please be aware that after any esptool command you have to reconnect to the device. (So, you have to disconnect vcc, press and hold the button and connect VCC again.) 2.2. (Optional) Erase Flash \u00b6 This step is also optional, but recommended. esptool.py --port /dev/ttyUSB0 erase_flash esptool.py v2.3.1 Connecting.... Detecting chip type... ESP8266 Chip is ESP8266EX Features: WiFi Uploading stub... Running stub... Stub running... Erasing flash (this may take a while)... Chip erase completed successfully in 3.2s Hard resetting via RTS pin... 2.3. Flashing The Firmware \u00b6 Till this step I haven't mentioned the firmware itself. How can you download the pre-compiled firmware? You don't need to compile the firmware, it can be downloaded from github: releases The sonoff.bin has always worked for me: sonoff.bin Before you use this link, please make sure that no newer version has been released. So, let's burn the firmware: esptool.py --port /dev/ttyUSB0 write_flash -fs 1MB -fm dout 0x0 sonoff.bin esptool.py v2.3.1 Connecting.... Detecting chip type... ESP8266 Chip is ESP8266EX Features: WiFi Uploading stub... Running stub... Stub running... Configuring flash size... Compressed 539040 bytes to 368171... Wrote 539040 bytes (368171 compressed) at 0x00000000 in 32.6 seconds (effective 132.4 kbit/s)... Hash of data verified. Leaving... Hard resetting via RTS pin... That's all. You can assemble your sonoff and start using it. :) In the future chapters I'll give you some tips for configuring the devices and using them with MQTT and OpenHAB. 3. Settings \u00b6 3.1. Connect The Device To Your Local Network - WiFi Setup \u00b6 The first things to do is understand how the button with the new Tasmote FW works. Here is the link: Button-Usage 1 short press: Toggles the relay either directly or by sending a MQTT message like cmnd/sonoff/POWER1 ON. This will blink the LED twice and sends a MQTT status message like stat/sonoff/POWER1 ON. If cmnd/sonoff/ButtonRetain on has been used the MQTT message will also contain the MQTT retain flag. 2 short presses: Toggles the relay 2 if available on the device like Sonoff Dual. This will blink the LED twice and sends a MQTT status message like stat/sonoff/POWER2 on. 3 short presses: Start Wifi smartconfig allowing for SSID and Password configuration using an Android mobile phone with the ESP8266 SmartConfig app. The LED will blink during the config period. A single button press during this period will abort and restart sonoff. 4 short presses: Start Wifi manager providing an Access Point with IP address 192.168.4.1 and a web server allowing the configuration of Wifi. The LED will blink during the config period. A single button press during this period will abort and restart sonoff. 5 short presses: Start Wifi Protected Setup (WPS) allowing for SSID and Password configuration using the router's WPS button or webpage. The LED will blink during the config period. A single button press during this period will abort and restart sonoff. 6 short presses: Will restart the module 7 short presses: Start OTA download of firmware. The green LED is lit during the update Pressing the button for over forty seconds: Reset settings to defaults as defined in user_config.h and restarts the device I always used the 4 short presses option to configure WiFi. After the WiFi is successfully set up you can configure your device via your web browser using the IP address. In order to figure out the IP address use your router configuration page or console if accessible. Or you can use nmap to find IP addresses: sudo nmap -sP 172.20.1.* Example output: Starting Nmap 7.40 ( https://nmap.org ) at 2018-07-01 18:35 CEST Nmap scan report for 172.20.1.1 Host is up (0.022s latency). MAC Address: B4:E6:2D:15:82:48 (Unknown) Nmap scan report for 172.20.1.2 Host is up (0.0045s latency). MAC Address: 60:01:94:9C:65:48 (Espressif) 3.2. Configure Module \u00b6 The first thing to do after the WiFi settings is tell the Firmware which module we are using (Basic/POW/4CH/etc.). To do this open the module configuration page. Example: http://172.20.1.9/md Or simply open the main page: http://172.20.1.9/ , select \"Configuration\" then \"Configure Module\" option. Use the drop-down list to select your device type. This step is extremely important but straightforward. Without proper module selection you can't use your device features and it can lead to improper behavior. For example if you have a 4CH sonoff device, without selecting the right module you can control only the 1st channel, or with POW module you can't see the sensor data. 3.3. Configure MQTT \u00b6 Actually you can use your device via its web interface without MQTT, but one of the goals of this post is to integrate sonoff devices to OpenHab, and the best way (or the only way) to do this is use MQTT. Open the MQTT configuration page (eg.: http://172.20.1.9/mq ) Example configuration: Without manual mqtt configuration the firmware tries to connect to the mqtt server using mDNS. I've never tried this feature so I don't know if it works or not, and don't know what happens when multiple MQTT servers are available, or when user/password is needed to connect. With the configuration on the screenshot above you can subscribe to the topic and see what happens. I'm using the following syntax as topic: sonoff/[MAC ADDRESS without colon]/%prpefix% The prefix can be tele,state or cmnd. Example subscribe : mosquitto_sub -v -h 172.16.0.250 -u ****** -P ***** -t 'sonoff/B4E62D14BE5A/#' Example outputs : 1. sonoff/B4E62D14BE5A/tele/SENSOR {\"Time\":\"2018-07-01T18:57:27\",\"ENERGY\":{\"Total\":2.262,\"Yesterday\":1.330,\"Today\":0.077,\"Power\":2,\"Factor\":0.05,\"Voltage\":222,\"Current\":0.146}} 2. sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T18:57:33\",\"Uptime\":\"2T00:34:12\",\"Vcc\":3.142,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":52,\"APMac\":\"6C:3B:6B:A0:D2:79\"}} 3. sonoff/B4E62D14BE5A/stat/POWER OFF Explanation: This is a Sonoff POW module which reports power usage. You can see that the message is in JSON format. {\"Time\":\"2018-07-01T18:57:27\",\"ENERGY\":{\"Total\":2.262,\"Yesterday\":1.330,\"Today\":0.077,\"Power\":2,\"Factor\":0.05,\"Voltage\":222,\"Current\":0.146}} Tasmota firmware reports telemetry information (RSSI, Uptime, power state, etc.) periodically (in every 300s by default). This message is also in JSON format. When you turn the relay on or off (even with the web interface or via MQTT message) the device reports the new state. In this particular situation I turned the device off using the web interface. These three steps are essential to use your device with OpenHAB & MQTT. You can go through all web configuration elements/options, but this post doesn't aim to give you a complete guideline to Tasmota firmware. 3.4. Sending Commands To The Device Via MQTT \u00b6 Before you start you can check all available commands on Tasmota GitHub page: https://github.com/arendst/Sonoff-Tasmota/wiki/Commands If you are planning to deal with Tasmota firmware I recommend you to bookmark this page, it is very helpful. When you want to send commands to tasmota you always have to use the cmnd prefix. Since we are speaking about relays, maybe the first question comes up: how to ON or OFF them. Let's see the web page mentioned before (Commands): In this first example I will explain all available options to make the configuration method as clear as possible. So in a terminal tab I always subscribe to the device topic to see what happens. Command: mosquitto_sub -v -h 172.16.0.250 -u ***** -P ***** -t 'sonoff/B4E62D14BE5A/#' 3.4.1. Check The State Of The Relay: \u00b6 Command: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -n On the \"subscribe\" tab you can see these messages: sonoff/B4E62D14BE5A/cmnd/power (null) sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"} sonoff/B4E62D14BE5A/stat/POWER ON First Line: The command you sent to the device. Second Line: The result of your command. To any command tasmota relies with a \"RESULT\" message. Third Line: Status of the relay If you have for example Sonoff Dual, or Sonoff 4ch you can specify which channel you want to use. For example if you want to check the 3rd channel you can use this command: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power3 -n I'm using a POW module for introduction, it has no 3rd channel so get \"unknown\" result: sonoff/B4E62D14BE5A/cmnd/power3 (null) sonoff/B4E62D14BE5A/stat/RESULT {\"Command\":\"Unknown\"} If your device has only one channel do not use the number, so use only the \"POWER\" word instead of \"POWER1\". But in case of any multi-channels you have to use POWER1, POWER2, etc. 3.4.2. ON/OFF/TOGGLE The Relay \u00b6 Regarding relays the most important activity is to turn on and off them. Command: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m on MQTT messages: sonoff/B4E62D14BE5A/cmnd/power on sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"} sonoff/B4E62D14BE5A/stat/POWER ON Command: mosquitto_pub -h 172.16.0.250 -u vinyo -P Timike -t sonoff/B4E62D14BE5A/cmnd/power -m on MQTT messages: sonoff/B4E62D14BE5A/cmnd/power off sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"OFF\"} sonoff/B4E62D14BE5A/stat/POWER OFF As you can see in the table you can use numbers instead of command, eg: 2 / toggle. Let's fire the toggle command twice: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 2 mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 2 MQTT messages: sonoff/B4E62D14BE5A/cmnd/power 2 sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"OFF\"} sonoff/B4E62D14BE5A/stat/POWER OFF sonoff/B4E62D14BE5A/cmnd/power 2 sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"} sonoff/B4E62D14BE5A/stat/POWER ON 3.4.3. BLINK The Relay \u00b6 This is an interesting feature of Tasmota FW. But first please check the following options related to blinking. With this option the relay will turn on \"BlinkCount\" times for \"BlinkTime\" second*0.1. Check the default settings using these commands: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/BlinkCount -n mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/BlinkTime -n MQTT messages: sonoff/B4E62D14BE5A/cmnd/BlinkCount (null) sonoff/B4E62D14BE5A/stat/RESULT {\"BlinkCount\":10} sonoff/B4E62D14BE5A/cmnd/BlinkTime (null) sonoff/B4E62D14BE5A/stat/RESULT {\"BlinkTime\":10} If you connected a light to your relay, it will turn on for 1 second, 10 times. So, turn on for 1 sec, then turn off for 1 sec, turn on for 1 sec, turn on for 1 sec, and so on. After turning on the relay ten times it remains OFF. But you can terminate blinking with \"4/blinkoff\" command. Examples: Start Blinking: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 3 Terminate Blinking: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 4 Maybe the missing feature of this equipment that you cannot configure BlinkCount and BlinkTime for each relays when use device with multiple channels. In the rest of this configuration section I will show you some useful and exciting features of tasmota firmware. 3.4.4. Set Telemetry Period \u00b6 If you need the status information of your device more frequently than its default (300s) you can configure the telemetry period with the following command: mosquitto_pub -h 172.16.0.250 -u ***** -P ****** -t sonoff/B4E62D14BE5A/cmnd/TelePeriod -m 30 After runing this command tasmota will publish \"STATE\" messages in every 30 seconds: sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T20:53:09\",\"Uptime\":\"2T02:29:48\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":58,\"APMac\":\"6C:3B:6B:A0:D2:79\"}} sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T20:53:39\",\"Uptime\":\"2T02:30:18\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":58,\"APMac\":\"6C:3B:6B:A0:D2:79\"}} Times: 2018-07-01T20:53:09 2018-07-01T20:53:39 3.4.5. Set Timezone \u00b6 Command: mosquitto_pub -h 172.16.0.250 -u ***** -P ***** -t sonoff/B4E62D14BE5A/cmnd/Timezone -m 2 3.4.6. Set Pulse Time For Relay(s) \u00b6 This feature is useful when you always want to turn off your relay after a certain time. Actually it is a simple timer. If you set up pulse time to 10 minutes, the relay will turn off after 10 minutes every time you turn it on. I think these examples are far enough to understand how mqtt messages work, and after now that you know that you should be able to send your own commands to your device(s). The next section aims to demonstrate how to use Tasmota firmware with OpenHAB. 4. OpenHAB Integration \u00b6 So, I think this section will be the most interesting and useful part of this post. Since this post is about Tasmota firmware and its integration to OpenHAB I don't want to go deep inside the Mosquito install & setup and MQTT binding settings in OpenHAB. You can install and enable MQTT binding in PaperUI, and here is an example of mqtt.cfg . mqtt:openhabPI.url=tcp://localhost:1883 mqtt:openhabPI.clientId=openhabPI mqtt:openhabPI.user=********* mqtt:openhabPI.pwd=********* mqtt:openhabPI.retain=true 4.1. OpenHAB Items \u00b6 As always first we need to set up the items. 4.1.1. Turn The Relay(s) On And Off Item(s) \u00b6 My first example is for 1CH sonoff modules (S20, Basic, POW, etc). Switch prod_sonoff_BCDDC28027AD_switch1 \"Pantry Fan\" <fan> (sonoffsw,Sonoff) { mqtt=\">[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default], <[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)], <[openhabPI:sonoff/BCDDC28027AD/stat/POWER:state:default]\", autoupdate=\"true\" } The second example is for 4CH modules: Switch prod_sonoff_6001949C6548_switch1 \"SL-Right 1,3\" <light> (sonoffsw,Sonoff,SL_ALL,SL_RIGHT,SL_13,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power1:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER1)], <[openhabPI:sonoff/6001949C6548/stat/POWER1:state:default]\", autoupdate=\"true\" } Switch prod_sonoff_6001949C6548_switch2 \"SL-Right 2,4\" <light> (sonoffsw,Sonoff,SL_ALL,SL_RIGHT,SL_24,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power2:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER2)], <[openhabPI:sonoff/6001949C6548/stat/POWER2:state:default]\", autoupdate=\"true\" } Switch prod_sonoff_6001949C6548_switch3 \"SL-Left 1,3\" <light> (sonoffsw,Sonoff,SL_ALL,SL_LEFT,SL_13,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power3:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER3)], <[openhabPI:sonoff/6001949C6548/stat/POWER3:state:default]\", autoupdate=\"true\" } Switch prod_sonoff_6001949C6548_switch4 \"SL-Left 2,4\" <light> (sonoffsw,Sonoff,SL_ALL,SL_LEFT,SL_24,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power4:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER4)], <[openhabPI:sonoff/6001949C6548/stat/POWER4:state:default]\", autoupdate=\"true\" } I try to explain my setup step by step. There are 3 mqtt settings, 1 inbound and 2 outbounds: >[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default] This is the outbound one. It's intended to control (turn on / off) the relay. When you turn the relay on / off in OpenHAB the proper command will be sent to the device. Topic: sonoff/BCDDC28027AD/cmnd/power1 Type: command Trgger: * Transformation: default When you turn ON/OFF the relay OpenHAB publish ON/OFF message to sonoff/6001949C6548/cmnd/power4 topic. It is the same as described in 3.4.2. section: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/BCDDC28027AD/cmnd/power1 -m on <[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)] With this you subscribe to the sonoff/BCDDC28027AD/tele/STATE topic, and every time the sonoff device publish telemetry information the item's (relay) state will be updated to the actual state. <[openhabPI:sonoff/BCDDC28027AD/stat/POWER:state:default] This is very similar to the previous one, this updates the state of this item (prod_sonoff_BCDDC28027AD_switch1). Maybe you are wondering why I'm using two different mqtt topics to update the item state. The reason is very simple: I want to make sure that the item state is always the actual state of the relay. The telemetry (tele) topic is useful when you restart the OpenHAB and you don't have persistence set up for this item. In this case you lose the item state, but after the device posts its telemetry information the item state is updated. The second topic (stat) updates the item state immediately when you manually turn on/off the relay (with the button(s) on it). This configuration can be useful, when you control the relay via mosquitto_pub command, or using another application. With these two inbound settings you can make sure that the item is always up-to-date. There is an important thing to notice: the difference between 1ch and 4ch configuration: 1CH: >[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default] 4CH: >[openhabPI:sonoff/6001949C6548/cmnd/power1:command:*:default] >[openhabPI:sonoff/6001949C6548/cmnd/power2:command:*:default] >[openhabPI:sonoff/6001949C6548/cmnd/power3:command:*:default] >[openhabPI:sonoff/6001949C6548/cmnd/power4:command:*:default] You can use power and even power1 with 1CH device, BUT in case of multi-channel device you have to use the appropriate channel number. 1CH: <[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)] 4CH: <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER1)] <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER2)] <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER3)] <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER4)] BUT! When you configure telemetry topic subscription you can't use POWER1 for 1CH device, the right configuration is just the POWER without the number. The situation is the same when you configure stat topic ( sonoff/BCDDC28027AD/stat/POWER ). The version without the number has to be used! Of course, the multi-channel device configurations have to contain the number for each channel. 4.1.2. Telemetry Information \u00b6 If you want to display information about your device you can use its telemetry topic. Example: String prod_sonoff_6001949C6548_lwt \"S20A - Status [MAP(status_sonoff.map):%s]\" <lwt> (g_slwt) { mqtt=\"<[openhabPI:sonoff/6001949C6548/tele/LWT:state:default]\", autoupdate=\"true\" } Number prod_sonoff_6001949C6548__RSSI \"RSSI [%d %%]\" <signal> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.Wifi.RSSI)]\" } String prod_sonoff_6001949C6548_uptime \"Uptime [%s]\" <timer> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.Uptime)]\" } JSON example (provided by the device): { \"Time\":\"2018-07-01T18:57:33\", \"Uptime\":\"2T00:34:12\", \"Vcc\":3.142, \"POWER\":\"ON\", \"Wifi\":{ \"AP\":1, \"SSId\":\"Vinyo-Net\", \"RSSI\":52, \"APMac\":\"6C:3B:6B:A0:D2:79\" } } If you have a bit experience with JSONs I think this configuration should be clear for you. So if you need the WIFI RSSI value, the right configuration is: JSONPATH($.Wifi.RSSI) Tasmota firmware provide LWT (Last Will Testament) with retain flag, so you can use it to show the device status (Active/Inactive/Unknown). The device is in UNKNOWN state when you restart OpenHAB and the LWT topic is not updated since the restart. Here is my transformation map for LWT: Online=ACTIVE Offline=INACTIVE -=UNKNOWN =UNKNOWN NULL=No data Using persistence (not retain!) for LWT is a bit dangerous, because while the OpehHAB is offline, the state of the device may change, if so, after the OpenHAB becames online again, the last saved state will be displayed, not the actual one. 4.1.3. Other Examples \u00b6 This chapter is actually about more examples of displaying more information about your Sonoff device. String prod_sonoff_6001949C6548_hostname \"Hostname [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Hostname)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_BuildDateTime \"FW Build Date [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS2:state:JSONPATH($.StatusFWR.BuildDateTime)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Vcc \"Vcc [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Vcc)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Time \"Time [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Time)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_SSId \"SSId [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Wifi.SSId)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_IPAddress \"IPAddress [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.IPAddress)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Mac \"Mac [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Mac)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Subnetmask \"Subnetmask [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Subnetmask)]\", autoupdate=\"true\" } OK. Haven't you noticed something? Look closer. :) That's it, the topics: sonoff/6001949C6548/stat/ STATUS5 sonoff/6001949C6548/stat/ STATUS11 sonoff/6001949C6548/stat/ STATUS2 This information isn't posted automatically, or after a certain amount of time. In order to receive them you have to publish 0 to sonoff/6001949C6548/cmnd/STATUS topic. You can try this with mosquitto_sub command: mosquitto_pub -h 172.16.0.250 -u ***** -P ***** -t sonoff/B4E62D14BE5A/cmnd/STATUS -m 0 And the result (from a POW module): sonoff/B4E62D14BE5A/stat/STATUS {\"Status\":{\"Module\":6,\"FriendlyName\":[\"Sonoff\"],\"Topic\":\"sonoff\",\"ButtonTopic\":\"0\",\"Power\":1,\"PowerOnState\":1,\"LedState\":6,\"SaveData\":1,\"SaveState\":1,\"ButtonRetain\":0,\"PowerRetain\":0}} sonoff/B4E62D14BE5A/stat/STATUS1 {\"StatusPRM\":{\"Baudrate\":115200,\"GroupTopic\":\"sonoffs\",\"OtaUrl\":\"http://sonoff.maddox.co.uk/tasmota/sonoff.bin\",\"RestartReason\":\"Software/System restart\",\"Uptime\":\"19T13:57:19\",\"StartupUTC\":\"2018-07-11T03:38:44\",\"Sleep\":0,\"BootCount\":18,\"SaveCount\":171,\"SaveAddress\":\"F9000\"}} sonoff/B4E62D14BE5A/stat/STATUS2 {\"StatusFWR\":{\"Version\":\"5.14.0\",\"BuildDateTime\":\"2018-05-15T15:29:54\",\"Boot\":31,\"Core\":\"2_3_0\",\"SDK\":\"1.5.3(aec24ac9)\"}} sonoff/B4E62D14BE5A/stat/STATUS3 {\"StatusLOG\":{\"SerialLog\":2,\"WebLog\":2,\"SysLog\":0,\"LogHost\":\"\",\"LogPort\":514,\"SSId\":[\"Vinyo-Net\",\"\"],\"TelePeriod\":30,\"SetOption\":[\"00008009\",\"55818000\"]}} sonoff/B4E62D14BE5A/stat/STATUS4 {\"StatusMEM\":{\"ProgramSize\":526,\"Free\":476,\"Heap\":19,\"ProgramFlashSize\":1024,\"FlashSize\":4096,\"FlashMode\":3}} sonoff/B4E62D14BE5A/stat/STATUS5 {\"StatusNET\":{\"Hostname\":\"sonoff_B4E62D14BE5A\",\"IPAddress\":\"172.20.1.9\",\"Gateway\":\"172.16.0.1\",\"Subnetmask\":\"255.240.0.0\",\"DNSServer\":\"172.16.0.1\",\"Mac\":\"B4:E6:2D:14:BE:5A\",\"Webserver\":2,\"WifiConfig\":2}} sonoff/B4E62D14BE5A/stat/STATUS6 {\"StatusMQT\":{\"MqttHost\":\"172.16.0.250\",\"MqttPort\":1883,\"MqttClientMask\":\"DVES_%06X\",\"MqttClient\":\"DVES_14BE5A\",\"MqttUser\":\"vinyo\",\"MqttType\":1,\"MAX_PACKET_SIZE\":1000,\"KEEPALIVE\":15}} sonoff/B4E62D14BE5A/stat/STATUS7 {\"StatusTIM\":{\"UTC\":\"Mon Jul 30 17:36:03 2018\",\"Local\":\"Mon Jul 30 19:36:03 2018\",\"StartDST\":\"Sun Mar 25 02:00:00 2018\",\"EndDST\":\"Sun Oct 28 03:00:00 2018\",\"Timezone\":2,\"Sunrise\":\"06:21\",\"Sunset\":\"21:32\"}} sonoff/B4E62D14BE5A/stat/STATUS9 {\"StatusPTH\":{\"PowerDelta\":80,\"PowerLow\":0,\"PowerHigh\":0,\"VoltageLow\":0,\"VoltageHigh\":0,\"CurrentLow\":0,\"CurrentHigh\":0}} sonoff/B4E62D14BE5A/stat/STATUS10 {\"StatusSNS\":{\"Time\":\"2018-07-30T19:36:03\",\"ENERGY\":{\"Total\":40.988,\"Yesterday\":2.198,\"Today\":0.058,\"Power\":3,\"Factor\":0.09,\"Voltage\":220,\"Current\":0.161}}} sonoff/B4E62D14BE5A/stat/STATUS11 {\"StatusSTS\":{\"Time\":\"2018-07-30T19:36:03\",\"Uptime\":\"19T13:57:19\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":52,\"APMac\":\"6C:3B:6B:A0:D2:79\"}}} All these information can be displayed in OpenHAB if you want. Only one thing left: as I mentioned before these topics aren't updated automatically, so you need a button for update them, but first an item must be created: Switch prod_sonoff_6001949C6548_update \"UpdateInfo\" <update> (Sonoff) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/STATUS:command:*:0]\", autoupdate=\"false\"} Regardless of the switch command (ON/OFF) we have to publish 0 to the device: command:*:0 4.2. OpenHAB Sitemap \u00b6 Now we have the items configured, but I think it would be useful to display them in the OpenHAB. :) To do that we have to configure our sitemap, as well. Since this post is not an 'OpenHAB how to...', I will show you only one way to use the configured items. 4.2.1. Power Switches \u00b6 Maybe the most useful step if I post the configuration of my 4CH device: Frame label=\"Switches\" { Text item=prod_sonoff_6001949C6548_lwt Switch item=prod_sonoff_6001949C6548_switch1 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] Switch item=prod_sonoff_6001949C6548_switch2 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] Switch item=prod_sonoff_6001949C6548_switch3 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] Switch item=prod_sonoff_6001949C6548_switch4 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] } The only interesting thing in this configuration is the visibility part. Visibility configurations are intended to hide the switches when the device is not in \"Online\" state. You can skip this part, but I think it makes no sense to switch an unavailable device. 4.2.2. Display Information \u00b6 Frame label=\"Info\" { Text item=prod_sonoff_6001949C6548__RSSI Text item=prod_sonoff_6001949C6548_uptime Switch item=prod_sonoff_6001949C6548_update mappings=[ON=\"Go!\"] Text item=prod_sonoff_6001949C6548_Time visibility=[prod_sonoff_6001949C6548_Time!=\"NULL\"] Text item=prod_sonoff_6001949C6548_fwver visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"] Text item=prod_sonoff_6001949C6548_hostname visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"] Text item=prod_sonoff_6001949C6548_BuildDateTime visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"] Text item=prod_sonoff_6001949C6548_Vcc visibility=[prod_sonoff_6001949C6548_Vcc!=\"NULL\"] Text item=prod_sonoff_6001949C6548_SSId visibility=[prod_sonoff_6001949C6548_SSId!=\"NULL\"] Text item=prod_sonoff_6001949C6548_IPAddress visibility=[prod_sonoff_6001949C6548_IPAddress!=\"NULL\"] Text item=prod_sonoff_6001949C6548_Mac visibility=[prod_sonoff_6001949C6548_Mac!=\"NULL\"] Text item=prod_sonoff_6001949C6548_Subnetmask visibility=[prod_sonoff_6001949C6548_Subnetmask!=\"NULL\"] } // END : label=\"Info\" The RSSI and uptime values are posted with the telemetry information, so these are updated regularly. But the items after the Swtich item are updated only when you post 0 to the STATUS topic (Section: 4.1.3. Other Examples). To make it much more understandable here is the item configuration again: Switch prod_sonoff_6001949C6548_update \"UpdateInfo\" <update> (Sonoff) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/STATUS:command:*:0]\", autoupdate=\"false\"} And all these items are displayed only if they are not \"NULL\". I hope it is clear. If you read this article carefully you can configure your own device. Finally here are two screenshot about how it should look like: 5. Use Tasmota FW With Your Own Setup \u00b6 And last but not least, this is my bonus chapter. :) What is this chapter about? Since Tasmota is an brilliant OpenSource software and Sonoff devices are base on ESP8266 OpenSource hardware you can build your own smart home switch. You will need: ESP8266 module (ESP01, ESP07, NodeMCU devkit, etc). Relay (1CH, 2CH, etc) 5V 5V Power Supply 5V to 3V3 converter etc. :) Please take a look at my 4CH setup in the beginning of this post for details. Of course you can create your own PCB, as well. :) To customize Tasmota you will need an IDE (**I**ntegrated **D**evelopment **E**nvironment). If you do a google search for \"compile tasmota\" the first hit is exactly what we need. :) Beginner Guide Create your own Firmware Build So please forgive me, but I don't bother with a guide of \"How to install and configure Atom\", especially because it is only a few steps. As I mentioned, some times, I'm not a developer so maybe my explanations are not always 100% right or clear, but always try to give you working solutions. 5.1. user_config.h \u00b6 It this file you can pre-configure some values, most of them are configurable via the web interface. Firstly, the most important thing is the Wifi set up. Why? Please, imagine the situation when you use an ESP01 and you used up all its GPIO port for switching relays. Why is it a problem? Because you don't have any GPIO pin left for a button, which is essential to put the device in Wifi config mode. You can pre-configure two different stations: #define STA_SSID1 \"My Wifi Station\" #define STA_PASS1 \"UnbrakeablePassword\" #define STA_SSID2 \"\" #define STA_PASS2 \"\" After you compile your firmware with these setup the device will automatically connect to your wifi network. If you scroll down a bit in the user_config.h , you can find the MQTT related settings. There is no reason to leave it blank. :) Example: #define MQTT_USE 1 #define MQTT_HOST \"172.16.0.250\" #define MQTT_FINGERPRINT1 \"A5 02 FF 13 99 9F 8B 39 8E F1 83 4F 11 23 65 0B 32 36 FC 07\" #define MQTT_FINGERPRINT2 \"A5 02 FF 13 99 9F 8B 39 8E F1 83 4F 11 23 65 0B 32 36 FC 07\" #define MQTT_PORT 1883 #define MQTT_USER \"userName\" #define MQTT_PASS \"Password\" Please modify only the necessary fields. Moreover, you can configure the topic: #define MQTT_FULLTOPIC \"%topic%//%prefix%/\" Some other exciting options: NTP server: #define NTP_SERVER1 \"pool.ntp.org\" #define NTP_SERVER2 \"nl.pool.ntp.org\" #define NTP_SERVER3 \"0.nl.pool.ntp.org\" Time Zone: #define APP_TIMEZONE 1 Switch Mode: #define SWITCH_MODE TOGGLE // [SwitchMode] TOGGLE, FOLLOW, FOLLOW_INV, PUSHBUTTON, PUSHBUTTON_INV, PUSHBUTTONHOLD, PUSHBUTTONHOLD_INV, PUSHBUTTON_TOGGLE (the wall switch state) MQTT retain: #define MQTT_TELE_RETAIN 0 // Tele messages may send retain flag (0 = off, 1 = on) And so on, and so on.... Please scroll down the file and configure everything you need. One more thing: In this file you have to configure which sensors you plan to use with your device, but do not select too many of them, because you can easily run into OOM exception. 5.2. sonoff_template.h \u00b6 Here I had the feeling that I should know more about programming. :) First create your own template. Example: { \"Sonoff Custom\", // Sonoff Basic (ESP8266) GPIO_REL1_INV, // GPIO00 Button GPIO_USER, // GPIO01 Serial RXD and Optional sensor GPIO_REL2_INV, // GPIO02 GPIO_USER, // GPIO03 Serial TXD and Optional sensor GPIO_USER, // GPIO04 Optional sensor 0, // GPIO05 GPIO_USER, // GPIO06 (SD_CLK Flash) 0, // GPIO07 (SD_DATA0 Flash QIO/DIO/DOUT) 0, // GPIO08 (SD_DATA1 Flash QIO/DIO/DOUT) 0, // GPIO09 (SD_DATA2 Flash QIO) 0, // GPIO10 (SD_DATA3 Flash QIO) 0, // GPIO11 (SD_CMD Flash) 0, // GPIO12 Red Led and Relay (0 = Off, 1 = On) GPIO_LED1_INV, // GPIO13 Green Led (0 = On, 1 = Off) GPIO_USER, // GPIO14 Optional sensor 0, // GPIO15 0, // GPIO16 0 // ADC0 Analog input }, As you can see I modified the template of the \"Basic\" module. My first goal was to use an ESP01 with Tasmota. ESP01 modules have only 2 usable GPIO pins (GPIO0 and GPIO2). Both of them are connected to the relay: GPIO_REL1_INV , GPIO_REL2_INV . Originally the GPIO0 was used to connect it with a button (I haven't modified the comment here on purpose for demonstration.) What functionality can be used for the PINs? You can find all available functions here: // User selectable GPIO functionality enum UserSelectablePins { GPIO_NONE, // Not used GPIO_DHT11, // DHT11 GPIO_DHT22, // DHT21, DHT22, AM2301, AM2302, AM2321 ... ... GPIO_REL1, // Relays GPIO_REL2, ... ... GPIO_REL1_INV, GPIO_REL2_INV, ... ... So what is the difference between GPIO_REL1 and GPIO_REL1_INV ? This configuration is related to NO (**N**ormally **O**pen) and NC (**N**ormally **C**losed) setup of the relay (if applicable). Most relays have 3 connections: NC, NO, COM (common). When you set the PIN to low: NC means that the circuit is closed. NO means that the circuit is open. When you set the PIN to high the meanings of NC an NO are reverse. So the usage of GPIO_REL1 and GPIO_REL1_INV depends on your hardware setup. If you connect your stuff (which you want to turn OFF and ON) to NO you want to use the inverse version, because the relay will close the circuit when the PIN is put to low (0) state. If you are using NC , you should choose the GPIO_REL1 . I hope this is clear for you, if not, give it a try. :) There is special functionality which is the GPIO_USER . If you set a GPIO pin to this, you will be able to select its functionality on the web interface: And where does the list come from? // Text in webpage Module Parameters and commands GPIOS and GPIO const char kSensorNames[] PROGMEM = D_SENSOR_NONE \"|\" ... ... 5.3. Example \u00b6 Maybe it will be more clear if I show you an example. For demonstration I used a NodeMCU DevKit v0.9, 1CH 5V relay and a DHT22 sensor. Hardware connections: DHT22 VCC --> 3V3 GND --> GND DATA --> D2 Relay VCC --> 5V GND --> GND INPUT --> D3 To be able to choose the appropriate GPIO pins we have to know what D2 and D3 means. You can see in the picture that D1 is actually the GPIO5 and D2 is the GPIO4. So the following modifications are needed in the sonoff_template.h file: --- comp/Sonoff-Tasmota-development/sonoff/sonoff_template.h 2018-07-31 20:10:22.000000000 +0200 +++ Sonoff-Tasmota-development/sonoff/sonoff_template.h 2018-08-01 19:45:49.785921996 +0200 @@ -157,6 +157,7 @@ // Supported hardware modules enum SupportedModules { SONOFF_BASIC, + BLOGTST, SONOFF_RF, SONOFF_SV, SONOFF_TH, @@ -220,6 +221,7 @@ const uint8_t kNiceList[MAXMODULE] PROGMEM = { SONOFF_BASIC, + BLOGTST, SONOFF_RF, SONOFF_TH, SONOFF_DUAL, @@ -288,6 +290,26 @@ 0, // GPIO16 0 // ADC0 Analog input }, + { \"Sonoff Blogtst\", // Sonoff Basic (ESP8266) + GPIO_KEY1, // GPIO00 Button + GPIO_USER, // GPIO01 Serial RXD and Optional sensor + 0, // GPIO02 + GPIO_USER, // GPIO03 Serial TXD and Optional sensor + GPIO_DHT22, // GPIO04 Optional sensor + GPIO_REL1, // GPIO05 + 0, // GPIO06 (SD_CLK Flash) + 0, // GPIO07 (SD_DATA0 Flash QIO/DIO/DOUT) + 0, // GPIO08 (SD_DATA1 Flash QIO/DIO/DOUT) + 0, // GPIO09 (SD_DATA2 Flash QIO) + 0, // GPIO10 (SD_DATA3 Flash QIO) + 0, // GPIO11 (SD_CMD Flash) + 0, // GPIO12 Red Led and Relay (0 = Off, 1 = On) + GPIO_LED1_INV, // GPIO13 Green Led (0 = On, 1 = Off) + GPIO_USER, // GPIO14 Optional sensor + 0, // GPIO15 + 0, // GPIO16 + 0 // ADC0 Analog input + }, { \"Sonoff RF\", // Sonoff RF (ESP8266) GPIO_KEY1, // GPIO00 Button GPIO_USER, // GPIO01 Serial RXD and Optional sensor @@ -970,4 +992,4 @@ */ -#endif // _SONOFF_TEMPLATE_H_ \\ No newline at end of file +#endif // _SONOFF_TEMPLATE_H_ I defined a new template for my setup with \"Sonoff Blogtst\" name based on the \"Sonoff Basic\" module template. The following 3 lines were modified (GPIO04,05,12): + GPIO_DHT22, // GPIO04 Optional sensor + GPIO_REL1, // GPIO05 + 0, // GPIO12 Red Led and Relay (0 = Off, 1 = On) GPIO04 is connected to my DHT22 sensor. GPIO05 is connected to the relay \"IN\" pin. Originally the Sonoff Basic modules use the GPIO12 to control the relay, but we don't, so set it to \"0\". Important If you define a completely new template you have to add two additional item to two different array: kNiceList , SupportedModules . enum SupportedModules { SONOFF_BASIC, + BLOGTST, const uint8_t kNiceList[MAXMODULE] PROGMEM = { SONOFF_BASIC, + BLOGTST, And in the right order! Example: if you write your new template definition before the \"Sonoff Basic\" definition, you should put BLOGTST before SONOFF_BASIC in the kNiceList and SupportedModules arrays, as well. You are almost done. Since the NodeMCU DevKit has \"user\" button we have two options: Configure the firmware to use this button. In this scenario you can use the user button to put the device to WiFi configuration mode. If you scroll back a bit to the picture about the pinout you can see that the button is connected to GPIO16. If you choose this option set the GPIO16 from 0 to GPIO_KEY1 . Configure the WiFi parameters in the user_config.h . To do this you should simply modify the STA_SSID1 and STA_PASS1 . Example: --- comp/Sonoff-Tasmota-development/sonoff/user_config.h 2018-07-31 20:10:22.000000000 +0200 +++ Sonoff-Tasmota-development/sonoff/user_config.h 2018-08-01 19:34:45.629732934 +0200 @@ -59,8 +59,8 @@ #define WIFI_SUBNETMASK \"255.255.255.0\" // [IpAddress3] If not using DHCP set Network mask #define WIFI_DNS \"192.168.2.27\" // [IpAddress4] If not using DHCP set DNS IP address (might be equal to WIFI_GATEWAY) -#define STA_SSID1 \"\" // [Ssid1] Wifi SSID -#define STA_PASS1 \"\" // [Password1] Wifi password +#define STA_SSID1 \"**********\" // [Ssid1] Wifi SSID +#define STA_PASS1 \"**********\" // [Password1] Wifi password #define STA_SSID2 \"\" // [Ssid2] Optional alternate AP Wifi SSID #define STA_PASS2 \"\" // [Password2] Optional alternate AP Wifi password #define WIFI_CONFIG_TOOL WIFI_WAIT // [WifiConfig] Default tool if wifi fails to connect To use DHCP (default) take a look at this line: #define WIFI_IP_ADDRESS \"0.0.0.0\" // [IpAddress1] Set to 0.0.0.0 for using DHCP or IP address Next steps: build! & upload. :) I've already written about uploading the firmware, but here is the example: sudo esptool.py --port /dev/ttyUSB0 write_flash -fs 1MB -fm dout 0x0 firmware.bin Open the web interface (configure module option) and select the newly configured template: If everything is fine, after the reboot you should see the temperature and humidity values on the main page: And, of course you can turn the relay on and off with the \"Toggle\" button. In a nutshell this is how you can use Tasmota firmware with your own hardware setup in nutshell. Maybe my explanations are not always the best, but I really hope this post is useful for you in case you want to work with Sonoff / Tasmota / OpenHAB or both of them... :) If you understand all these things I'm pretty sure you can build you own setup, even with custom hardware. I think at this time Sonoff+Tasmota is the cheapest solution to control equipment with OpenHab. Maybe with custom hardware setup could be cheaper but not much, and you should be aware of the time of assembling.","title":"Sonoff Relays With OpenHab And Tasmota Firmware"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#sonoff-relays-with-openhab-and-tasmota-firmware","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!","title":"Sonoff Relays With OpenHab And Tasmota Firmware"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#tldr","text":"I'm using OpenHab to control some Lights and equipment in my house and garden. I used to develop my own circuits using ESPs, relays, power supplies etc., but some months ago I found the Sonoff products, which offer the same or more functions than my own devices. Just for comparison here is my 4ch relay with ESP07: And the 4CH relay from Sonoff: However, all my own relays and sensors work fine, I had to develop a firmware for them. Since I'm not a not developer it was a big challenge for me, even with the LUA FW. So I decided to order some Sonoff products and tried them. For the very first time I tried with its factory firmware, but it has a lot of limitations: It cannot be used with OpenHab. This is a show stopper disadvantage for me. It has no web interface. Very limited scheduler. (No sunset / sunrise option, etc) Scenes cannot be shared. Uses the sonoff own cloud infrastructure. That's why nobody knows what kind of information is sent to the sonoff servers. (The big brother is always watching you. :) ) I started Googling and found the Tasmota firmware easily, which is for Sonoff products. It is available on Github: Sonoff-Tasmota There are a lot of excellent articles on the Internet which are about How to update your sonoff product with Tasmota firmware, so mine is something like \"Yet, another article about....\" :) So, you need: One supported sonoff product. (You can find the supported device list on the official Tasmota Github page.) 4 solder-able pins. (For GND, RxD, TxD and Vcc) Soldering Station. (And willingness to solder of course.) USB to TTL converter. A PC or notebook. Screwdriver. Before we start, let's say some words about Sonoffs. I'v ordered 4 different types of relays: S2 smart Socket, POW (One ch relay with power monitoring capability), Sonoff basic and Sonoff 4ch. All of them are working perfectly with Tasmota. You can visit the sonoff store on Aliexpress and I bet you will be surprised at how cheap these things are. At the moment the cheapest relay is the sonoff basic, it costs only 5$, and you can get a full system for switching things around your house, garden, holiday house even if you are far away. What about the quality? So-so. For this price I think the quality is acceptable, but not the best. For example the product sheet of the Basic says that the maximum load is 2200W. Seeing the soldering inside the cover I don't think it can bear 2.2kW. But from the other hand there is not a lot of equipment which we want to control over the internet and consumes 2.2kW. In my case I want to control some lights, fans and my water pump with these relays, so sonoff relays are perfectly suitable for my goal.","title":"TL;DR"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#1-soldering","text":"As I wrote above I have 4 types of Sonoff relays. Thanks to God the most expensive one (4ch) have pre-soldered pins on it. Helpful links: Hardware-Preparation Sonoff-4CH-and-4CH-Pro Please be aware (4ch) that \"The printed labels on the PCB for Rx and Tx are incorrectly swapped as can be seen on the image.\"","title":"1. Soldering"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#11-sonoff-pow","text":"","title":"1.1. Sonoff POW"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#12-sonoff-s20-socket","text":"","title":"1.2. Sonoff S20 (Socket)"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#13-basic","text":"The GPIO14 can be used for example to connect DHT22 sensor (or any other sensor supported by Tasmota) to the sonoff. I will assist you later how to configure Tasmota to be able to use additional sensor. The button on sonoffs are always connected to GPIO0, except when the board has more then one button (eg.: 4ch), in this case the 1st button is connected to GPIO0.","title":"1.3. Basic"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#2-upload-the-firmware","text":"To be honest this is the easiest step. Danger Do not connect AC power and the serial connection at the same time! Connection Matrix: USB 2 TTL <--> Sonoff GND <--> GND TxD <--> RxD RxD <--> TxD VCC <--> VCC In order to put your device into FW upgrade mode, press and hold the button on it while connecting to VCC. You can connect all the wires and press the button when you connect the USB2TTL to your computer, or leave the VCC (only the vcc) disconnected, connect the USB2TTL to the computer, press and hold the button and connect the VCC. Useful links: upgrading-sonoff-stock-firmware-to-sonoff-tasmota-usb-to-serial-and-ota-update-methods https://github.com/arendst/Sonoff-Tasmota/wiki/Esptool","title":"2. Upload The Firmware"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#21-optional-take-backup","text":"Taking backup is always optional, but essential. If you skip this step, you lose the possibility to restore the factory firmware. (It is not 100% true because I will make the stock firmware available here.) You will need Esptool to achieve this step. If you don't know how to install it, please take a look at this page: esptool For Windows users maybe the \"ESP8266Flasher\" could be an option ( nodemcu-flasher ). esptool.py --port /dev/ttyUSB0 read_flash 0x00000 0x100000 image1M.bin esptool.py v2.3.1 Connecting.... Detecting chip type... ESP8266 Chip is ESP8266EX Features: WiFi Uploading stub... Running stub... Stub running... 1048576 (100 %) 1048576 (100 %) Read 1048576 bytes at 0x0 in 94.9 seconds (88.4 kbit/s)... Hard resetting via RTS pin... Please be aware that after any esptool command you have to reconnect to the device. (So, you have to disconnect vcc, press and hold the button and connect VCC again.)","title":"2.1. (Optional) Take Backup"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#22-optional-erase-flash","text":"This step is also optional, but recommended. esptool.py --port /dev/ttyUSB0 erase_flash esptool.py v2.3.1 Connecting.... Detecting chip type... ESP8266 Chip is ESP8266EX Features: WiFi Uploading stub... Running stub... Stub running... Erasing flash (this may take a while)... Chip erase completed successfully in 3.2s Hard resetting via RTS pin...","title":"2.2. (Optional) Erase Flash"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#23-flashing-the-firmware","text":"Till this step I haven't mentioned the firmware itself. How can you download the pre-compiled firmware? You don't need to compile the firmware, it can be downloaded from github: releases The sonoff.bin has always worked for me: sonoff.bin Before you use this link, please make sure that no newer version has been released. So, let's burn the firmware: esptool.py --port /dev/ttyUSB0 write_flash -fs 1MB -fm dout 0x0 sonoff.bin esptool.py v2.3.1 Connecting.... Detecting chip type... ESP8266 Chip is ESP8266EX Features: WiFi Uploading stub... Running stub... Stub running... Configuring flash size... Compressed 539040 bytes to 368171... Wrote 539040 bytes (368171 compressed) at 0x00000000 in 32.6 seconds (effective 132.4 kbit/s)... Hash of data verified. Leaving... Hard resetting via RTS pin... That's all. You can assemble your sonoff and start using it. :) In the future chapters I'll give you some tips for configuring the devices and using them with MQTT and OpenHAB.","title":"2.3. Flashing The Firmware"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#3-settings","text":"","title":"3. Settings"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#31-connect-the-device-to-your-local-network-wifi-setup","text":"The first things to do is understand how the button with the new Tasmote FW works. Here is the link: Button-Usage 1 short press: Toggles the relay either directly or by sending a MQTT message like cmnd/sonoff/POWER1 ON. This will blink the LED twice and sends a MQTT status message like stat/sonoff/POWER1 ON. If cmnd/sonoff/ButtonRetain on has been used the MQTT message will also contain the MQTT retain flag. 2 short presses: Toggles the relay 2 if available on the device like Sonoff Dual. This will blink the LED twice and sends a MQTT status message like stat/sonoff/POWER2 on. 3 short presses: Start Wifi smartconfig allowing for SSID and Password configuration using an Android mobile phone with the ESP8266 SmartConfig app. The LED will blink during the config period. A single button press during this period will abort and restart sonoff. 4 short presses: Start Wifi manager providing an Access Point with IP address 192.168.4.1 and a web server allowing the configuration of Wifi. The LED will blink during the config period. A single button press during this period will abort and restart sonoff. 5 short presses: Start Wifi Protected Setup (WPS) allowing for SSID and Password configuration using the router's WPS button or webpage. The LED will blink during the config period. A single button press during this period will abort and restart sonoff. 6 short presses: Will restart the module 7 short presses: Start OTA download of firmware. The green LED is lit during the update Pressing the button for over forty seconds: Reset settings to defaults as defined in user_config.h and restarts the device I always used the 4 short presses option to configure WiFi. After the WiFi is successfully set up you can configure your device via your web browser using the IP address. In order to figure out the IP address use your router configuration page or console if accessible. Or you can use nmap to find IP addresses: sudo nmap -sP 172.20.1.* Example output: Starting Nmap 7.40 ( https://nmap.org ) at 2018-07-01 18:35 CEST Nmap scan report for 172.20.1.1 Host is up (0.022s latency). MAC Address: B4:E6:2D:15:82:48 (Unknown) Nmap scan report for 172.20.1.2 Host is up (0.0045s latency). MAC Address: 60:01:94:9C:65:48 (Espressif)","title":"3.1. Connect The Device To Your Local Network - WiFi Setup"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#32-configure-module","text":"The first thing to do after the WiFi settings is tell the Firmware which module we are using (Basic/POW/4CH/etc.). To do this open the module configuration page. Example: http://172.20.1.9/md Or simply open the main page: http://172.20.1.9/ , select \"Configuration\" then \"Configure Module\" option. Use the drop-down list to select your device type. This step is extremely important but straightforward. Without proper module selection you can't use your device features and it can lead to improper behavior. For example if you have a 4CH sonoff device, without selecting the right module you can control only the 1st channel, or with POW module you can't see the sensor data.","title":"3.2. Configure Module"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#33-configure-mqtt","text":"Actually you can use your device via its web interface without MQTT, but one of the goals of this post is to integrate sonoff devices to OpenHab, and the best way (or the only way) to do this is use MQTT. Open the MQTT configuration page (eg.: http://172.20.1.9/mq ) Example configuration: Without manual mqtt configuration the firmware tries to connect to the mqtt server using mDNS. I've never tried this feature so I don't know if it works or not, and don't know what happens when multiple MQTT servers are available, or when user/password is needed to connect. With the configuration on the screenshot above you can subscribe to the topic and see what happens. I'm using the following syntax as topic: sonoff/[MAC ADDRESS without colon]/%prpefix% The prefix can be tele,state or cmnd. Example subscribe : mosquitto_sub -v -h 172.16.0.250 -u ****** -P ***** -t 'sonoff/B4E62D14BE5A/#' Example outputs : 1. sonoff/B4E62D14BE5A/tele/SENSOR {\"Time\":\"2018-07-01T18:57:27\",\"ENERGY\":{\"Total\":2.262,\"Yesterday\":1.330,\"Today\":0.077,\"Power\":2,\"Factor\":0.05,\"Voltage\":222,\"Current\":0.146}} 2. sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T18:57:33\",\"Uptime\":\"2T00:34:12\",\"Vcc\":3.142,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":52,\"APMac\":\"6C:3B:6B:A0:D2:79\"}} 3. sonoff/B4E62D14BE5A/stat/POWER OFF Explanation: This is a Sonoff POW module which reports power usage. You can see that the message is in JSON format. {\"Time\":\"2018-07-01T18:57:27\",\"ENERGY\":{\"Total\":2.262,\"Yesterday\":1.330,\"Today\":0.077,\"Power\":2,\"Factor\":0.05,\"Voltage\":222,\"Current\":0.146}} Tasmota firmware reports telemetry information (RSSI, Uptime, power state, etc.) periodically (in every 300s by default). This message is also in JSON format. When you turn the relay on or off (even with the web interface or via MQTT message) the device reports the new state. In this particular situation I turned the device off using the web interface. These three steps are essential to use your device with OpenHAB & MQTT. You can go through all web configuration elements/options, but this post doesn't aim to give you a complete guideline to Tasmota firmware.","title":"3.3. Configure MQTT"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#34-sending-commands-to-the-device-via-mqtt","text":"Before you start you can check all available commands on Tasmota GitHub page: https://github.com/arendst/Sonoff-Tasmota/wiki/Commands If you are planning to deal with Tasmota firmware I recommend you to bookmark this page, it is very helpful. When you want to send commands to tasmota you always have to use the cmnd prefix. Since we are speaking about relays, maybe the first question comes up: how to ON or OFF them. Let's see the web page mentioned before (Commands): In this first example I will explain all available options to make the configuration method as clear as possible. So in a terminal tab I always subscribe to the device topic to see what happens. Command: mosquitto_sub -v -h 172.16.0.250 -u ***** -P ***** -t 'sonoff/B4E62D14BE5A/#'","title":"3.4. Sending Commands To The Device Via MQTT"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#341-check-the-state-of-the-relay","text":"Command: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -n On the \"subscribe\" tab you can see these messages: sonoff/B4E62D14BE5A/cmnd/power (null) sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"} sonoff/B4E62D14BE5A/stat/POWER ON First Line: The command you sent to the device. Second Line: The result of your command. To any command tasmota relies with a \"RESULT\" message. Third Line: Status of the relay If you have for example Sonoff Dual, or Sonoff 4ch you can specify which channel you want to use. For example if you want to check the 3rd channel you can use this command: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power3 -n I'm using a POW module for introduction, it has no 3rd channel so get \"unknown\" result: sonoff/B4E62D14BE5A/cmnd/power3 (null) sonoff/B4E62D14BE5A/stat/RESULT {\"Command\":\"Unknown\"} If your device has only one channel do not use the number, so use only the \"POWER\" word instead of \"POWER1\". But in case of any multi-channels you have to use POWER1, POWER2, etc.","title":"3.4.1. Check The State Of The Relay:"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#342-onofftoggle-the-relay","text":"Regarding relays the most important activity is to turn on and off them. Command: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m on MQTT messages: sonoff/B4E62D14BE5A/cmnd/power on sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"} sonoff/B4E62D14BE5A/stat/POWER ON Command: mosquitto_pub -h 172.16.0.250 -u vinyo -P Timike -t sonoff/B4E62D14BE5A/cmnd/power -m on MQTT messages: sonoff/B4E62D14BE5A/cmnd/power off sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"OFF\"} sonoff/B4E62D14BE5A/stat/POWER OFF As you can see in the table you can use numbers instead of command, eg: 2 / toggle. Let's fire the toggle command twice: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 2 mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 2 MQTT messages: sonoff/B4E62D14BE5A/cmnd/power 2 sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"OFF\"} sonoff/B4E62D14BE5A/stat/POWER OFF sonoff/B4E62D14BE5A/cmnd/power 2 sonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"} sonoff/B4E62D14BE5A/stat/POWER ON","title":"3.4.2. ON/OFF/TOGGLE The Relay"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#343-blink-the-relay","text":"This is an interesting feature of Tasmota FW. But first please check the following options related to blinking. With this option the relay will turn on \"BlinkCount\" times for \"BlinkTime\" second*0.1. Check the default settings using these commands: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/BlinkCount -n mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/BlinkTime -n MQTT messages: sonoff/B4E62D14BE5A/cmnd/BlinkCount (null) sonoff/B4E62D14BE5A/stat/RESULT {\"BlinkCount\":10} sonoff/B4E62D14BE5A/cmnd/BlinkTime (null) sonoff/B4E62D14BE5A/stat/RESULT {\"BlinkTime\":10} If you connected a light to your relay, it will turn on for 1 second, 10 times. So, turn on for 1 sec, then turn off for 1 sec, turn on for 1 sec, turn on for 1 sec, and so on. After turning on the relay ten times it remains OFF. But you can terminate blinking with \"4/blinkoff\" command. Examples: Start Blinking: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 3 Terminate Blinking: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 4 Maybe the missing feature of this equipment that you cannot configure BlinkCount and BlinkTime for each relays when use device with multiple channels. In the rest of this configuration section I will show you some useful and exciting features of tasmota firmware.","title":"3.4.3. BLINK The Relay"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#344-set-telemetry-period","text":"If you need the status information of your device more frequently than its default (300s) you can configure the telemetry period with the following command: mosquitto_pub -h 172.16.0.250 -u ***** -P ****** -t sonoff/B4E62D14BE5A/cmnd/TelePeriod -m 30 After runing this command tasmota will publish \"STATE\" messages in every 30 seconds: sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T20:53:09\",\"Uptime\":\"2T02:29:48\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":58,\"APMac\":\"6C:3B:6B:A0:D2:79\"}} sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T20:53:39\",\"Uptime\":\"2T02:30:18\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":58,\"APMac\":\"6C:3B:6B:A0:D2:79\"}} Times: 2018-07-01T20:53:09 2018-07-01T20:53:39","title":"3.4.4. Set Telemetry Period"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#345-set-timezone","text":"Command: mosquitto_pub -h 172.16.0.250 -u ***** -P ***** -t sonoff/B4E62D14BE5A/cmnd/Timezone -m 2","title":"3.4.5. Set Timezone"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#346-set-pulse-time-for-relays","text":"This feature is useful when you always want to turn off your relay after a certain time. Actually it is a simple timer. If you set up pulse time to 10 minutes, the relay will turn off after 10 minutes every time you turn it on. I think these examples are far enough to understand how mqtt messages work, and after now that you know that you should be able to send your own commands to your device(s). The next section aims to demonstrate how to use Tasmota firmware with OpenHAB.","title":"3.4.6. Set Pulse Time For Relay(s)"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#4-openhab-integration","text":"So, I think this section will be the most interesting and useful part of this post. Since this post is about Tasmota firmware and its integration to OpenHAB I don't want to go deep inside the Mosquito install & setup and MQTT binding settings in OpenHAB. You can install and enable MQTT binding in PaperUI, and here is an example of mqtt.cfg . mqtt:openhabPI.url=tcp://localhost:1883 mqtt:openhabPI.clientId=openhabPI mqtt:openhabPI.user=********* mqtt:openhabPI.pwd=********* mqtt:openhabPI.retain=true","title":"4. OpenHAB Integration"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#41-openhab-items","text":"As always first we need to set up the items.","title":"4.1. OpenHAB Items"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#411-turn-the-relays-on-and-off-items","text":"My first example is for 1CH sonoff modules (S20, Basic, POW, etc). Switch prod_sonoff_BCDDC28027AD_switch1 \"Pantry Fan\" <fan> (sonoffsw,Sonoff) { mqtt=\">[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default], <[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)], <[openhabPI:sonoff/BCDDC28027AD/stat/POWER:state:default]\", autoupdate=\"true\" } The second example is for 4CH modules: Switch prod_sonoff_6001949C6548_switch1 \"SL-Right 1,3\" <light> (sonoffsw,Sonoff,SL_ALL,SL_RIGHT,SL_13,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power1:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER1)], <[openhabPI:sonoff/6001949C6548/stat/POWER1:state:default]\", autoupdate=\"true\" } Switch prod_sonoff_6001949C6548_switch2 \"SL-Right 2,4\" <light> (sonoffsw,Sonoff,SL_ALL,SL_RIGHT,SL_24,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power2:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER2)], <[openhabPI:sonoff/6001949C6548/stat/POWER2:state:default]\", autoupdate=\"true\" } Switch prod_sonoff_6001949C6548_switch3 \"SL-Left 1,3\" <light> (sonoffsw,Sonoff,SL_ALL,SL_LEFT,SL_13,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power3:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER3)], <[openhabPI:sonoff/6001949C6548/stat/POWER3:state:default]\", autoupdate=\"true\" } Switch prod_sonoff_6001949C6548_switch4 \"SL-Left 2,4\" <light> (sonoffsw,Sonoff,SL_ALL,SL_LEFT,SL_24,Lights) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/power4:command:*:default], <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER4)], <[openhabPI:sonoff/6001949C6548/stat/POWER4:state:default]\", autoupdate=\"true\" } I try to explain my setup step by step. There are 3 mqtt settings, 1 inbound and 2 outbounds: >[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default] This is the outbound one. It's intended to control (turn on / off) the relay. When you turn the relay on / off in OpenHAB the proper command will be sent to the device. Topic: sonoff/BCDDC28027AD/cmnd/power1 Type: command Trgger: * Transformation: default When you turn ON/OFF the relay OpenHAB publish ON/OFF message to sonoff/6001949C6548/cmnd/power4 topic. It is the same as described in 3.4.2. section: mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/BCDDC28027AD/cmnd/power1 -m on <[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)] With this you subscribe to the sonoff/BCDDC28027AD/tele/STATE topic, and every time the sonoff device publish telemetry information the item's (relay) state will be updated to the actual state. <[openhabPI:sonoff/BCDDC28027AD/stat/POWER:state:default] This is very similar to the previous one, this updates the state of this item (prod_sonoff_BCDDC28027AD_switch1). Maybe you are wondering why I'm using two different mqtt topics to update the item state. The reason is very simple: I want to make sure that the item state is always the actual state of the relay. The telemetry (tele) topic is useful when you restart the OpenHAB and you don't have persistence set up for this item. In this case you lose the item state, but after the device posts its telemetry information the item state is updated. The second topic (stat) updates the item state immediately when you manually turn on/off the relay (with the button(s) on it). This configuration can be useful, when you control the relay via mosquitto_pub command, or using another application. With these two inbound settings you can make sure that the item is always up-to-date. There is an important thing to notice: the difference between 1ch and 4ch configuration: 1CH: >[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default] 4CH: >[openhabPI:sonoff/6001949C6548/cmnd/power1:command:*:default] >[openhabPI:sonoff/6001949C6548/cmnd/power2:command:*:default] >[openhabPI:sonoff/6001949C6548/cmnd/power3:command:*:default] >[openhabPI:sonoff/6001949C6548/cmnd/power4:command:*:default] You can use power and even power1 with 1CH device, BUT in case of multi-channel device you have to use the appropriate channel number. 1CH: <[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)] 4CH: <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER1)] <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER2)] <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER3)] <[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER4)] BUT! When you configure telemetry topic subscription you can't use POWER1 for 1CH device, the right configuration is just the POWER without the number. The situation is the same when you configure stat topic ( sonoff/BCDDC28027AD/stat/POWER ). The version without the number has to be used! Of course, the multi-channel device configurations have to contain the number for each channel.","title":"4.1.1. Turn The Relay(s) On And Off Item(s)"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#412-telemetry-information","text":"If you want to display information about your device you can use its telemetry topic. Example: String prod_sonoff_6001949C6548_lwt \"S20A - Status [MAP(status_sonoff.map):%s]\" <lwt> (g_slwt) { mqtt=\"<[openhabPI:sonoff/6001949C6548/tele/LWT:state:default]\", autoupdate=\"true\" } Number prod_sonoff_6001949C6548__RSSI \"RSSI [%d %%]\" <signal> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.Wifi.RSSI)]\" } String prod_sonoff_6001949C6548_uptime \"Uptime [%s]\" <timer> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.Uptime)]\" } JSON example (provided by the device): { \"Time\":\"2018-07-01T18:57:33\", \"Uptime\":\"2T00:34:12\", \"Vcc\":3.142, \"POWER\":\"ON\", \"Wifi\":{ \"AP\":1, \"SSId\":\"Vinyo-Net\", \"RSSI\":52, \"APMac\":\"6C:3B:6B:A0:D2:79\" } } If you have a bit experience with JSONs I think this configuration should be clear for you. So if you need the WIFI RSSI value, the right configuration is: JSONPATH($.Wifi.RSSI) Tasmota firmware provide LWT (Last Will Testament) with retain flag, so you can use it to show the device status (Active/Inactive/Unknown). The device is in UNKNOWN state when you restart OpenHAB and the LWT topic is not updated since the restart. Here is my transformation map for LWT: Online=ACTIVE Offline=INACTIVE -=UNKNOWN =UNKNOWN NULL=No data Using persistence (not retain!) for LWT is a bit dangerous, because while the OpehHAB is offline, the state of the device may change, if so, after the OpenHAB becames online again, the last saved state will be displayed, not the actual one.","title":"4.1.2. Telemetry Information"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#413-other-examples","text":"This chapter is actually about more examples of displaying more information about your Sonoff device. String prod_sonoff_6001949C6548_hostname \"Hostname [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Hostname)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_BuildDateTime \"FW Build Date [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS2:state:JSONPATH($.StatusFWR.BuildDateTime)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Vcc \"Vcc [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Vcc)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Time \"Time [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Time)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_SSId \"SSId [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Wifi.SSId)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_IPAddress \"IPAddress [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.IPAddress)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Mac \"Mac [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Mac)]\", autoupdate=\"true\" } String prod_sonoff_6001949C6548_Subnetmask \"Subnetmask [%s]\" <info> (Sonoff) { mqtt=\"<[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Subnetmask)]\", autoupdate=\"true\" } OK. Haven't you noticed something? Look closer. :) That's it, the topics: sonoff/6001949C6548/stat/ STATUS5 sonoff/6001949C6548/stat/ STATUS11 sonoff/6001949C6548/stat/ STATUS2 This information isn't posted automatically, or after a certain amount of time. In order to receive them you have to publish 0 to sonoff/6001949C6548/cmnd/STATUS topic. You can try this with mosquitto_sub command: mosquitto_pub -h 172.16.0.250 -u ***** -P ***** -t sonoff/B4E62D14BE5A/cmnd/STATUS -m 0 And the result (from a POW module): sonoff/B4E62D14BE5A/stat/STATUS {\"Status\":{\"Module\":6,\"FriendlyName\":[\"Sonoff\"],\"Topic\":\"sonoff\",\"ButtonTopic\":\"0\",\"Power\":1,\"PowerOnState\":1,\"LedState\":6,\"SaveData\":1,\"SaveState\":1,\"ButtonRetain\":0,\"PowerRetain\":0}} sonoff/B4E62D14BE5A/stat/STATUS1 {\"StatusPRM\":{\"Baudrate\":115200,\"GroupTopic\":\"sonoffs\",\"OtaUrl\":\"http://sonoff.maddox.co.uk/tasmota/sonoff.bin\",\"RestartReason\":\"Software/System restart\",\"Uptime\":\"19T13:57:19\",\"StartupUTC\":\"2018-07-11T03:38:44\",\"Sleep\":0,\"BootCount\":18,\"SaveCount\":171,\"SaveAddress\":\"F9000\"}} sonoff/B4E62D14BE5A/stat/STATUS2 {\"StatusFWR\":{\"Version\":\"5.14.0\",\"BuildDateTime\":\"2018-05-15T15:29:54\",\"Boot\":31,\"Core\":\"2_3_0\",\"SDK\":\"1.5.3(aec24ac9)\"}} sonoff/B4E62D14BE5A/stat/STATUS3 {\"StatusLOG\":{\"SerialLog\":2,\"WebLog\":2,\"SysLog\":0,\"LogHost\":\"\",\"LogPort\":514,\"SSId\":[\"Vinyo-Net\",\"\"],\"TelePeriod\":30,\"SetOption\":[\"00008009\",\"55818000\"]}} sonoff/B4E62D14BE5A/stat/STATUS4 {\"StatusMEM\":{\"ProgramSize\":526,\"Free\":476,\"Heap\":19,\"ProgramFlashSize\":1024,\"FlashSize\":4096,\"FlashMode\":3}} sonoff/B4E62D14BE5A/stat/STATUS5 {\"StatusNET\":{\"Hostname\":\"sonoff_B4E62D14BE5A\",\"IPAddress\":\"172.20.1.9\",\"Gateway\":\"172.16.0.1\",\"Subnetmask\":\"255.240.0.0\",\"DNSServer\":\"172.16.0.1\",\"Mac\":\"B4:E6:2D:14:BE:5A\",\"Webserver\":2,\"WifiConfig\":2}} sonoff/B4E62D14BE5A/stat/STATUS6 {\"StatusMQT\":{\"MqttHost\":\"172.16.0.250\",\"MqttPort\":1883,\"MqttClientMask\":\"DVES_%06X\",\"MqttClient\":\"DVES_14BE5A\",\"MqttUser\":\"vinyo\",\"MqttType\":1,\"MAX_PACKET_SIZE\":1000,\"KEEPALIVE\":15}} sonoff/B4E62D14BE5A/stat/STATUS7 {\"StatusTIM\":{\"UTC\":\"Mon Jul 30 17:36:03 2018\",\"Local\":\"Mon Jul 30 19:36:03 2018\",\"StartDST\":\"Sun Mar 25 02:00:00 2018\",\"EndDST\":\"Sun Oct 28 03:00:00 2018\",\"Timezone\":2,\"Sunrise\":\"06:21\",\"Sunset\":\"21:32\"}} sonoff/B4E62D14BE5A/stat/STATUS9 {\"StatusPTH\":{\"PowerDelta\":80,\"PowerLow\":0,\"PowerHigh\":0,\"VoltageLow\":0,\"VoltageHigh\":0,\"CurrentLow\":0,\"CurrentHigh\":0}} sonoff/B4E62D14BE5A/stat/STATUS10 {\"StatusSNS\":{\"Time\":\"2018-07-30T19:36:03\",\"ENERGY\":{\"Total\":40.988,\"Yesterday\":2.198,\"Today\":0.058,\"Power\":3,\"Factor\":0.09,\"Voltage\":220,\"Current\":0.161}}} sonoff/B4E62D14BE5A/stat/STATUS11 {\"StatusSTS\":{\"Time\":\"2018-07-30T19:36:03\",\"Uptime\":\"19T13:57:19\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":52,\"APMac\":\"6C:3B:6B:A0:D2:79\"}}} All these information can be displayed in OpenHAB if you want. Only one thing left: as I mentioned before these topics aren't updated automatically, so you need a button for update them, but first an item must be created: Switch prod_sonoff_6001949C6548_update \"UpdateInfo\" <update> (Sonoff) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/STATUS:command:*:0]\", autoupdate=\"false\"} Regardless of the switch command (ON/OFF) we have to publish 0 to the device: command:*:0","title":"4.1.3. Other Examples"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#42-openhab-sitemap","text":"Now we have the items configured, but I think it would be useful to display them in the OpenHAB. :) To do that we have to configure our sitemap, as well. Since this post is not an 'OpenHAB how to...', I will show you only one way to use the configured items.","title":"4.2. OpenHAB Sitemap"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#421-power-switches","text":"Maybe the most useful step if I post the configuration of my 4CH device: Frame label=\"Switches\" { Text item=prod_sonoff_6001949C6548_lwt Switch item=prod_sonoff_6001949C6548_switch1 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] Switch item=prod_sonoff_6001949C6548_switch2 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] Switch item=prod_sonoff_6001949C6548_switch3 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] Switch item=prod_sonoff_6001949C6548_switch4 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"] } The only interesting thing in this configuration is the visibility part. Visibility configurations are intended to hide the switches when the device is not in \"Online\" state. You can skip this part, but I think it makes no sense to switch an unavailable device.","title":"4.2.1. Power Switches"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#422-display-information","text":"Frame label=\"Info\" { Text item=prod_sonoff_6001949C6548__RSSI Text item=prod_sonoff_6001949C6548_uptime Switch item=prod_sonoff_6001949C6548_update mappings=[ON=\"Go!\"] Text item=prod_sonoff_6001949C6548_Time visibility=[prod_sonoff_6001949C6548_Time!=\"NULL\"] Text item=prod_sonoff_6001949C6548_fwver visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"] Text item=prod_sonoff_6001949C6548_hostname visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"] Text item=prod_sonoff_6001949C6548_BuildDateTime visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"] Text item=prod_sonoff_6001949C6548_Vcc visibility=[prod_sonoff_6001949C6548_Vcc!=\"NULL\"] Text item=prod_sonoff_6001949C6548_SSId visibility=[prod_sonoff_6001949C6548_SSId!=\"NULL\"] Text item=prod_sonoff_6001949C6548_IPAddress visibility=[prod_sonoff_6001949C6548_IPAddress!=\"NULL\"] Text item=prod_sonoff_6001949C6548_Mac visibility=[prod_sonoff_6001949C6548_Mac!=\"NULL\"] Text item=prod_sonoff_6001949C6548_Subnetmask visibility=[prod_sonoff_6001949C6548_Subnetmask!=\"NULL\"] } // END : label=\"Info\" The RSSI and uptime values are posted with the telemetry information, so these are updated regularly. But the items after the Swtich item are updated only when you post 0 to the STATUS topic (Section: 4.1.3. Other Examples). To make it much more understandable here is the item configuration again: Switch prod_sonoff_6001949C6548_update \"UpdateInfo\" <update> (Sonoff) { mqtt=\">[openhabPI:sonoff/6001949C6548/cmnd/STATUS:command:*:0]\", autoupdate=\"false\"} And all these items are displayed only if they are not \"NULL\". I hope it is clear. If you read this article carefully you can configure your own device. Finally here are two screenshot about how it should look like:","title":"4.2.2. Display Information"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#5-use-tasmota-fw-with-your-own-setup","text":"And last but not least, this is my bonus chapter. :) What is this chapter about? Since Tasmota is an brilliant OpenSource software and Sonoff devices are base on ESP8266 OpenSource hardware you can build your own smart home switch. You will need: ESP8266 module (ESP01, ESP07, NodeMCU devkit, etc). Relay (1CH, 2CH, etc) 5V 5V Power Supply 5V to 3V3 converter etc. :) Please take a look at my 4CH setup in the beginning of this post for details. Of course you can create your own PCB, as well. :) To customize Tasmota you will need an IDE (**I**ntegrated **D**evelopment **E**nvironment). If you do a google search for \"compile tasmota\" the first hit is exactly what we need. :) Beginner Guide Create your own Firmware Build So please forgive me, but I don't bother with a guide of \"How to install and configure Atom\", especially because it is only a few steps. As I mentioned, some times, I'm not a developer so maybe my explanations are not always 100% right or clear, but always try to give you working solutions.","title":"5. Use Tasmota FW With Your Own Setup"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#51-user_configh","text":"It this file you can pre-configure some values, most of them are configurable via the web interface. Firstly, the most important thing is the Wifi set up. Why? Please, imagine the situation when you use an ESP01 and you used up all its GPIO port for switching relays. Why is it a problem? Because you don't have any GPIO pin left for a button, which is essential to put the device in Wifi config mode. You can pre-configure two different stations: #define STA_SSID1 \"My Wifi Station\" #define STA_PASS1 \"UnbrakeablePassword\" #define STA_SSID2 \"\" #define STA_PASS2 \"\" After you compile your firmware with these setup the device will automatically connect to your wifi network. If you scroll down a bit in the user_config.h , you can find the MQTT related settings. There is no reason to leave it blank. :) Example: #define MQTT_USE 1 #define MQTT_HOST \"172.16.0.250\" #define MQTT_FINGERPRINT1 \"A5 02 FF 13 99 9F 8B 39 8E F1 83 4F 11 23 65 0B 32 36 FC 07\" #define MQTT_FINGERPRINT2 \"A5 02 FF 13 99 9F 8B 39 8E F1 83 4F 11 23 65 0B 32 36 FC 07\" #define MQTT_PORT 1883 #define MQTT_USER \"userName\" #define MQTT_PASS \"Password\" Please modify only the necessary fields. Moreover, you can configure the topic: #define MQTT_FULLTOPIC \"%topic%//%prefix%/\" Some other exciting options: NTP server: #define NTP_SERVER1 \"pool.ntp.org\" #define NTP_SERVER2 \"nl.pool.ntp.org\" #define NTP_SERVER3 \"0.nl.pool.ntp.org\" Time Zone: #define APP_TIMEZONE 1 Switch Mode: #define SWITCH_MODE TOGGLE // [SwitchMode] TOGGLE, FOLLOW, FOLLOW_INV, PUSHBUTTON, PUSHBUTTON_INV, PUSHBUTTONHOLD, PUSHBUTTONHOLD_INV, PUSHBUTTON_TOGGLE (the wall switch state) MQTT retain: #define MQTT_TELE_RETAIN 0 // Tele messages may send retain flag (0 = off, 1 = on) And so on, and so on.... Please scroll down the file and configure everything you need. One more thing: In this file you have to configure which sensors you plan to use with your device, but do not select too many of them, because you can easily run into OOM exception.","title":"5.1. user_config.h"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#52-sonoff_templateh","text":"Here I had the feeling that I should know more about programming. :) First create your own template. Example: { \"Sonoff Custom\", // Sonoff Basic (ESP8266) GPIO_REL1_INV, // GPIO00 Button GPIO_USER, // GPIO01 Serial RXD and Optional sensor GPIO_REL2_INV, // GPIO02 GPIO_USER, // GPIO03 Serial TXD and Optional sensor GPIO_USER, // GPIO04 Optional sensor 0, // GPIO05 GPIO_USER, // GPIO06 (SD_CLK Flash) 0, // GPIO07 (SD_DATA0 Flash QIO/DIO/DOUT) 0, // GPIO08 (SD_DATA1 Flash QIO/DIO/DOUT) 0, // GPIO09 (SD_DATA2 Flash QIO) 0, // GPIO10 (SD_DATA3 Flash QIO) 0, // GPIO11 (SD_CMD Flash) 0, // GPIO12 Red Led and Relay (0 = Off, 1 = On) GPIO_LED1_INV, // GPIO13 Green Led (0 = On, 1 = Off) GPIO_USER, // GPIO14 Optional sensor 0, // GPIO15 0, // GPIO16 0 // ADC0 Analog input }, As you can see I modified the template of the \"Basic\" module. My first goal was to use an ESP01 with Tasmota. ESP01 modules have only 2 usable GPIO pins (GPIO0 and GPIO2). Both of them are connected to the relay: GPIO_REL1_INV , GPIO_REL2_INV . Originally the GPIO0 was used to connect it with a button (I haven't modified the comment here on purpose for demonstration.) What functionality can be used for the PINs? You can find all available functions here: // User selectable GPIO functionality enum UserSelectablePins { GPIO_NONE, // Not used GPIO_DHT11, // DHT11 GPIO_DHT22, // DHT21, DHT22, AM2301, AM2302, AM2321 ... ... GPIO_REL1, // Relays GPIO_REL2, ... ... GPIO_REL1_INV, GPIO_REL2_INV, ... ... So what is the difference between GPIO_REL1 and GPIO_REL1_INV ? This configuration is related to NO (**N**ormally **O**pen) and NC (**N**ormally **C**losed) setup of the relay (if applicable). Most relays have 3 connections: NC, NO, COM (common). When you set the PIN to low: NC means that the circuit is closed. NO means that the circuit is open. When you set the PIN to high the meanings of NC an NO are reverse. So the usage of GPIO_REL1 and GPIO_REL1_INV depends on your hardware setup. If you connect your stuff (which you want to turn OFF and ON) to NO you want to use the inverse version, because the relay will close the circuit when the PIN is put to low (0) state. If you are using NC , you should choose the GPIO_REL1 . I hope this is clear for you, if not, give it a try. :) There is special functionality which is the GPIO_USER . If you set a GPIO pin to this, you will be able to select its functionality on the web interface: And where does the list come from? // Text in webpage Module Parameters and commands GPIOS and GPIO const char kSensorNames[] PROGMEM = D_SENSOR_NONE \"|\" ... ...","title":"5.2. sonoff_template.h"},{"location":"old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#53-example","text":"Maybe it will be more clear if I show you an example. For demonstration I used a NodeMCU DevKit v0.9, 1CH 5V relay and a DHT22 sensor. Hardware connections: DHT22 VCC --> 3V3 GND --> GND DATA --> D2 Relay VCC --> 5V GND --> GND INPUT --> D3 To be able to choose the appropriate GPIO pins we have to know what D2 and D3 means. You can see in the picture that D1 is actually the GPIO5 and D2 is the GPIO4. So the following modifications are needed in the sonoff_template.h file: --- comp/Sonoff-Tasmota-development/sonoff/sonoff_template.h 2018-07-31 20:10:22.000000000 +0200 +++ Sonoff-Tasmota-development/sonoff/sonoff_template.h 2018-08-01 19:45:49.785921996 +0200 @@ -157,6 +157,7 @@ // Supported hardware modules enum SupportedModules { SONOFF_BASIC, + BLOGTST, SONOFF_RF, SONOFF_SV, SONOFF_TH, @@ -220,6 +221,7 @@ const uint8_t kNiceList[MAXMODULE] PROGMEM = { SONOFF_BASIC, + BLOGTST, SONOFF_RF, SONOFF_TH, SONOFF_DUAL, @@ -288,6 +290,26 @@ 0, // GPIO16 0 // ADC0 Analog input }, + { \"Sonoff Blogtst\", // Sonoff Basic (ESP8266) + GPIO_KEY1, // GPIO00 Button + GPIO_USER, // GPIO01 Serial RXD and Optional sensor + 0, // GPIO02 + GPIO_USER, // GPIO03 Serial TXD and Optional sensor + GPIO_DHT22, // GPIO04 Optional sensor + GPIO_REL1, // GPIO05 + 0, // GPIO06 (SD_CLK Flash) + 0, // GPIO07 (SD_DATA0 Flash QIO/DIO/DOUT) + 0, // GPIO08 (SD_DATA1 Flash QIO/DIO/DOUT) + 0, // GPIO09 (SD_DATA2 Flash QIO) + 0, // GPIO10 (SD_DATA3 Flash QIO) + 0, // GPIO11 (SD_CMD Flash) + 0, // GPIO12 Red Led and Relay (0 = Off, 1 = On) + GPIO_LED1_INV, // GPIO13 Green Led (0 = On, 1 = Off) + GPIO_USER, // GPIO14 Optional sensor + 0, // GPIO15 + 0, // GPIO16 + 0 // ADC0 Analog input + }, { \"Sonoff RF\", // Sonoff RF (ESP8266) GPIO_KEY1, // GPIO00 Button GPIO_USER, // GPIO01 Serial RXD and Optional sensor @@ -970,4 +992,4 @@ */ -#endif // _SONOFF_TEMPLATE_H_ \\ No newline at end of file +#endif // _SONOFF_TEMPLATE_H_ I defined a new template for my setup with \"Sonoff Blogtst\" name based on the \"Sonoff Basic\" module template. The following 3 lines were modified (GPIO04,05,12): + GPIO_DHT22, // GPIO04 Optional sensor + GPIO_REL1, // GPIO05 + 0, // GPIO12 Red Led and Relay (0 = Off, 1 = On) GPIO04 is connected to my DHT22 sensor. GPIO05 is connected to the relay \"IN\" pin. Originally the Sonoff Basic modules use the GPIO12 to control the relay, but we don't, so set it to \"0\". Important If you define a completely new template you have to add two additional item to two different array: kNiceList , SupportedModules . enum SupportedModules { SONOFF_BASIC, + BLOGTST, const uint8_t kNiceList[MAXMODULE] PROGMEM = { SONOFF_BASIC, + BLOGTST, And in the right order! Example: if you write your new template definition before the \"Sonoff Basic\" definition, you should put BLOGTST before SONOFF_BASIC in the kNiceList and SupportedModules arrays, as well. You are almost done. Since the NodeMCU DevKit has \"user\" button we have two options: Configure the firmware to use this button. In this scenario you can use the user button to put the device to WiFi configuration mode. If you scroll back a bit to the picture about the pinout you can see that the button is connected to GPIO16. If you choose this option set the GPIO16 from 0 to GPIO_KEY1 . Configure the WiFi parameters in the user_config.h . To do this you should simply modify the STA_SSID1 and STA_PASS1 . Example: --- comp/Sonoff-Tasmota-development/sonoff/user_config.h 2018-07-31 20:10:22.000000000 +0200 +++ Sonoff-Tasmota-development/sonoff/user_config.h 2018-08-01 19:34:45.629732934 +0200 @@ -59,8 +59,8 @@ #define WIFI_SUBNETMASK \"255.255.255.0\" // [IpAddress3] If not using DHCP set Network mask #define WIFI_DNS \"192.168.2.27\" // [IpAddress4] If not using DHCP set DNS IP address (might be equal to WIFI_GATEWAY) -#define STA_SSID1 \"\" // [Ssid1] Wifi SSID -#define STA_PASS1 \"\" // [Password1] Wifi password +#define STA_SSID1 \"**********\" // [Ssid1] Wifi SSID +#define STA_PASS1 \"**********\" // [Password1] Wifi password #define STA_SSID2 \"\" // [Ssid2] Optional alternate AP Wifi SSID #define STA_PASS2 \"\" // [Password2] Optional alternate AP Wifi password #define WIFI_CONFIG_TOOL WIFI_WAIT // [WifiConfig] Default tool if wifi fails to connect To use DHCP (default) take a look at this line: #define WIFI_IP_ADDRESS \"0.0.0.0\" // [IpAddress1] Set to 0.0.0.0 for using DHCP or IP address Next steps: build! & upload. :) I've already written about uploading the firmware, but here is the example: sudo esptool.py --port /dev/ttyUSB0 write_flash -fs 1MB -fm dout 0x0 firmware.bin Open the web interface (configure module option) and select the newly configured template: If everything is fine, after the reboot you should see the temperature and humidity values on the main page: And, of course you can turn the relay on and off with the \"Toggle\" button. In a nutshell this is how you can use Tasmota firmware with your own hardware setup in nutshell. Maybe my explanations are not always the best, but I really hope this post is useful for you in case you want to work with Sonoff / Tasmota / OpenHAB or both of them... :) If you understand all these things I'm pretty sure you can build you own setup, even with custom hardware. I think at this time Sonoff+Tasmota is the cheapest solution to control equipment with OpenHab. Maybe with custom hardware setup could be cheaper but not much, and you should be aware of the time of assembling.","title":"5.3. Example"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/","text":"Compile Apache HTTPD 2.4.X & PHP \u00b6 Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! What Is Needed / Requirements \u00b6 Compiling apache from source is very easy you have to follow some steps. In case of almost all linux distributions Apache http server can be installed via its package manager. There are some advantages of compiling Apache (or any other application) from source, but If you don't have any special requirement I advise to install Apache from package manager. But yet in certain cases you have to compile from source: You do not have root access. In this case it will be hard to install the necessary packages. :) But If every needed packages are installed you can compile apache. And run it with your own user. If you install apache from package manager, apache will write its log files with root user, and the config files will be writable for only root user. Of course you can give (r/w) permission to other user to these files. I think this scenario is rare, and without root access your possibilities are very limited. You can install different version of the application, which cannot be install via package manager. You can complie the application with certain flags and options which may be missing in the repository. Disadvantages: You have to install all dependencies manually. Your package manager will unaware of the changes. Your applications will have to be updated manually. Maybe you package manager will be overwrite the dependencies. In my case I had to install the following packages: apt-get install libpcre3-dev apt-get install libxml2-dev My OS: Debian GNU/Linux 8 Linux vps10 2.6.32-042stab116.2 #1 SMP Fri Jun 24 15:33:57 MSK 2016 x86_64 GNU/Linux These packages must be downloaded: Apache HTTPD source. APR and APR util PHP 5 Preparation \u00b6 I created a certain user which will be used all over the whole install process and this user will run the apace process. adduser apache2 mkdir /opt/apache2 chown apache2:apache2 /opt/apache2/ I modified this user's shell to /bin/false and home directory to /opt/apache2 . root@vps10:/home/vinyo# cat /etc/passwd|grep apa apache2:x:1002:1002:,,,:/opt/apache2/:/bin/false So this user won't be able to login to the linux box, but you can use sudo to switch to apache2 user: sudo -u apache2 bash Download the necessary sources \u00b6 I put all sources to: /opt/apache2/sources cd /opt/apache2/ mkdir sources cd sources/ wget http://xenia.sote.hu/ftp/mirrors/www.apache.org//httpd/httpd-2.4.23.tar.gz wget http://xenia.sote.hu/ftp/mirrors/www.apache.org//apr/apr-1.5.2.tar.gz wget http://xenia.sote.hu/ftp/mirrors/www.apache.org//apr/apr-util-1.5.4.tar.gz wget http://fr2.php.net/distributions/php-5.6.25.tar.gz Compile Apache whit apr and apr-util \u00b6 First you have to untar the apache source cd /opt/apache2/sources/ tar xf httpd-2.4.23.tar.gz Untar apr, apr-util cd /opt/apache2/sources/httpd-2.4.23/srclib tar xf /opt/apache2/sources/apr-1.5.2.tar.gz tar xf /opt/apache2/sources/apr-util-1.5.4.tar.gz Rename APR directories mv apr-1.5.2 apr mv apr-util-1.5.4 apr-util Compile Apache cd /opt/apache2/sources/httpd-2.4.23 ./configure --enable-mpms-shared=all --enable-ssl --prefix=/opt/apache2/httpd-2.4.23 make make install Apache will be installed to /opt/apache2/httpd-2.4.23 . After the make install command you can start you newly installed apache httpd server. But if you try to start apache you will be get this error: apache2@vps10:/opt/apache2/httpd-2.4.23/bin$ ./apachectl start (13)Permission denied: AH00072: make_sock: could not bind to address [::]:80 (13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:80 no listening sockets available, shutting down AH00015: Unable to open logs Yes, only root user can bind ports below 1025. So we have to modify listen port in /opt/apache2/httpd-2.4.23/conf/httpd.conf file. From: Listen 80 To: Listen 8080 After that little modification apache can be started. Check the log file: cat /opt/apache2/httpd-2.4.23/logs/error_log [Sat Aug 27 12:02:53.021908 2016] [mpm_event:notice] [pid 30164:tid 140121349551872] AH00489: Apache/2.4.23 (Unix) PHP/5.6.25 configured -- resuming normal operations [Sat Aug 27 12:02:53.021972 2016] [core:notice] [pid 30164:tid 140121349551872] AH00094: Command line: '/opt/apache2/httpd-2.4.23/bin/httpd' Now your apache web server is accessible on port 8080, you can call it from your browser. Install PHP \u00b6 Previously we downloaded the php source, now untar it: cd /opt/apache2/sources tar xf php-5.6.25.tar.gz Compile ./configure --with-apxs2=/opt/apache2/httpd-2.4.23/bin/apxs --with-mysql --prefix=/opt/apache2/php-5.6.25 make make install This will create a new directory: /opt/apache2/php-5.6.25 This is the install path of php. During the installation php will modify your apache configuration to load php module: LoadModule php5_module modules/libphp5.so And you can find the php module in apache modules directory: apache2@vps10:/opt/apache2/httpd-2.4.23/modules$ ls -al | grep php -rwxr-xr-x 1 apache2 apache2 28575008 Aug 27 09:09 libphp5.so Before you restart your apache2 instance place your php.ini to the appropriate place: apache2@vps10:/opt/apache2/php-5.6.25/lib$ cp /opt/apache2/sources/php-5.6.25/php.ini-production php.ini To check if php is working place info.php file to your htdocs directory with this content: apache2@vps10:/opt/apache2/httpd-2.4.23/htdocs$ cat info.php <?php phpinfo(); ?> You can call this file using lynx: lynx --dump http://localhost:8080/info.php | grep php.ini Configuration File (php.ini) Path /opt/apache2/php-5.6.25/lib Loaded Configuration File /opt/apache2/php-5.6.25/lib/php.ini Summary \u00b6 As you can see \"compiling apache from source\" consist of a few steps, but after that if you need any additional php module you will have to install them manually by: Using /opt/apache2/php-5.6.25/bin/ pear Using /opt/apache2/php-5.6.25/bin/ pecl Compile from source (phpize, configure, make, make install) If you can afford, consider using your package manager to install apache & php, because it is much simpler and in most cases you won't need to compile from source.","title":"Compile Apache HTTPD 2.4.X & PHP"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#compile-apache-httpd-24x-php","text":"Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!","title":"Compile Apache HTTPD 2.4.X &amp; PHP"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#what-is-needed-requirements","text":"Compiling apache from source is very easy you have to follow some steps. In case of almost all linux distributions Apache http server can be installed via its package manager. There are some advantages of compiling Apache (or any other application) from source, but If you don't have any special requirement I advise to install Apache from package manager. But yet in certain cases you have to compile from source: You do not have root access. In this case it will be hard to install the necessary packages. :) But If every needed packages are installed you can compile apache. And run it with your own user. If you install apache from package manager, apache will write its log files with root user, and the config files will be writable for only root user. Of course you can give (r/w) permission to other user to these files. I think this scenario is rare, and without root access your possibilities are very limited. You can install different version of the application, which cannot be install via package manager. You can complie the application with certain flags and options which may be missing in the repository. Disadvantages: You have to install all dependencies manually. Your package manager will unaware of the changes. Your applications will have to be updated manually. Maybe you package manager will be overwrite the dependencies. In my case I had to install the following packages: apt-get install libpcre3-dev apt-get install libxml2-dev My OS: Debian GNU/Linux 8 Linux vps10 2.6.32-042stab116.2 #1 SMP Fri Jun 24 15:33:57 MSK 2016 x86_64 GNU/Linux These packages must be downloaded: Apache HTTPD source. APR and APR util PHP 5","title":"What Is Needed / Requirements"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#preparation","text":"I created a certain user which will be used all over the whole install process and this user will run the apace process. adduser apache2 mkdir /opt/apache2 chown apache2:apache2 /opt/apache2/ I modified this user's shell to /bin/false and home directory to /opt/apache2 . root@vps10:/home/vinyo# cat /etc/passwd|grep apa apache2:x:1002:1002:,,,:/opt/apache2/:/bin/false So this user won't be able to login to the linux box, but you can use sudo to switch to apache2 user: sudo -u apache2 bash","title":"Preparation"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#download-the-necessary-sources","text":"I put all sources to: /opt/apache2/sources cd /opt/apache2/ mkdir sources cd sources/ wget http://xenia.sote.hu/ftp/mirrors/www.apache.org//httpd/httpd-2.4.23.tar.gz wget http://xenia.sote.hu/ftp/mirrors/www.apache.org//apr/apr-1.5.2.tar.gz wget http://xenia.sote.hu/ftp/mirrors/www.apache.org//apr/apr-util-1.5.4.tar.gz wget http://fr2.php.net/distributions/php-5.6.25.tar.gz","title":"Download the necessary sources"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#compile-apache-whit-apr-and-apr-util","text":"First you have to untar the apache source cd /opt/apache2/sources/ tar xf httpd-2.4.23.tar.gz Untar apr, apr-util cd /opt/apache2/sources/httpd-2.4.23/srclib tar xf /opt/apache2/sources/apr-1.5.2.tar.gz tar xf /opt/apache2/sources/apr-util-1.5.4.tar.gz Rename APR directories mv apr-1.5.2 apr mv apr-util-1.5.4 apr-util Compile Apache cd /opt/apache2/sources/httpd-2.4.23 ./configure --enable-mpms-shared=all --enable-ssl --prefix=/opt/apache2/httpd-2.4.23 make make install Apache will be installed to /opt/apache2/httpd-2.4.23 . After the make install command you can start you newly installed apache httpd server. But if you try to start apache you will be get this error: apache2@vps10:/opt/apache2/httpd-2.4.23/bin$ ./apachectl start (13)Permission denied: AH00072: make_sock: could not bind to address [::]:80 (13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:80 no listening sockets available, shutting down AH00015: Unable to open logs Yes, only root user can bind ports below 1025. So we have to modify listen port in /opt/apache2/httpd-2.4.23/conf/httpd.conf file. From: Listen 80 To: Listen 8080 After that little modification apache can be started. Check the log file: cat /opt/apache2/httpd-2.4.23/logs/error_log [Sat Aug 27 12:02:53.021908 2016] [mpm_event:notice] [pid 30164:tid 140121349551872] AH00489: Apache/2.4.23 (Unix) PHP/5.6.25 configured -- resuming normal operations [Sat Aug 27 12:02:53.021972 2016] [core:notice] [pid 30164:tid 140121349551872] AH00094: Command line: '/opt/apache2/httpd-2.4.23/bin/httpd' Now your apache web server is accessible on port 8080, you can call it from your browser.","title":"Compile Apache whit apr and apr-util"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#install-php","text":"Previously we downloaded the php source, now untar it: cd /opt/apache2/sources tar xf php-5.6.25.tar.gz Compile ./configure --with-apxs2=/opt/apache2/httpd-2.4.23/bin/apxs --with-mysql --prefix=/opt/apache2/php-5.6.25 make make install This will create a new directory: /opt/apache2/php-5.6.25 This is the install path of php. During the installation php will modify your apache configuration to load php module: LoadModule php5_module modules/libphp5.so And you can find the php module in apache modules directory: apache2@vps10:/opt/apache2/httpd-2.4.23/modules$ ls -al | grep php -rwxr-xr-x 1 apache2 apache2 28575008 Aug 27 09:09 libphp5.so Before you restart your apache2 instance place your php.ini to the appropriate place: apache2@vps10:/opt/apache2/php-5.6.25/lib$ cp /opt/apache2/sources/php-5.6.25/php.ini-production php.ini To check if php is working place info.php file to your htdocs directory with this content: apache2@vps10:/opt/apache2/httpd-2.4.23/htdocs$ cat info.php <?php phpinfo(); ?> You can call this file using lynx: lynx --dump http://localhost:8080/info.php | grep php.ini Configuration File (php.ini) Path /opt/apache2/php-5.6.25/lib Loaded Configuration File /opt/apache2/php-5.6.25/lib/php.ini","title":"Install PHP"},{"location":"old/linux/Compile_Apache_Httpd_2.4.x_Php/#summary","text":"As you can see \"compiling apache from source\" consist of a few steps, but after that if you need any additional php module you will have to install them manually by: Using /opt/apache2/php-5.6.25/bin/ pear Using /opt/apache2/php-5.6.25/bin/ pecl Compile from source (phpize, configure, make, make install) If you can afford, consider using your package manager to install apache & php, because it is much simpler and in most cases you won't need to compile from source.","title":"Summary"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Create Self Singet Certificate \u00b6 Easyest Way \u00b6 You can create Self Signed Certificate for you web server with just one command: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mysitename.key -out mysitename.crt References: https://www.sslshopper.com/article-how-to-create-and-install-an-apache-self-signed-certificate.html https://httpd.apache.org/docs/2.4/ssl/ssl_faq.html With CSR (Certificate Signing Request) - DES3 \u00b6 Honestly there is no real difference between this and the previous method, if you use a self signed certificate. But if you create CSR you can send it to Certifying Authority (CA) to be signed. And this method is useful when you want to use the same key with different certs. Generate Private Key openssl genrsa -des3 -out example.key 2048 I recommend that create at lease 2048 bit key. openssl genrsa -des3 -out example.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ ............++++++ e is 65537 (0x10001) Enter pass phrase for example.key: Verifying - Enter pass phrase for example.key: Generate a CSR openssl req -new -key example.key -out example.csr Output: openssl req -new -key example.key -out example.csr Enter pass phrase for example.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:HU State or Province Name (full name) [Some-State]:SomeState Locality Name (eg, city) []:City Organization Name (eg, company) [Internet Widgits Pty Ltd]:SomeState's Company Organizational Unit Name (eg, section) []:Technology Common Name (e.g. server FQDN or YOUR name) []:example.com Email Address []:no-spam@realmail.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:12345678 An optional company name []: At this point you can send your CSR file to a CA, if you need a \"real\", trusted cert. Remove Passphrase from Key If you skip these steps apache will ask for the passphrase at each startup. cp example.key example.key.org openssl rsa -in example.key.org -out example.key Generating Self-Signed Certificate openssl x509 -req -days 365 -in example.csr -signkey example.key -out example.crt Now you have some new files: ls -lrt total 12 -rw-r--r-- 1 janos.vincze bio 761 Aug 15 12:53 example.csr -rw-r--r-- 1 janos.vincze bio 963 Aug 15 12:59 example.key.org -rw-r--r-- 1 janos.vincze bio 887 Aug 15 12:59 example.key -rw-r--r-- 1 janos.vincze bio 1001 Aug 15 13:03 example.crt But you only need the .key and .crt file to configure apache. With root key CA \u00b6 I don't know if there is anybody who wants to use a root CA key on its own webpage(s). I can imagine one scenario when it can be useful. Inside an organization you can create a root CA key and sign all your certificate with it, then import the CA to all clients. For example, you have many web servers inside your intranet and sign all its certificate with your own CA. Clients inside your network can use these webpages as \"trusted\" provider if the root CA pub key is imported to the browser or to the system. I will show you how to install root CA cert into Firefox and Internet Explorer, but first we need to follow these steps to create the necessary files. Generate ROOT CA openssl genrsa -des3 -out rootCA.key 2048 openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem -config rootCA.conf As you can see we are using a configuration file: rootCA.conf So you first need to create something like this: [req] distinguished_name = req_distinguished_name [req_distinguished_name] countryName = HU countryName_default = HU stateOrProvinceName = Budapest stateOrProvinceName_default = Budapest localityName = Budapest localityName_default = Budapest organizationalUnitName = Technology organizationalUnitName_default = Technology commonName = VinczeJanosRootCA commonName_default = VinczeJanosRootCA organizationName = Some Ltd organizationName_default = Some Ltd. E=jvincze84@gmail.com commonName_max = 64 Generate web server key(s) openssl genrsa -out server1.key 2048 You should generate one key per sites. Generate CSR for the key This step is very similar to the previously mentioned. Generate the CSR: openssl req -sha256 -new -out server1.csr -key server1.key -config config.cnf Backup the original server key: cp server1.key server1.key.org Remove the Passphrase openssl rsa -in server1.key.org -out server1.key You will use this key on the server. NOTE: You can see another config file: config.cnf This is necessary for the server key/crt. And please note that you can use alt.names in the configuration files. This is very useful if you have multiple domain names for one server or virtualhost. For example, you have two domain name: www.server.com and login.server.com. And these names are associated to one apache virtualhost: www.server.com -> ServerName and login.server.com -> ServerAlias. Example Config File: [req] distinguished_name = req_distinguished_name req_extensions = v3_req [req_distinguished_name] countryName = HU countryName_default = HU stateOrProvinceName = Budapest stateOrProvinceName_default = Budapest localityName = Budapest localityName_default = Budapest organizationalUnitName = Technology organizationalUnitName_default = Technology commonName = server1.company.com commonName_default = server1.company.com organizationName = Company Ltd. organizationName_default = Company Ltd. E=boss@company.com commonName_max = 64 [ v3_req ] # Extensions to add to a certificate request subjectAltName = @alt_names [alt_names] DNS.1 = server1.company.com DNS.2 = server2.company.com Sign your csr with the root CA key openssl x509 -req -in server1.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server1.crt -days 3650 -extensions v3_req -extfile config.cnf This command will create the server1.crt which is to be used on Apache webserver. Ok now we have the .key and .crt files. Check the cert: openssl x509 -in server1.crt -text -noout Output: Certificate: Data: Version: 3 (0x2) Serial Number: 87:8b:67:2d:2d:60:2c:48 Signature Algorithm: sha256WithRSAEncryption Issuer: C=HU, ST=Budapest, L=Budapest, OU=Technology, CN=VinczeJanosRootCA, O=Some Ltd. Validity Not Before: Aug 16 09:59:59 2016 GMT Not After : Aug 14 09:59:59 2026 GMT Subject: C=HU, ST=Budapest, L=Budapest, OU=Technology, CN=server1.company.com, O=Company Ltd. Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:9c:ed:ec:7d:b4:bf:4e:ff:3a:ab:ef:d5:a3:fd: a1:a7:96:d0:30:c5:69:f7:a7:6c:91:ef:78:7f:03: e9:48:f3:11:45:12:39:f6:4e:ed:79:60:df:f0:6b: 9a:59:16:7a:22:31:34:c7:10:df:a0:ca:c6:fb:6a: ee:77:a3:6d:89:d2:b3:db:7f:f2:f9:d0:b5:5b:f2: ed:0c:8e:03:85:5d:75:8a:de:29:dd:cd:d6:a8:7b: 8f:2c:5b:77:95:19:b9:da:42:d0:15:d5:c5:20:08: 61:83:2a:18:78:c9:1a:7c:55:df:25:ff:6a:69:53: 09:1a:22:a0:b6:98:63:09:ef:a9:3f:54:56:4d:78: ea:2f:d7:cd:e8:58:8e:08:64:45:59:a5:c4:93:d7: ac:b5:99:1d:5c:7a:3b:6b:85:c7:cb:33:8c:e4:b0: bf:80:f1:cd:d7:68:70:dc:a0:ba:bd:fd:02:d3:36: 3d:11:c9:f9:71:c8:dd:2f:3f:b5:5d:8a:66:2e:34: 33:32:44:b3:49:78:5b:13:f9:8f:6f:42:d1:1f:f5: bb:4d:6f:b1:81:42:c2:93:3c:f2:81:5d:1d:1d:19: a4:40:e2:d1:2c:a5:2e:6d:fa:ad:ff:31:c3:65:58: e3:ba:50:10:80:3e:53:86:ce:0e:43:df:cd:77:dd: f9:f3 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Alternative Name: DNS:server1.company.com, DNS:server2.company.com Signature Algorithm: sha256WithRSAEncryption af:80:32:53:42:9c:8f:9e:4f:4b:e5:05:cc:41:5b:2f:c8:68: 1d:eb:d8:8c:07:56:d3:ba:77:d4:f9:89:7e:ea:28:57:58:59: 9e:df:bd:84:eb:2a:48:06:8e:44:c6:35:52:79:4e:c7:c7:0d: 2d:4c:08:aa:5a:95:2a:10:65:7b:56:59:26:bb:fc:4e:5b:6c: 73:08:18:d0:2b:59:a2:90:78:7c:2f:1d:d7:41:4e:87:59:71: 78:87:59:8f:f9:67:33:ae:d6:77:f0:70:00:38:e5:e8:41:67: a1:b5:1d:33:ff:8a:89:97:99:cd:6c:b2:77:01:57:03:35:a5: 25:0d:4b:19:dd:d3:ed:98:66:0a:c2:94:17:42:68:6f:2a:19: e1:cb:d3:2e:e7:e5:3a:8b:6e:3d:86:51:e9:29:56:9e:7e:b0: 34:96:78:bf:60:8b:db:07:2a:3e:a3:2f:44:2a:70:8f:16:b2: c8:97:31:a0:ea:53:87:48:9d:6d:e3:20:33:c3:68:2a:40:37: 06:cb:fe:4c:01:6f:a2:6a:f1:43:0f:ed:1c:84:4e:a7:4d:a7: 7d:44:21:56:46:94:2f:75:6d:cf:be:1b:46:cd:5c:ef:e6:f6: 6e:9a:53:b5:96:9a:a7:08:73:31:14:27:57:e3:66:63:cd:82: 3a:f3:e0:3c Minimal Apache (1.4) configuration \u00b6 Now we can create an apache self signed certificate with 3 different methods, but as result we have to have one .crt and one .key file. This VirtualHost example redirects all http request to https, and works as a transparent proxy: <VirtualHost *:80> ServerName http://pve.server.com RewriteEngine On RewriteCond %{HTTPS} !=on RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L] </VirtualHost> <VirtualHost *:443> ServerAdmin webmaster@localhost SSLProxyEngine on SSLProxyCheckPeerCN off SSLProxyVerify none SSLProxyCheckPeerName off SSLProxyCheckPeerExpire off SSLProxyProtocol all DocumentRoot /var/www/html ServerName https://pve.server.com SSLEngine on SSLCertificateFile /etc/apache2/cert/pve.crt SSLCertificateKeyFile /etc/apache2/cert/pve.key BrowserMatch \"MSIE [17-9]\" ssl-unclean-shutdown DocumentRoot /var/www <Directory /> Options FollowSymLinks AllowOverride None </Directory> <Directory /var/www/> Options Indexes FollowSymLinks MultiViews Indexes AllowOverride all Order allow,deny allow from all </Directory> ErrorLog ${APACHE_LOG_DIR}/pve-error.log CustomLog ${APACHE_LOG_DIR}/pve-access.log combined ProxyRequests off ProxyPreserveHost on ProxyPass / https://10.30.16.100:8006/ ProxyPassReverse / https://10.30.16.100:8006/ </VirtualHost> Import you root CA key to Firefox \u00b6 If you don't want to get a \"self Signed certificate\" warning in FF you can import you root ca public key to Firefox with a few easy steps. Go to about:preferences , Advanced , Certificate . And Click View Certificates . In the pop-up window Choose Authories and click \" import \" Import your rootCA.pem file. Next time you visit your website FF will trust its certificate.","title":"Create Self Signed Certificate for Apache WebServer"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#create-self-singet-certificate","text":"","title":"Create Self Singet Certificate"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#easyest-way","text":"You can create Self Signed Certificate for you web server with just one command: openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mysitename.key -out mysitename.crt References: https://www.sslshopper.com/article-how-to-create-and-install-an-apache-self-signed-certificate.html https://httpd.apache.org/docs/2.4/ssl/ssl_faq.html","title":"Easyest Way"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#with-csr-certificate-signing-request-des3","text":"Honestly there is no real difference between this and the previous method, if you use a self signed certificate. But if you create CSR you can send it to Certifying Authority (CA) to be signed. And this method is useful when you want to use the same key with different certs. Generate Private Key openssl genrsa -des3 -out example.key 2048 I recommend that create at lease 2048 bit key. openssl genrsa -des3 -out example.key 1024 Generating RSA private key, 1024 bit long modulus ....++++++ ............++++++ e is 65537 (0x10001) Enter pass phrase for example.key: Verifying - Enter pass phrase for example.key: Generate a CSR openssl req -new -key example.key -out example.csr Output: openssl req -new -key example.key -out example.csr Enter pass phrase for example.key: You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [AU]:HU State or Province Name (full name) [Some-State]:SomeState Locality Name (eg, city) []:City Organization Name (eg, company) [Internet Widgits Pty Ltd]:SomeState's Company Organizational Unit Name (eg, section) []:Technology Common Name (e.g. server FQDN or YOUR name) []:example.com Email Address []:no-spam@realmail.com Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []:12345678 An optional company name []: At this point you can send your CSR file to a CA, if you need a \"real\", trusted cert. Remove Passphrase from Key If you skip these steps apache will ask for the passphrase at each startup. cp example.key example.key.org openssl rsa -in example.key.org -out example.key Generating Self-Signed Certificate openssl x509 -req -days 365 -in example.csr -signkey example.key -out example.crt Now you have some new files: ls -lrt total 12 -rw-r--r-- 1 janos.vincze bio 761 Aug 15 12:53 example.csr -rw-r--r-- 1 janos.vincze bio 963 Aug 15 12:59 example.key.org -rw-r--r-- 1 janos.vincze bio 887 Aug 15 12:59 example.key -rw-r--r-- 1 janos.vincze bio 1001 Aug 15 13:03 example.crt But you only need the .key and .crt file to configure apache.","title":"With CSR (Certificate Signing Request) - DES3"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#with-root-key-ca","text":"I don't know if there is anybody who wants to use a root CA key on its own webpage(s). I can imagine one scenario when it can be useful. Inside an organization you can create a root CA key and sign all your certificate with it, then import the CA to all clients. For example, you have many web servers inside your intranet and sign all its certificate with your own CA. Clients inside your network can use these webpages as \"trusted\" provider if the root CA pub key is imported to the browser or to the system. I will show you how to install root CA cert into Firefox and Internet Explorer, but first we need to follow these steps to create the necessary files. Generate ROOT CA openssl genrsa -des3 -out rootCA.key 2048 openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem -config rootCA.conf As you can see we are using a configuration file: rootCA.conf So you first need to create something like this: [req] distinguished_name = req_distinguished_name [req_distinguished_name] countryName = HU countryName_default = HU stateOrProvinceName = Budapest stateOrProvinceName_default = Budapest localityName = Budapest localityName_default = Budapest organizationalUnitName = Technology organizationalUnitName_default = Technology commonName = VinczeJanosRootCA commonName_default = VinczeJanosRootCA organizationName = Some Ltd organizationName_default = Some Ltd. E=jvincze84@gmail.com commonName_max = 64 Generate web server key(s) openssl genrsa -out server1.key 2048 You should generate one key per sites. Generate CSR for the key This step is very similar to the previously mentioned. Generate the CSR: openssl req -sha256 -new -out server1.csr -key server1.key -config config.cnf Backup the original server key: cp server1.key server1.key.org Remove the Passphrase openssl rsa -in server1.key.org -out server1.key You will use this key on the server. NOTE: You can see another config file: config.cnf This is necessary for the server key/crt. And please note that you can use alt.names in the configuration files. This is very useful if you have multiple domain names for one server or virtualhost. For example, you have two domain name: www.server.com and login.server.com. And these names are associated to one apache virtualhost: www.server.com -> ServerName and login.server.com -> ServerAlias. Example Config File: [req] distinguished_name = req_distinguished_name req_extensions = v3_req [req_distinguished_name] countryName = HU countryName_default = HU stateOrProvinceName = Budapest stateOrProvinceName_default = Budapest localityName = Budapest localityName_default = Budapest organizationalUnitName = Technology organizationalUnitName_default = Technology commonName = server1.company.com commonName_default = server1.company.com organizationName = Company Ltd. organizationName_default = Company Ltd. E=boss@company.com commonName_max = 64 [ v3_req ] # Extensions to add to a certificate request subjectAltName = @alt_names [alt_names] DNS.1 = server1.company.com DNS.2 = server2.company.com Sign your csr with the root CA key openssl x509 -req -in server1.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server1.crt -days 3650 -extensions v3_req -extfile config.cnf This command will create the server1.crt which is to be used on Apache webserver. Ok now we have the .key and .crt files. Check the cert: openssl x509 -in server1.crt -text -noout Output: Certificate: Data: Version: 3 (0x2) Serial Number: 87:8b:67:2d:2d:60:2c:48 Signature Algorithm: sha256WithRSAEncryption Issuer: C=HU, ST=Budapest, L=Budapest, OU=Technology, CN=VinczeJanosRootCA, O=Some Ltd. Validity Not Before: Aug 16 09:59:59 2016 GMT Not After : Aug 14 09:59:59 2026 GMT Subject: C=HU, ST=Budapest, L=Budapest, OU=Technology, CN=server1.company.com, O=Company Ltd. Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:9c:ed:ec:7d:b4:bf:4e:ff:3a:ab:ef:d5:a3:fd: a1:a7:96:d0:30:c5:69:f7:a7:6c:91:ef:78:7f:03: e9:48:f3:11:45:12:39:f6:4e:ed:79:60:df:f0:6b: 9a:59:16:7a:22:31:34:c7:10:df:a0:ca:c6:fb:6a: ee:77:a3:6d:89:d2:b3:db:7f:f2:f9:d0:b5:5b:f2: ed:0c:8e:03:85:5d:75:8a:de:29:dd:cd:d6:a8:7b: 8f:2c:5b:77:95:19:b9:da:42:d0:15:d5:c5:20:08: 61:83:2a:18:78:c9:1a:7c:55:df:25:ff:6a:69:53: 09:1a:22:a0:b6:98:63:09:ef:a9:3f:54:56:4d:78: ea:2f:d7:cd:e8:58:8e:08:64:45:59:a5:c4:93:d7: ac:b5:99:1d:5c:7a:3b:6b:85:c7:cb:33:8c:e4:b0: bf:80:f1:cd:d7:68:70:dc:a0:ba:bd:fd:02:d3:36: 3d:11:c9:f9:71:c8:dd:2f:3f:b5:5d:8a:66:2e:34: 33:32:44:b3:49:78:5b:13:f9:8f:6f:42:d1:1f:f5: bb:4d:6f:b1:81:42:c2:93:3c:f2:81:5d:1d:1d:19: a4:40:e2:d1:2c:a5:2e:6d:fa:ad:ff:31:c3:65:58: e3:ba:50:10:80:3e:53:86:ce:0e:43:df:cd:77:dd: f9:f3 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Subject Alternative Name: DNS:server1.company.com, DNS:server2.company.com Signature Algorithm: sha256WithRSAEncryption af:80:32:53:42:9c:8f:9e:4f:4b:e5:05:cc:41:5b:2f:c8:68: 1d:eb:d8:8c:07:56:d3:ba:77:d4:f9:89:7e:ea:28:57:58:59: 9e:df:bd:84:eb:2a:48:06:8e:44:c6:35:52:79:4e:c7:c7:0d: 2d:4c:08:aa:5a:95:2a:10:65:7b:56:59:26:bb:fc:4e:5b:6c: 73:08:18:d0:2b:59:a2:90:78:7c:2f:1d:d7:41:4e:87:59:71: 78:87:59:8f:f9:67:33:ae:d6:77:f0:70:00:38:e5:e8:41:67: a1:b5:1d:33:ff:8a:89:97:99:cd:6c:b2:77:01:57:03:35:a5: 25:0d:4b:19:dd:d3:ed:98:66:0a:c2:94:17:42:68:6f:2a:19: e1:cb:d3:2e:e7:e5:3a:8b:6e:3d:86:51:e9:29:56:9e:7e:b0: 34:96:78:bf:60:8b:db:07:2a:3e:a3:2f:44:2a:70:8f:16:b2: c8:97:31:a0:ea:53:87:48:9d:6d:e3:20:33:c3:68:2a:40:37: 06:cb:fe:4c:01:6f:a2:6a:f1:43:0f:ed:1c:84:4e:a7:4d:a7: 7d:44:21:56:46:94:2f:75:6d:cf:be:1b:46:cd:5c:ef:e6:f6: 6e:9a:53:b5:96:9a:a7:08:73:31:14:27:57:e3:66:63:cd:82: 3a:f3:e0:3c","title":"With root key CA"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#minimal-apache-14-configuration","text":"Now we can create an apache self signed certificate with 3 different methods, but as result we have to have one .crt and one .key file. This VirtualHost example redirects all http request to https, and works as a transparent proxy: <VirtualHost *:80> ServerName http://pve.server.com RewriteEngine On RewriteCond %{HTTPS} !=on RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L] </VirtualHost> <VirtualHost *:443> ServerAdmin webmaster@localhost SSLProxyEngine on SSLProxyCheckPeerCN off SSLProxyVerify none SSLProxyCheckPeerName off SSLProxyCheckPeerExpire off SSLProxyProtocol all DocumentRoot /var/www/html ServerName https://pve.server.com SSLEngine on SSLCertificateFile /etc/apache2/cert/pve.crt SSLCertificateKeyFile /etc/apache2/cert/pve.key BrowserMatch \"MSIE [17-9]\" ssl-unclean-shutdown DocumentRoot /var/www <Directory /> Options FollowSymLinks AllowOverride None </Directory> <Directory /var/www/> Options Indexes FollowSymLinks MultiViews Indexes AllowOverride all Order allow,deny allow from all </Directory> ErrorLog ${APACHE_LOG_DIR}/pve-error.log CustomLog ${APACHE_LOG_DIR}/pve-access.log combined ProxyRequests off ProxyPreserveHost on ProxyPass / https://10.30.16.100:8006/ ProxyPassReverse / https://10.30.16.100:8006/ </VirtualHost>","title":"Minimal Apache (1.4) configuration"},{"location":"old/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#import-you-root-ca-key-to-firefox","text":"If you don't want to get a \"self Signed certificate\" warning in FF you can import you root ca public key to Firefox with a few easy steps. Go to about:preferences , Advanced , Certificate . And Click View Certificates . In the pop-up window Choose Authories and click \" import \" Import your rootCA.pem file. Next time you visit your website FF will trust its certificate.","title":"Import you root CA key to Firefox"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Install (compile) & Configure motion + AV tools \u00b6 0. Update running system & Install necessary pacgakes \u00b6 As always, start with updating you linux system. apt-get update apt-get upgrade Install packages apt-get install libjpeg-dev libjpeg62-turbo-dev autoconf automake build-essential libzip-dev git yasm nasm pkg-config libavutil-dev libavformat-dev libavcodec-dev libswscale-dev autoconf automake build-essential pkgconf libtool libzip-dev libjpeg62 git libavformat-dev libavcodec-dev libavutil-dev libswscale-dev libavdevice-dev ca-certificates webp libwebp-dev curl lynx zip apache2 libx264-dev x264 libav-tools mpv 1. Install libav12 \u00b6 Note This step is optional , because at the 0. step we installed libav-tools package which contains avconv. But with compiling from source we get a newer version: Repository: avconv version 11.8-6:11.8-1~deb8u1+rpi1 Compiled: avconv version 12, Copyright (c) 2000-2016 the Libav developers 1.1. Download source code from the git repository \u00b6 cd /usr/src git clone https://github.com/todostreaming/libav12 1.2. Configure & make and make install \u00b6 1.2.1. Configure \u00b6 cd /usr/src/libav12 ./configure --enable-libwebp --enable-libx264 --logfile=/root/libav-conf-$(date +%s).log --enable-gpl --prefix=/opt/libav12 | tee -a /root/libav-conf-$(date +%s).out After the configure is done you can check two log files: /root/libav-conf-*.log /root/libav-conf-*.out The configure script must be done without any error. If you see errors in one of the log files please do not continue, and try to fix them. Usually the most error cause by a missing library and easy can be fixed with install the missing lib using apt-get install command. 1.2.2. Compile the source code: \u00b6 To speed up this process lets use -j2 option. (Or if you want -j3 or -j4) make -j2 During this process maybe you will some warning messages, but no errors. So you can skip them. -j [jobs], --jobs[=jobs] Specifies the number of jobs (commands) to run simultaneously. If there is more than one -j option, the last one is effective. If the -j option is given without an argument, make will not limit the number of jobs that can run simultaneously. 1.2.3. Install libav \u00b6 To do this simply run this command: make install The binaries will be available in the directory specified with \"--prefix\" option when we run configure script. In our case: /opt/libav12 References: https://wiki.libav.org https://libav.org https://github.com/todostreaming/libav12 2. Install / Compile FFMpeg \u00b6 FFMpeg is no longer available in Debian repository, so if you want to use it you have to compile on your own. cd /usr/src git clone https://github.com/FFmpeg/FFmpeg root@rpi2camsrv01:/usr/src/FFmpeg# ./configure --prefix=/opt/ffmpeg --enable-libx264 --enable-libwebp --enable-gpl --enable-nonfree make make install Note: The make command runs very long time. Please be patient, and / or try run with -j[234] option. 3. Install / Compile Motion \u00b6 Motion can be easily installed with apt-get install motion , so you my think this post is absolutely useless and has no sense. But if you want to use RTSP stream, you have to have the latest (or at lease 4.x) version of motion. The version of apt-get repository is too old and has not this feature: root@rpi2camsrv01:/opt/motion-3.4.1-316/bin# apt-cache show motion Package: motion Source: motion (3.2.12+git20140228-4) Version: 3.2.12+git20140228-4+b2 By using the git source code we can install the latest version: motion Version 4.0.1+git8a1b9a97 If you want to use motion with mysql support in the future you need some additional packages: apt-get install libmysql++-dev libmysqlclient-dev 3.1. Clone from git \u00b6 /usr/src git clone https://github.com/Motion-Project/motion.git 3.2. Configure \u00b6 Before you start you may want to read the INSTALL guide: less /usr/src/motion/INSTALL You can choose which library do you want to use ffmpeg / libav: cd /usr/src/motion autoreconf -fiv ./configure --prefix=/opt/motion-3.4.1-316 --with-ffmpeg=/opt/ffmpeg #OR ./configure --prefix=/opt/motion-3.4.1-316 --with-ffmpeg=/opt/libav12 You can configure without \"with-ffmpeg\" option, it this case configure script will use the already installed libraries (libav-tool) After the configure script done you will see something like this: ************************** Configure status motion 4.0.1+git8a1b9a9 ************************** OS : Linux pthread support: Yes jpeg support: Yes webp support: No V4L2 support: Yes BKTR support: No MMAL support: Yes ... MMAL_CFLAGS: -std=gnu99 -DHAVE_MMAL -Irasppicam -I/opt/vc/include ... MMAL_OBJ: mmalcam.o raspicam/RaspiCamControl.o raspicam/RaspiCLI.o ... MMAL_LIBS: -L/opt/vc/lib -lmmal_core -lmmal_util -lmmal_vc_client -lvcos -lvchostif -lvchiq_arm FFmpeg support: Yes ... FFMPEG_CFLAGS: -I/opt/libav12/include ... FFMPEG_LIBS: -lswscale -lavdevice -lavformat -lavcodec -lx264 -lwebp -lz -pthread -lavresample -L/opt/libav12/lib -lavutil -lm SQLite3 support: No MYSQL support: Yes PostgreSQL support: No CFLAGS: -g -O2 -I/usr/local/include -g -O2 -D_THREAD_SAFE LIBS: -lm -L/usr/local/lib -pthread -ljpeg -lmysqlclient -lz LDFLAGS: -L/usr/local/lib Install prefix: /opt/motion-3.4.1-316 If you need webp support configure with --with-webp option. The necessary packages are already installed in the 0. step (webp libwebp-dev). /configure --prefix=/opt/motion-3.4.1-316 --with-ffmpeg=/opt/libav12 --with-webp 3.2. Make & Make install \u00b6 Now we have only to easy steps left, run make and make install. make make install Your motion instance is now ready to use, and can be located in /opt/motion-3.4.1-316 . Update - 2017.05.09 \u00b6 If you want to use fmpeg to encode/decode .webm files you need to install these extra packages: apt-get install libvorbis-dev libvpx-dev mjpegtools (optional) apt-get instell vpx-tools imagemagick And configure ffmpeg with these parameters : ./configure --prefix=/opt/ffmpeg-webm --enable-libx264 --enable-libwebp --enable-gpl --enable-nonfree --enable-libvpx --enable-libvorbis 4. Configure Motion to start at boot \u00b6 4.1. Create user for motion service \u00b6 useradd motion root@rpi2camsrv01:/lib/systemd/system# id motion uid=1002(motion) gid=1002(motion) groups=1002(motion) 4.2. Edit motion.service systemd config file \u00b6 An example service file can be found in the motion install directory: /opt/motion-3.4.1-316/share/motion/examples/motion.service Copy this file to systemd directory: cd /lib/systemd/system cp /opt/motion-3.4.1-316/share/motion/examples/motion.service . Create dir for PID file mkdir /opt/motion-3.4.1-316/var Edit motion.sevice file [Unit] Description=Motion daemon After=local-fs.target network.target [Service] User=motion Group=motion PIDFile=/opt/motion-3.4.1-316/var/motion.pid ExecStart=/opt/motion-3.4.1-316/bin/motion -n Type=simple StandardError=null [Install] WantedBy=multi-user.target Change Owner of motion chown -R motion:motion /opt/motion-3.4.1-316/ Try to start motion.service root@rpi2camsrv01:/lib/systemd/system# systemctl start motion.service Check the service root@rpi2camsrv01:/lib/systemd/system# systemctl status motion.service \u00e2\u2014\u008f motion.service - Motion daemon Loaded: loaded (/lib/systemd/system/motion.service; disabled) Active: active (running) since Fri 2017-03-24 09:36:46 UTC; 5s ago Main PID: 12812 (motion) CGroup: /system.slice/motion.service \u00e2\u201d\u201d\u00e2\u201d\u20ac12812 /opt/motion-3.4.1-316/bin/motion -n Stop service and install service file root@rpi2camsrv01:/lib/systemd/system# systemctl stop motion.service root@rpi2camsrv01:/lib/systemd/system# systemctl enable motion.service Created symlink from /etc/systemd/system/multi-user.target.wants/motion.service to /lib/systemd/system/motion.service. Now you should use 'motion' user to configure motion, otherwise the owner of files may be changed causing permission denied. root@rpi2camsrv01:/lib/systemd/system# sudo su - motion No directory, logging in with HOME=/ motion@rpi2camsrv01:/opt/motion-3.4.1-316$ You can create home directory to motion user: mkdir /home/motion chown -R motion:motion /home/motion/ You motion installation is ready for use now. 5. Configure Motion to use an ONVIF IP camera via RTSP stream \u00b6 You can buy low budget IP cameras from eBay, Aliexpress and so on. Nowadays almost all cheap cameras are using ONVIF interface. I have 3 IP cameras and I have to say that the web interfaces of them are really poor. My main problem is that two of them have web interface which can be accessed only with IE, because of ActiveX. :( I hate it because I usually use Linux. Another option to configure you camera is to use some ONVIF manager software. Usually the provider of the camera send a software, as well, but its also need Windows. :( I tried to find some ONVIF manager for Linux, but could not find a really good one. I don't care too much because normally a camera have to be set up once, and then can be used. To use these cameras with MOTION you have to determine the stream URL. If you are lucky you can find it somewhere in the web interface. The following steps are valid only if you have already configured you camera. This means that the cam is connected to your network at least, and you know it IP address. This post doesn't aim to describe 'how to configure your cam', so you have to do it on your own. It is more exciting to find you ONVIF camera's stream URLs. If you don't like my method you can do some googleing to find the URLs of your camera. 5.1. What you need? \u00b6 Installed SOAPUI . Your camera WebService URL. Unfortunately this can be different from mine. Here is two examples: http://172.19.0.3:8080 http://172.19.0.2/onvif/device_service At this step the main problem is that I don't know exact method to find your camera web service URL. If the two example above aren't working you have to do some searching on Google. According to the Official Documentation the URL should be this: The entry point for the device management service is fixed to: http://onvif_host/onvif/device_service 5.2. Load ONVIF device WSDL to soapUI \u00b6 URL: https://www.onvif.org/ver10/device/wsdl/devicemgmt.wsdl 5.3. Modify Endpoint \u00b6 You can specify username password if your camera is using authentication Assign the newly added endpoint to the requests, without this you can specify the endpoint for all requests. Now you can close this window. 5.4. GetServices \u00b6 Find 'GetServices' in the left panel. Modify the request to: <wsdl:IncludeCapability>true</wsdl:IncludeCapability> You will get all web services which supported by your camera. I paste only the relavant information from the request here. <tds:Namespace>http://www.onvif.org/ver10/device/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/devices</tds:XAddr> <tds:Namespace>http://www.onvif.org/ver10/media/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/media</tds:XAddr> <tds:Namespace>http://www.onvif.org/ver10/events/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/events</tds:XAddr> <tds:Namespace>http://www.onvif.org/ver20/analytics/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/analytics</tds:XAddr> We need the media one: http://www.onvif.org/ver10/media/wsdl Copy paste this URL to your browser to get the WSDL location. You will be redirected to this URL: https://www.onvif.org/ver10/media/wsdl/media.wsdl7 Add this WSDL to your soapUI project. For reference please see chapter 5.2. 5.5. MediaBinding / GetProfiles \u00b6 With this you get the profiles of you camera. I don't copy paste the whole response here just some important parts. I have two profiles: <trt:Profiles fixed=\"true\" token=\"MainProfileToken\"> <trt:Profiles fixed=\"true\" token=\"SubProfileToken\"> You can find everything about the profiles for example resolution: <tt:VideoEncoderConfiguration token=\"main_video_encoder_cfg_token\"> <tt:Name>main_video_encoder_cfg</tt:Name> <tt:UseCount>1</tt:UseCount> <tt:Encoding>H264</tt:Encoding> <tt:Resolution> <tt:Width>1920</tt:Width> <tt:Height>1080</tt:Height> </tt:Resolution> I want to use this profile in Motion (MainProfileToken). These profiles can be found on the web interface, but maybe the name is different. 5.6. Get URLS (MediaBinding / GetStreamUri) \u00b6 Now you have to edit this XML before start the request. Open this URL in you browser: https://www.onvif.org/ver10/media/wsdl/media.wsdl And find 'GetStreamUri' operation. You will see how to configure this request. Example: <soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:wsdl=\"http://www.onvif.org/ver10/media/wsdl\" xmlns:sch=\"http://www.onvif.org/ver10/schema\"> <soap:Header/> <soap:Body> <wsdl:GetStreamUri> <wsdl:StreamSetup> <sch:Stream>RTP-Unicast</sch:Stream> <sch:Transport> <sch:Protocol>RTSP</sch:Protocol> <!--Optional:--> <sch:Tunnel/> </sch:Transport> <!--You may enter ANY elements at this point--> </wsdl:StreamSetup> <wsdl:ProfileToken>MainProfileToken</wsdl:ProfileToken> </wsdl:GetStreamUri> </soap:Body> </soap:Envelope> The most important things to note the ProfileToken: <wsdl:ProfileToken>MainProfileToken</wsdl:ProfileToken> Finally we have the stream URL: <tt:Uri>rtsp://172.19.0.3:554/11</tt:Uri> You can try this url in VLC media player. Just for demonstration my <wsdl:ProfileToken>SubProfileToken</wsdl:ProfileToken> address is: <tt:Uri>rtsp://172.19.0.3:554/12</tt:Uri> 5.7. (Bonus) MediaBinding / GetSnapshotUri \u00b6 You can get snapshot images from ONVIF cameras. To get the URL use GetSnapshotUri request: <tt:Uri>http://172.19.0.3:80/web/auto.jpg?-usr=admin&amp;-pwd=admin&amp;</tt:Uri> 5.8. (Bonus) DeviceBinding / GetDeviceInformation \u00b6 Just for fun my last sample request is the 'GetDeviceInformation'. Request: <soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:wsdl=\"http://www.onvif.org/ver10/device/wsdl\"> <soap:Header/> <soap:Body> <wsdl:GetDeviceInformation/> </soap:Body> </soap:Envelope> Response: ... <SOAP-ENV:Header/> <SOAP-ENV:Body> <tds:GetDeviceInformationResponse> <tds:Manufacturer>IPCAM</tds:Manufacturer> <tds:Model>C6F0SiZ3N0P0L0</tds:Model> <tds:FirmwareVersion>V6.1.10.2.1-20150624</tds:FirmwareVersion> <tds:SerialNumber>00E0F8218509</tds:SerialNumber> <tds:HardwareId>V6.1.10.2.1-20150624</tds:HardwareId> </tds:GetDeviceInformationResponse> </SOAP-ENV:Body> </SOAP-ENV:Envelope> Note: Not all requests are support by all cameras, but the main features are implemented (, I hope). Maybe your camera has different Profile names, but the xml tags must be the same. 5.9. Motion Configuration changes \u00b6 Change directory to motion conf cd /opt/motion-3.4.1-316/etc/motion Rename motion-dist.conf to motion.conf mv motion-dist.conf motion.conf Turn on daemon mode daemon on Configure Logs Run: mkdir -p /opt/motion-3.4.1-316/var/log/motion Chage conf to: logfile /opt/motion-3.4.1-316/var/log/motion/motion.log Increase log level log_level 7 Comment out videodevice line, since we will use netcam. #videodevice /dev/video0 Change resolution # Image width (pixels). Valid range: Camera dependent, default: 352 width 1920 # Image height (pixels). Valid range: Camera dependent, default: 288 height 1080 Change frame rate framerate 7 Specify netcam_url netcam_url rtsp://172.19.0.4:554/11 Switch keepalive on netcam_keepalive on Motion Detection Settings Change treshold: threshold 2500 Increase minimum motion frame: minimum_motion_frames 3 Modify event_gap: event_gap 10 Set max movie time to 10 mins: max_movie_time 600 This is optional. I don't save any movies, but only images. It's your choice. Turn video save off: ffmpeg_output_movies off File save settings: Specify target dir to save images (and / or videos) mkdir -p /opt/motion-3.4.1-316/var/spool/motion target_dir /opt/motion-3.4.1-316/var/spool/motion Set up the pictures name conversation: picture_filename imgs/%Y-%m-%d/%H-%M-%S__%q-%D picture_filename imgs/%Y-%m-%d/%H/%H-%M-%S__%q-%D This will save images to [target_dir]/imgs/YEAR-MONTH-DAY/HOUR-MIN-SEC__FRAMENUMBER_MOTION.jpg format. (optional) Enable access to web control and Live View from anywhere stream_localhost off webcontrol_localhost off I do not save movies, because movies are taken real time, and uses CPU moves are captured real-time, I mean 10 minutes movies takes 10 minutes to watch about 10 minutes long motion detection, so instead of this I create timelapse movies based on saved images. This way I can check the movies faster than the event happened and if I find something, I can look for the saved images to investigate it deeper. Motion has a log of configuration it worth it to look through the motion.conf file I'm sure you will find exciting and useful options. My setup is just enough to detect motion and save images. I post process the images using shell scripts and ffmpeg (or libav) tools. 6. Create movie from still images (*.jpg) \u00b6 Now we have a lot of images from the camera. My motion save one images in every 30 seconds: # Make automated snapshot every N seconds (default: 0 = disabled) snapshot_interval 30 And save images when motion is detected: # Threshold for number of changed pixels in an image that # triggers motion detection (default: 1500) threshold 2500 My motion saves images in the following directory structure: /storage/motion/output/cam1/imgs/[DATE]/[HOUR]/*.jpg Setup: picture_filename imgs/%Y-%m-%d/%H/%H-%M-%S__%q-%D target_dir /storage/motion/output/cam1 ... target_dir /storage/motion/output/cam2 Each camera uses its own directory. I'm using this shell script to create movie from the images: #!/bin/bash ROOT_DIR=\"/storage/motion/output\" function LOG() { echo \"[ $( date +%F\\ %T ) ] - $1 \" } ### ## VARIABLES ### TMP_DIR=\"/storage/motion/tmp\" CAM_IMG_DIRS=$( mktemp $TMP_DIR/dovid_XXXXXXXXXXX.txt ) NOW_DATE=$( date +%Y-%m-%d-%H ) LOG \"###\" LOG \"## Starting SCRITP\" LOG \"###\" LOG \"Prepare temorary file: $CAM_IMG_DIRS\" LOG \"Running first FOR loop: Searching for camera dirs (cam[X]). \" for DIR in $( find $ROOT_DIR -mindepth 1 -maxdepth 1 -type d | egrep 'cam[0-9]{1,1}' | sort -n ) do CAM_NUM=$( echo $DIR | egrep -o 'cam[0-9]{1,1}' ) # Extract camera number, example: cam1, cam2 ... LOG \"Runninng second FOR loop: Searching for dates in camera[X] dir\" for DATE_IN_DIR in $( find $DIR/imgs -mindepth 1 -maxdepth 1 -type d | egrep '[0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2}' | sort -n ) do LOG \"Running third FOR loop: Searching HOURS in camX/DATE/* ($DATE_IN_DIR) \" for HOUR_IN_DATE_IN_DIR in $( find $DATE_IN_DIR -mindepth 1 -maxdepth 1 -type d | egrep '[0-9]{2,2}' | sort -n ) do DATE_FROM_DIR=$( basename $DATE_IN_DIR ) # Extract Date From Directory Name HOUR_FROM_DIR=$( basename $HOUR_IN_DATE_IN_DIR ) # Extract HOUR number VID_DIR=\"$ROOT_DIR/$CAM_NUM/vids/$DATE_FROM_DIR\" # Preapre VIDEO DIR VID_FILE=\"$VID_DIR/$DATE_FROM_DIR-$HOUR_FROM_DIR.avi\" # SET Video FILE [ ! -d $VID_DIR ] && mkdir -p $VID_DIR if [ ! -f $VID_FILE ] then LOG \"Adding this line to $CAM_IMG_DIRS\" echo \"$HOUR_IN_DATE_IN_DIR|$VID_FILE\" | tee -a $CAM_IMG_DIRS fi done done done LOG \"Preparing temporary file is done.\" LOG \"################ PROCESSING temporary file ################\" while IFS='|' read -r IMG_DIR VID_FILE do LOG \"+++++ Processing: +++++\" LOG \"-- Image Dir: $IMG_DIR\" LOG \"-- Video File: $VID_FILE\" VID_FILE_BN=$( basename $VID_FILE ) # Video File Base Name VID_FILE_BN_WO_EXT=\"${VID_FILE_BN%.*}\" # Video File WO Base Name LOG \"-- Checking the the hour...\" if [ \"$VID_FILE_BN_WO_EXT\" != \"$NOW_DATE\" ] then LOG \"!!!!!!!!!!!!!!!!!! Encoding: $VID_FILE !!!!!!!!!!!!!!!!!!\" START_ENC_DATE=$( date +%s ) for FILE in $( find $IMG_DIR -type f -name '*.jpg' | sort ) do cat $FILE done | /opt/ffmpeg/bin/ffmpeg -loglevel warning -r 25 -f image2pipe -i - -c:v mpeg4 -vtag xvid -qscale:v 14 $VID_FILE RETVAL=$? STOP_ENC_DATE=$( date +%s ) LOG \"Encoding DONE in $(( $STOP_ENC_DATE - $START_ENC_DATE )) seconds, and finsihed with $RETVAL error code.\" else LOG \"$NOW_DATE is the current date (hour), so we can not process this folder. Skipping... ($VID_FILE) \" fi done <$CAM_IMG_DIRS echo echo This script creates separate movie files per hours. Example: motion@camsrv01:/opt/motion-3.4.1-316/etc/motion$ ls -al /storage/motion/output/cam1/vids/2017-05-13 total 680340 drwxr-xr-x 2 motion motion 4096 May 14 00:10 . drwxr-xr-x 9 motion motion 4096 May 14 00:10 .. -rw-r--r-- 1 motion motion 11216416 May 13 01:10 2017-05-13-00.avi -rw-r--r-- 1 motion motion 4704858 May 13 02:10 2017-05-13-01.avi -rw-r--r-- 1 motion motion 4349738 May 13 03:10 2017-05-13-02.avi -rw-r--r-- 1 motion motion 6069618 May 13 04:10 2017-05-13-03.avi -rw-r--r-- 1 motion motion 10084490 May 13 05:10 2017-05-13-04.avi -rw-r--r-- 1 motion motion 16269868 May 13 06:10 2017-05-13-05.avi -rw-r--r-- 1 motion motion 29998296 May 13 07:11 2017-05-13-06.avi -rw-r--r-- 1 motion motion 42320628 May 13 08:11 2017-05-13-07.avi -rw-r--r-- 1 motion motion 57703934 May 13 09:12 2017-05-13-08.avi -rw-r--r-- 1 motion motion 46332720 May 13 10:11 2017-05-13-09.avi -rw-r--r-- 1 motion motion 48425176 May 13 11:11 2017-05-13-10.avi -rw-r--r-- 1 motion motion 47206092 May 13 12:11 2017-05-13-11.avi -rw-r--r-- 1 motion motion 43665386 May 13 13:11 2017-05-13-12.avi -rw-r--r-- 1 motion motion 36392852 May 13 14:11 2017-05-13-13.avi -rw-r--r-- 1 motion motion 64864312 May 13 15:11 2017-05-13-14.avi -rw-r--r-- 1 motion motion 44121274 May 13 16:11 2017-05-13-15.avi -rw-r--r-- 1 motion motion 43531740 May 13 17:11 2017-05-13-16.avi -rw-r--r-- 1 motion motion 30732256 May 13 18:11 2017-05-13-17.avi -rw-r--r-- 1 motion motion 25023926 May 13 19:10 2017-05-13-18.avi -rw-r--r-- 1 motion motion 22104216 May 13 20:10 2017-05-13-19.avi -rw-r--r-- 1 motion motion 25939654 May 13 21:11 2017-05-13-20.avi -rw-r--r-- 1 motion motion 15746894 May 13 22:11 2017-05-13-21.avi -rw-r--r-- 1 motion motion 11119838 May 13 23:10 2017-05-13-22.avi -rw-r--r-- 1 motion motion 8694202 May 14 00:10 2017-05-13-23.avi Crontab: 10 * * * * /storage/motion/scripts/do_vids_v2.sh >>/storage/motion/logs/cam1_vid_create.log 2>&1","title":"Install (compile) & Configure motion + AV tools"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#install-compile-configure-motion-av-tools","text":"","title":"Install (compile) &amp; Configure motion + AV tools"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#0-update-running-system-install-necessary-pacgakes","text":"As always, start with updating you linux system. apt-get update apt-get upgrade Install packages apt-get install libjpeg-dev libjpeg62-turbo-dev autoconf automake build-essential libzip-dev git yasm nasm pkg-config libavutil-dev libavformat-dev libavcodec-dev libswscale-dev autoconf automake build-essential pkgconf libtool libzip-dev libjpeg62 git libavformat-dev libavcodec-dev libavutil-dev libswscale-dev libavdevice-dev ca-certificates webp libwebp-dev curl lynx zip apache2 libx264-dev x264 libav-tools mpv","title":"0. Update running system &amp; Install necessary pacgakes"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#1-install-libav12","text":"Note This step is optional , because at the 0. step we installed libav-tools package which contains avconv. But with compiling from source we get a newer version: Repository: avconv version 11.8-6:11.8-1~deb8u1+rpi1 Compiled: avconv version 12, Copyright (c) 2000-2016 the Libav developers","title":"1. Install libav12"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#11-download-source-code-from-the-git-repository","text":"cd /usr/src git clone https://github.com/todostreaming/libav12","title":"1.1. Download source code from the git repository"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#12-configure-make-and-make-install","text":"","title":"1.2. Configure &amp; make and make install"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#121-configure","text":"cd /usr/src/libav12 ./configure --enable-libwebp --enable-libx264 --logfile=/root/libav-conf-$(date +%s).log --enable-gpl --prefix=/opt/libav12 | tee -a /root/libav-conf-$(date +%s).out After the configure is done you can check two log files: /root/libav-conf-*.log /root/libav-conf-*.out The configure script must be done without any error. If you see errors in one of the log files please do not continue, and try to fix them. Usually the most error cause by a missing library and easy can be fixed with install the missing lib using apt-get install command.","title":"1.2.1. Configure"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#122-compile-the-source-code","text":"To speed up this process lets use -j2 option. (Or if you want -j3 or -j4) make -j2 During this process maybe you will some warning messages, but no errors. So you can skip them. -j [jobs], --jobs[=jobs] Specifies the number of jobs (commands) to run simultaneously. If there is more than one -j option, the last one is effective. If the -j option is given without an argument, make will not limit the number of jobs that can run simultaneously.","title":"1.2.2. Compile the source code:"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#123-install-libav","text":"To do this simply run this command: make install The binaries will be available in the directory specified with \"--prefix\" option when we run configure script. In our case: /opt/libav12 References: https://wiki.libav.org https://libav.org https://github.com/todostreaming/libav12","title":"1.2.3. Install libav"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#2-install-compile-ffmpeg","text":"FFMpeg is no longer available in Debian repository, so if you want to use it you have to compile on your own. cd /usr/src git clone https://github.com/FFmpeg/FFmpeg root@rpi2camsrv01:/usr/src/FFmpeg# ./configure --prefix=/opt/ffmpeg --enable-libx264 --enable-libwebp --enable-gpl --enable-nonfree make make install Note: The make command runs very long time. Please be patient, and / or try run with -j[234] option.","title":"2. Install / Compile FFMpeg"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#3-install-compile-motion","text":"Motion can be easily installed with apt-get install motion , so you my think this post is absolutely useless and has no sense. But if you want to use RTSP stream, you have to have the latest (or at lease 4.x) version of motion. The version of apt-get repository is too old and has not this feature: root@rpi2camsrv01:/opt/motion-3.4.1-316/bin# apt-cache show motion Package: motion Source: motion (3.2.12+git20140228-4) Version: 3.2.12+git20140228-4+b2 By using the git source code we can install the latest version: motion Version 4.0.1+git8a1b9a97 If you want to use motion with mysql support in the future you need some additional packages: apt-get install libmysql++-dev libmysqlclient-dev","title":"3. Install / Compile Motion"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#31-clone-from-git","text":"/usr/src git clone https://github.com/Motion-Project/motion.git","title":"3.1. Clone from git"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#32-make-make-install","text":"Now we have only to easy steps left, run make and make install. make make install Your motion instance is now ready to use, and can be located in /opt/motion-3.4.1-316 .","title":"3.2. Make &amp; Make install"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#update-20170509","text":"If you want to use fmpeg to encode/decode .webm files you need to install these extra packages: apt-get install libvorbis-dev libvpx-dev mjpegtools (optional) apt-get instell vpx-tools imagemagick And configure ffmpeg with these parameters : ./configure --prefix=/opt/ffmpeg-webm --enable-libx264 --enable-libwebp --enable-gpl --enable-nonfree --enable-libvpx --enable-libvorbis","title":"Update - 2017.05.09"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#4-configure-motion-to-start-at-boot","text":"","title":"4. Configure Motion to start at boot"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#41-create-user-for-motion-service","text":"useradd motion root@rpi2camsrv01:/lib/systemd/system# id motion uid=1002(motion) gid=1002(motion) groups=1002(motion)","title":"4.1. Create user for motion service"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#42-edit-motionservice-systemd-config-file","text":"An example service file can be found in the motion install directory: /opt/motion-3.4.1-316/share/motion/examples/motion.service Copy this file to systemd directory: cd /lib/systemd/system cp /opt/motion-3.4.1-316/share/motion/examples/motion.service . Create dir for PID file mkdir /opt/motion-3.4.1-316/var Edit motion.sevice file [Unit] Description=Motion daemon After=local-fs.target network.target [Service] User=motion Group=motion PIDFile=/opt/motion-3.4.1-316/var/motion.pid ExecStart=/opt/motion-3.4.1-316/bin/motion -n Type=simple StandardError=null [Install] WantedBy=multi-user.target Change Owner of motion chown -R motion:motion /opt/motion-3.4.1-316/ Try to start motion.service root@rpi2camsrv01:/lib/systemd/system# systemctl start motion.service Check the service root@rpi2camsrv01:/lib/systemd/system# systemctl status motion.service \u00e2\u2014\u008f motion.service - Motion daemon Loaded: loaded (/lib/systemd/system/motion.service; disabled) Active: active (running) since Fri 2017-03-24 09:36:46 UTC; 5s ago Main PID: 12812 (motion) CGroup: /system.slice/motion.service \u00e2\u201d\u201d\u00e2\u201d\u20ac12812 /opt/motion-3.4.1-316/bin/motion -n Stop service and install service file root@rpi2camsrv01:/lib/systemd/system# systemctl stop motion.service root@rpi2camsrv01:/lib/systemd/system# systemctl enable motion.service Created symlink from /etc/systemd/system/multi-user.target.wants/motion.service to /lib/systemd/system/motion.service. Now you should use 'motion' user to configure motion, otherwise the owner of files may be changed causing permission denied. root@rpi2camsrv01:/lib/systemd/system# sudo su - motion No directory, logging in with HOME=/ motion@rpi2camsrv01:/opt/motion-3.4.1-316$ You can create home directory to motion user: mkdir /home/motion chown -R motion:motion /home/motion/ You motion installation is ready for use now.","title":"4.2. Edit motion.service systemd config file"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#5-configure-motion-to-use-an-onvif-ip-camera-via-rtsp-stream","text":"You can buy low budget IP cameras from eBay, Aliexpress and so on. Nowadays almost all cheap cameras are using ONVIF interface. I have 3 IP cameras and I have to say that the web interfaces of them are really poor. My main problem is that two of them have web interface which can be accessed only with IE, because of ActiveX. :( I hate it because I usually use Linux. Another option to configure you camera is to use some ONVIF manager software. Usually the provider of the camera send a software, as well, but its also need Windows. :( I tried to find some ONVIF manager for Linux, but could not find a really good one. I don't care too much because normally a camera have to be set up once, and then can be used. To use these cameras with MOTION you have to determine the stream URL. If you are lucky you can find it somewhere in the web interface. The following steps are valid only if you have already configured you camera. This means that the cam is connected to your network at least, and you know it IP address. This post doesn't aim to describe 'how to configure your cam', so you have to do it on your own. It is more exciting to find you ONVIF camera's stream URLs. If you don't like my method you can do some googleing to find the URLs of your camera.","title":"5. Configure Motion to use an ONVIF IP camera via RTSP stream"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#51-what-you-need","text":"Installed SOAPUI . Your camera WebService URL. Unfortunately this can be different from mine. Here is two examples: http://172.19.0.3:8080 http://172.19.0.2/onvif/device_service At this step the main problem is that I don't know exact method to find your camera web service URL. If the two example above aren't working you have to do some searching on Google. According to the Official Documentation the URL should be this: The entry point for the device management service is fixed to: http://onvif_host/onvif/device_service","title":"5.1. What you need?"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#52-load-onvif-device-wsdl-to-soapui","text":"URL: https://www.onvif.org/ver10/device/wsdl/devicemgmt.wsdl","title":"5.2. Load ONVIF device WSDL to soapUI"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#53-modify-endpoint","text":"You can specify username password if your camera is using authentication Assign the newly added endpoint to the requests, without this you can specify the endpoint for all requests. Now you can close this window.","title":"5.3. Modify Endpoint"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#54-getservices","text":"Find 'GetServices' in the left panel. Modify the request to: <wsdl:IncludeCapability>true</wsdl:IncludeCapability> You will get all web services which supported by your camera. I paste only the relavant information from the request here. <tds:Namespace>http://www.onvif.org/ver10/device/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/devices</tds:XAddr> <tds:Namespace>http://www.onvif.org/ver10/media/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/media</tds:XAddr> <tds:Namespace>http://www.onvif.org/ver10/events/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/events</tds:XAddr> <tds:Namespace>http://www.onvif.org/ver20/analytics/wsdl</tds:Namespace> <tds:XAddr>http://172.19.0.3:8080/onvif/analytics</tds:XAddr> We need the media one: http://www.onvif.org/ver10/media/wsdl Copy paste this URL to your browser to get the WSDL location. You will be redirected to this URL: https://www.onvif.org/ver10/media/wsdl/media.wsdl7 Add this WSDL to your soapUI project. For reference please see chapter 5.2.","title":"5.4. GetServices"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#55-mediabinding-getprofiles","text":"With this you get the profiles of you camera. I don't copy paste the whole response here just some important parts. I have two profiles: <trt:Profiles fixed=\"true\" token=\"MainProfileToken\"> <trt:Profiles fixed=\"true\" token=\"SubProfileToken\"> You can find everything about the profiles for example resolution: <tt:VideoEncoderConfiguration token=\"main_video_encoder_cfg_token\"> <tt:Name>main_video_encoder_cfg</tt:Name> <tt:UseCount>1</tt:UseCount> <tt:Encoding>H264</tt:Encoding> <tt:Resolution> <tt:Width>1920</tt:Width> <tt:Height>1080</tt:Height> </tt:Resolution> I want to use this profile in Motion (MainProfileToken). These profiles can be found on the web interface, but maybe the name is different.","title":"5.5. MediaBinding / GetProfiles"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#56-get-urls-mediabinding-getstreamuri","text":"Now you have to edit this XML before start the request. Open this URL in you browser: https://www.onvif.org/ver10/media/wsdl/media.wsdl And find 'GetStreamUri' operation. You will see how to configure this request. Example: <soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:wsdl=\"http://www.onvif.org/ver10/media/wsdl\" xmlns:sch=\"http://www.onvif.org/ver10/schema\"> <soap:Header/> <soap:Body> <wsdl:GetStreamUri> <wsdl:StreamSetup> <sch:Stream>RTP-Unicast</sch:Stream> <sch:Transport> <sch:Protocol>RTSP</sch:Protocol> <!--Optional:--> <sch:Tunnel/> </sch:Transport> <!--You may enter ANY elements at this point--> </wsdl:StreamSetup> <wsdl:ProfileToken>MainProfileToken</wsdl:ProfileToken> </wsdl:GetStreamUri> </soap:Body> </soap:Envelope> The most important things to note the ProfileToken: <wsdl:ProfileToken>MainProfileToken</wsdl:ProfileToken> Finally we have the stream URL: <tt:Uri>rtsp://172.19.0.3:554/11</tt:Uri> You can try this url in VLC media player. Just for demonstration my <wsdl:ProfileToken>SubProfileToken</wsdl:ProfileToken> address is: <tt:Uri>rtsp://172.19.0.3:554/12</tt:Uri>","title":"5.6. Get URLS (MediaBinding / GetStreamUri)"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#57-bonus-mediabinding-getsnapshoturi","text":"You can get snapshot images from ONVIF cameras. To get the URL use GetSnapshotUri request: <tt:Uri>http://172.19.0.3:80/web/auto.jpg?-usr=admin&amp;-pwd=admin&amp;</tt:Uri>","title":"5.7. (Bonus) MediaBinding / GetSnapshotUri"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#58-bonus-devicebinding-getdeviceinformation","text":"Just for fun my last sample request is the 'GetDeviceInformation'. Request: <soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:wsdl=\"http://www.onvif.org/ver10/device/wsdl\"> <soap:Header/> <soap:Body> <wsdl:GetDeviceInformation/> </soap:Body> </soap:Envelope> Response: ... <SOAP-ENV:Header/> <SOAP-ENV:Body> <tds:GetDeviceInformationResponse> <tds:Manufacturer>IPCAM</tds:Manufacturer> <tds:Model>C6F0SiZ3N0P0L0</tds:Model> <tds:FirmwareVersion>V6.1.10.2.1-20150624</tds:FirmwareVersion> <tds:SerialNumber>00E0F8218509</tds:SerialNumber> <tds:HardwareId>V6.1.10.2.1-20150624</tds:HardwareId> </tds:GetDeviceInformationResponse> </SOAP-ENV:Body> </SOAP-ENV:Envelope> Note: Not all requests are support by all cameras, but the main features are implemented (, I hope). Maybe your camera has different Profile names, but the xml tags must be the same.","title":"5.8. (Bonus) DeviceBinding / GetDeviceInformation"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#59-motion-configuration-changes","text":"Change directory to motion conf cd /opt/motion-3.4.1-316/etc/motion Rename motion-dist.conf to motion.conf mv motion-dist.conf motion.conf Turn on daemon mode daemon on Configure Logs Run: mkdir -p /opt/motion-3.4.1-316/var/log/motion Chage conf to: logfile /opt/motion-3.4.1-316/var/log/motion/motion.log Increase log level log_level 7 Comment out videodevice line, since we will use netcam. #videodevice /dev/video0 Change resolution # Image width (pixels). Valid range: Camera dependent, default: 352 width 1920 # Image height (pixels). Valid range: Camera dependent, default: 288 height 1080 Change frame rate framerate 7 Specify netcam_url netcam_url rtsp://172.19.0.4:554/11 Switch keepalive on netcam_keepalive on Motion Detection Settings Change treshold: threshold 2500 Increase minimum motion frame: minimum_motion_frames 3 Modify event_gap: event_gap 10 Set max movie time to 10 mins: max_movie_time 600 This is optional. I don't save any movies, but only images. It's your choice. Turn video save off: ffmpeg_output_movies off File save settings: Specify target dir to save images (and / or videos) mkdir -p /opt/motion-3.4.1-316/var/spool/motion target_dir /opt/motion-3.4.1-316/var/spool/motion Set up the pictures name conversation: picture_filename imgs/%Y-%m-%d/%H-%M-%S__%q-%D picture_filename imgs/%Y-%m-%d/%H/%H-%M-%S__%q-%D This will save images to [target_dir]/imgs/YEAR-MONTH-DAY/HOUR-MIN-SEC__FRAMENUMBER_MOTION.jpg format. (optional) Enable access to web control and Live View from anywhere stream_localhost off webcontrol_localhost off I do not save movies, because movies are taken real time, and uses CPU moves are captured real-time, I mean 10 minutes movies takes 10 minutes to watch about 10 minutes long motion detection, so instead of this I create timelapse movies based on saved images. This way I can check the movies faster than the event happened and if I find something, I can look for the saved images to investigate it deeper. Motion has a log of configuration it worth it to look through the motion.conf file I'm sure you will find exciting and useful options. My setup is just enough to detect motion and save images. I post process the images using shell scripts and ffmpeg (or libav) tools.","title":"5.9. Motion Configuration changes"},{"location":"old/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#6-create-movie-from-still-images-jpg","text":"Now we have a lot of images from the camera. My motion save one images in every 30 seconds: # Make automated snapshot every N seconds (default: 0 = disabled) snapshot_interval 30 And save images when motion is detected: # Threshold for number of changed pixels in an image that # triggers motion detection (default: 1500) threshold 2500 My motion saves images in the following directory structure: /storage/motion/output/cam1/imgs/[DATE]/[HOUR]/*.jpg Setup: picture_filename imgs/%Y-%m-%d/%H/%H-%M-%S__%q-%D target_dir /storage/motion/output/cam1 ... target_dir /storage/motion/output/cam2 Each camera uses its own directory. I'm using this shell script to create movie from the images: #!/bin/bash ROOT_DIR=\"/storage/motion/output\" function LOG() { echo \"[ $( date +%F\\ %T ) ] - $1 \" } ### ## VARIABLES ### TMP_DIR=\"/storage/motion/tmp\" CAM_IMG_DIRS=$( mktemp $TMP_DIR/dovid_XXXXXXXXXXX.txt ) NOW_DATE=$( date +%Y-%m-%d-%H ) LOG \"###\" LOG \"## Starting SCRITP\" LOG \"###\" LOG \"Prepare temorary file: $CAM_IMG_DIRS\" LOG \"Running first FOR loop: Searching for camera dirs (cam[X]). \" for DIR in $( find $ROOT_DIR -mindepth 1 -maxdepth 1 -type d | egrep 'cam[0-9]{1,1}' | sort -n ) do CAM_NUM=$( echo $DIR | egrep -o 'cam[0-9]{1,1}' ) # Extract camera number, example: cam1, cam2 ... LOG \"Runninng second FOR loop: Searching for dates in camera[X] dir\" for DATE_IN_DIR in $( find $DIR/imgs -mindepth 1 -maxdepth 1 -type d | egrep '[0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2}' | sort -n ) do LOG \"Running third FOR loop: Searching HOURS in camX/DATE/* ($DATE_IN_DIR) \" for HOUR_IN_DATE_IN_DIR in $( find $DATE_IN_DIR -mindepth 1 -maxdepth 1 -type d | egrep '[0-9]{2,2}' | sort -n ) do DATE_FROM_DIR=$( basename $DATE_IN_DIR ) # Extract Date From Directory Name HOUR_FROM_DIR=$( basename $HOUR_IN_DATE_IN_DIR ) # Extract HOUR number VID_DIR=\"$ROOT_DIR/$CAM_NUM/vids/$DATE_FROM_DIR\" # Preapre VIDEO DIR VID_FILE=\"$VID_DIR/$DATE_FROM_DIR-$HOUR_FROM_DIR.avi\" # SET Video FILE [ ! -d $VID_DIR ] && mkdir -p $VID_DIR if [ ! -f $VID_FILE ] then LOG \"Adding this line to $CAM_IMG_DIRS\" echo \"$HOUR_IN_DATE_IN_DIR|$VID_FILE\" | tee -a $CAM_IMG_DIRS fi done done done LOG \"Preparing temporary file is done.\" LOG \"################ PROCESSING temporary file ################\" while IFS='|' read -r IMG_DIR VID_FILE do LOG \"+++++ Processing: +++++\" LOG \"-- Image Dir: $IMG_DIR\" LOG \"-- Video File: $VID_FILE\" VID_FILE_BN=$( basename $VID_FILE ) # Video File Base Name VID_FILE_BN_WO_EXT=\"${VID_FILE_BN%.*}\" # Video File WO Base Name LOG \"-- Checking the the hour...\" if [ \"$VID_FILE_BN_WO_EXT\" != \"$NOW_DATE\" ] then LOG \"!!!!!!!!!!!!!!!!!! Encoding: $VID_FILE !!!!!!!!!!!!!!!!!!\" START_ENC_DATE=$( date +%s ) for FILE in $( find $IMG_DIR -type f -name '*.jpg' | sort ) do cat $FILE done | /opt/ffmpeg/bin/ffmpeg -loglevel warning -r 25 -f image2pipe -i - -c:v mpeg4 -vtag xvid -qscale:v 14 $VID_FILE RETVAL=$? STOP_ENC_DATE=$( date +%s ) LOG \"Encoding DONE in $(( $STOP_ENC_DATE - $START_ENC_DATE )) seconds, and finsihed with $RETVAL error code.\" else LOG \"$NOW_DATE is the current date (hour), so we can not process this folder. Skipping... ($VID_FILE) \" fi done <$CAM_IMG_DIRS echo echo This script creates separate movie files per hours. Example: motion@camsrv01:/opt/motion-3.4.1-316/etc/motion$ ls -al /storage/motion/output/cam1/vids/2017-05-13 total 680340 drwxr-xr-x 2 motion motion 4096 May 14 00:10 . drwxr-xr-x 9 motion motion 4096 May 14 00:10 .. -rw-r--r-- 1 motion motion 11216416 May 13 01:10 2017-05-13-00.avi -rw-r--r-- 1 motion motion 4704858 May 13 02:10 2017-05-13-01.avi -rw-r--r-- 1 motion motion 4349738 May 13 03:10 2017-05-13-02.avi -rw-r--r-- 1 motion motion 6069618 May 13 04:10 2017-05-13-03.avi -rw-r--r-- 1 motion motion 10084490 May 13 05:10 2017-05-13-04.avi -rw-r--r-- 1 motion motion 16269868 May 13 06:10 2017-05-13-05.avi -rw-r--r-- 1 motion motion 29998296 May 13 07:11 2017-05-13-06.avi -rw-r--r-- 1 motion motion 42320628 May 13 08:11 2017-05-13-07.avi -rw-r--r-- 1 motion motion 57703934 May 13 09:12 2017-05-13-08.avi -rw-r--r-- 1 motion motion 46332720 May 13 10:11 2017-05-13-09.avi -rw-r--r-- 1 motion motion 48425176 May 13 11:11 2017-05-13-10.avi -rw-r--r-- 1 motion motion 47206092 May 13 12:11 2017-05-13-11.avi -rw-r--r-- 1 motion motion 43665386 May 13 13:11 2017-05-13-12.avi -rw-r--r-- 1 motion motion 36392852 May 13 14:11 2017-05-13-13.avi -rw-r--r-- 1 motion motion 64864312 May 13 15:11 2017-05-13-14.avi -rw-r--r-- 1 motion motion 44121274 May 13 16:11 2017-05-13-15.avi -rw-r--r-- 1 motion motion 43531740 May 13 17:11 2017-05-13-16.avi -rw-r--r-- 1 motion motion 30732256 May 13 18:11 2017-05-13-17.avi -rw-r--r-- 1 motion motion 25023926 May 13 19:10 2017-05-13-18.avi -rw-r--r-- 1 motion motion 22104216 May 13 20:10 2017-05-13-19.avi -rw-r--r-- 1 motion motion 25939654 May 13 21:11 2017-05-13-20.avi -rw-r--r-- 1 motion motion 15746894 May 13 22:11 2017-05-13-21.avi -rw-r--r-- 1 motion motion 11119838 May 13 23:10 2017-05-13-22.avi -rw-r--r-- 1 motion motion 8694202 May 14 00:10 2017-05-13-23.avi Crontab: 10 * * * * /storage/motion/scripts/do_vids_v2.sh >>/storage/motion/logs/cam1_vid_create.log 2>&1","title":"6. Create movie from still images (*.jpg)"},{"location":"old/linux/Install_Mps-youtube_Console_Based_Youtube_Player/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Install mps-youtube console based youtube player \u00b6 Why do I need console based youtube player? The answer is very simple. When I'm working in my Workshop I want to listen music from youtube. OK. I know that there are uncountable way to listen music online. I have some old desktop PC which have very limited resources, and can't able to play youtube clip in browser. This old PC is connected to an as old HIFI system as the PC itself. So I needed a lightweight youtube player which can be run on an old PC. After some gooleing I found mps-youtube. It is very simple to install and use it. I love it. :) This is exactly what I wanted. Here is the steps to install mps-youtube: Install python pip3: apt-get install python3-pip Install dependencies: apt-get install mplayer2 mpv Install mps-youtube: pip3 install mps-youtube Install youtube-dl: pip3 install youtube_dl Set player to mpv Run mpsyt Type: set player /usr/bin/mpv Reference: https://github.com/mps-youtube/mps-youtube Bonus: Some youdube-dl example. Download youtube video in MP3 format: youtube-dl --restrict-filenames --no-mtime --extract-audio --audio-format mp3 --audio-quality 0 -o /path/to/dir/%\\(title\\)s.%\\(ext\\)s [LINK] Where: * --restrict-filenames Restrict filenames to only ASCII characters, and avoid \"&\" and spaces in filenames * --no-mtime Do not use the Last-modified header to set the file modification time * -x, --extract-audio Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe) * --audio-format mp3 Save output as MP3. * audio-quality Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default 5) * -o /path/to/dir/%\\(title\\)s.%\\(ext\\)s Output directory and file pattern. Download Video File Simply run youtube-dl [LINK] , this will save the video in mp4 format. Process URLs from file: youtube-dl --batch-file list.txt","title":"Install mps-youtube console based youtube player"},{"location":"old/linux/Install_Mps-youtube_Console_Based_Youtube_Player/#install-mps-youtube-console-based-youtube-player","text":"Why do I need console based youtube player? The answer is very simple. When I'm working in my Workshop I want to listen music from youtube. OK. I know that there are uncountable way to listen music online. I have some old desktop PC which have very limited resources, and can't able to play youtube clip in browser. This old PC is connected to an as old HIFI system as the PC itself. So I needed a lightweight youtube player which can be run on an old PC. After some gooleing I found mps-youtube. It is very simple to install and use it. I love it. :) This is exactly what I wanted. Here is the steps to install mps-youtube: Install python pip3: apt-get install python3-pip Install dependencies: apt-get install mplayer2 mpv Install mps-youtube: pip3 install mps-youtube Install youtube-dl: pip3 install youtube_dl Set player to mpv Run mpsyt Type: set player /usr/bin/mpv Reference: https://github.com/mps-youtube/mps-youtube Bonus: Some youdube-dl example. Download youtube video in MP3 format: youtube-dl --restrict-filenames --no-mtime --extract-audio --audio-format mp3 --audio-quality 0 -o /path/to/dir/%\\(title\\)s.%\\(ext\\)s [LINK] Where: * --restrict-filenames Restrict filenames to only ASCII characters, and avoid \"&\" and spaces in filenames * --no-mtime Do not use the Last-modified header to set the file modification time * -x, --extract-audio Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe) * --audio-format mp3 Save output as MP3. * audio-quality Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default 5) * -o /path/to/dir/%\\(title\\)s.%\\(ext\\)s Output directory and file pattern. Download Video File Simply run youtube-dl [LINK] , this will save the video in mp4 format. Process URLs from file: youtube-dl --batch-file list.txt","title":"Install mps-youtube console based youtube player"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Create Your Own DynDns Service with Bind (Named) \u00b6 1. First you need to generate the private and public key \u00b6 You can do that with one simple command: dnssec-keygen -a HMAC-MD5 -b 256 -n HOST dyn-key dnssec-keygen -a HMAC-MD5 -b 256 -n HOST dyn-key I chose HMAC-MD5 hash algorithm, and I recommend to generate at least 256 bit keys. The -n option: -n <nametype>: ZONE | HOST | ENTITY | USER | OTHER We will have these two files: Kdyn-key.+157+60890.key Kdyn-key.+157+60890.private 2 Modify named.conf \u00b6 Add this line to named.conf : include \"/etc/bind/dns.keys\"; 3. Create dns.keys configuration file \u00b6 It must look like something similar to this example: cat dns.keys key dyn-key. { algorithm HMAC-MD5; secret \"fop39Dcbz9HZ9sQqzo64fHorSIJXnmGjJ980BwTg6O4=\"; }; We have to stop here for some words. Where is the \"secret\" come from? You can find this private key in Kdyn-key.+157+60890.private . In my case: cat Kdyn-key.+157+60890.private Private-key-format: v1.3 Algorithm: 157 (HMAC_MD5) Key: fop39Dcbz9HZ9sQqzo64fHorSIJXnmGjJ980BwTg6O4= Bits: AAA= Created: 20161015122904 Publish: 20161015122904 Activate: 20161015122904 4. Allow Update Zone with these keys \u00b6 Example: zone \"dyn.vinczejanos.info\" { type master; file \"/etc/bind/db.dyn.vinczejanos.info\"; allow-query { any; }; allow-update { key \"dyn-key.\"; }; }; After the configuration is done, do not forget to restart bind. /etc/init.d/bind9 restart 5. Check Update \u00b6 cat update.sh cat << EOF | nsupdate -k \"Kdyn-key.+157+60890.key\" server ns20.vinczejanos.info zone dyn.vinczejanos.info. update delete test-dyn.dyn.vinczejanos.info update add test-dyn.dyn.vinczejanos.info 60 A 192.168.0.1 show send EOF","title":"Create Your Own DynDns Service with Bind (Named)"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#create-your-own-dyndns-service-with-bind-named","text":"","title":"Create Your Own DynDns Service with Bind (Named)"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#1-first-you-need-to-generate-the-private-and-public-key","text":"You can do that with one simple command: dnssec-keygen -a HMAC-MD5 -b 256 -n HOST dyn-key dnssec-keygen -a HMAC-MD5 -b 256 -n HOST dyn-key I chose HMAC-MD5 hash algorithm, and I recommend to generate at least 256 bit keys. The -n option: -n <nametype>: ZONE | HOST | ENTITY | USER | OTHER We will have these two files: Kdyn-key.+157+60890.key Kdyn-key.+157+60890.private","title":"1. First you need to generate the private and public key"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#2-modify-namedconf","text":"Add this line to named.conf : include \"/etc/bind/dns.keys\";","title":"2 Modify named.conf"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#3-create-dnskeys-configuration-file","text":"It must look like something similar to this example: cat dns.keys key dyn-key. { algorithm HMAC-MD5; secret \"fop39Dcbz9HZ9sQqzo64fHorSIJXnmGjJ980BwTg6O4=\"; }; We have to stop here for some words. Where is the \"secret\" come from? You can find this private key in Kdyn-key.+157+60890.private . In my case: cat Kdyn-key.+157+60890.private Private-key-format: v1.3 Algorithm: 157 (HMAC_MD5) Key: fop39Dcbz9HZ9sQqzo64fHorSIJXnmGjJ980BwTg6O4= Bits: AAA= Created: 20161015122904 Publish: 20161015122904 Activate: 20161015122904","title":"3. Create dns.keys configuration file"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#4-allow-update-zone-with-these-keys","text":"Example: zone \"dyn.vinczejanos.info\" { type master; file \"/etc/bind/db.dyn.vinczejanos.info\"; allow-query { any; }; allow-update { key \"dyn-key.\"; }; }; After the configuration is done, do not forget to restart bind. /etc/init.d/bind9 restart","title":"4. Allow Update Zone with these keys"},{"location":"old/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#5-check-update","text":"cat update.sh cat << EOF | nsupdate -k \"Kdyn-key.+157+60890.key\" server ns20.vinczejanos.info zone dyn.vinczejanos.info. update delete test-dyn.dyn.vinczejanos.info update add test-dyn.dyn.vinczejanos.info 60 A 192.168.0.1 show send EOF","title":"5. Check Update"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! How To Compile Nodemcu Firmware \u00b6 In this post I will assist you through some easy steps on how to build NodeMCU firmware on your own. To do this firstly I created a vanilla Debian 8 OpenVZ container. If you don't want to bother to compile NodeMCU firmware on your own, you have another option: you can make it online. Here is the link . 0. Step \u00b6 Every time you install a new package you should start with updating your Linux system. apt-get update apt-get upgrade root@nodemcu:~# uname -a Linux nodemcu 2.6.32-39-pve #1 SMP Fri May 8 11:27:35 CEST 2015 x86_64 GNU/Linux root@nodemcu:~# cat /etc/issue Debian GNU/Linux 8 \\n \\l 1. Install the necessary packages \u00b6 apt-get install git gcc make libtool-bin gperf bison flex build-essential texinfo automake libtool cvs autoconf libncurses5-dev help2man wget bzip2 python-dev python-serial python3-serial You may need to install: apt-get install gawk 2. Clone packages from git \u00b6 First we have to create a new user, because esp-open-sdk can not be compiled with root user. useradd nodemcu Next create a working directory for nodemcu user: mkdir /opt/nodemcu Give all permission: chown -R nodemcu:nodemcu /opt/nodemcu/ chmod u+rwx /opt/nodemcu/ Change to nodemcu user sudo su - nodemcu Clone the neccessary packages form Git. cd /opt/nodemcu git clone https://github.com/nodemcu/nodemcu-firmware git clone --recursive https://github.com/pfalcon/esp-open-sdk 3. Compile esp-open-sdk \u00b6 This package is needed to compile the NodeMCU firmware. Esp-open-sdk contains some tools which may will be useful in the future, for example tools to flash you ESP8266 board. Steps: Change directory to /opt/nodemcu/esp-open-sdk cd /opt/nodemcu/esp-open-sdk Run make command make STANDALONE=y If the compilation is successfully finished you should get something like that: make[1]: Leaving directory '/opt/nodemcu/esp-open-sdk/esp-open-lwip' cp -a esp-open-lwip/include/arch esp-open-lwip/include/lwip esp-open-lwip/include/netif \\ esp-open-lwip/include/lwipopts.h \\ /opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/xtensa-lx106-elf/sysroot/usr/include/ Xtensa toolchain is built, to use it: export PATH=/opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/bin:$PATH Espressif ESP8266 SDK is installed, its libraries and headers are merged with the toolchain The point is the export line. Each time you want to compile NodeMCU firmware you have to run this export. 4. Configure and Compile NodeMCU firmware \u00b6 4.1. Configuration \u00b6 Before you run make command there are some configuration to do. Configuration files: /opt/nodemcu/nodemcu-firmware/app/include/user_config.h /opt/nodemcu/nodemcu-firmware/app/include/user_modules.h /opt/nodemcu/nodemcu-firmware/app/include/user_version.h user_version.h In this file you can configure version related properties. Example: #define NODE_VERSION \"NodeMCU 1.5.4.1 - custom bild by jvincze\" #ifndef BUILD_DATE #define BUILD_DATE \"2016-09-23\" user_modules.h Here you can configure that which modules will be included in the firmware. //#define LUA_USE_MODULES_AM2320 --> commented out, won't be compiled //#define LUA_USE_MODULES_APA102 #define LUA_USE_MODULES_BIT --> will be compiled //#define LUA_USE_MODULES_BMP085 user_config.h In this file there are some board related configuration, for example: memory size. NOTE: If you can not connect to your ESP after flashing it try to modify this value in user_config.h : From: #define BIT_RATE_DEFAULT BIT_RATE_115200 To: #define BIT_RATE_DEFAULT BIT_RATE_9600 And re-flash your ESP. 4.2. Compilation \u00b6 Now everything is ready to build our first NodeMCU firmware: export PATH=/opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/bin:$PATH cd /opt/nodemcu/nodemcu-firmware make Our brand now firmware can be found here: nodemcu@nodemcu:/opt/nodemcu/nodemcu-firmware/bin$ ls -al total 392 drwxr-xr-x 2 nodemcu nodemcu 4096 Sep 23 14:02 . drwxr-xr-x 15 nodemcu nodemcu 4096 Sep 23 13:58 .. -rw-r--r-- 1 nodemcu nodemcu 79 Sep 23 09:54 .gitignore -rw-r--r-- 1 nodemcu nodemcu 27808 Sep 23 14:02 0x00000.bin -rw-r--r-- 1 nodemcu nodemcu 354899 Sep 23 14:02 0x10000.bin References \u00b6 https://github.com/nodemcu/nodemcu-firmware http://nodemcu.readthedocs.io/en/master/en/build/#linux-build-environment https://nodemcu-build.com/ http://www.esp8266.com/wiki/doku.php?id=toolchain https://github.com/pfalcon/esp-open-sdk","title":"How to compile NodeMCU firmware"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#how-to-compile-nodemcu-firmware","text":"In this post I will assist you through some easy steps on how to build NodeMCU firmware on your own. To do this firstly I created a vanilla Debian 8 OpenVZ container. If you don't want to bother to compile NodeMCU firmware on your own, you have another option: you can make it online. Here is the link .","title":"How To Compile Nodemcu Firmware"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#0-step","text":"Every time you install a new package you should start with updating your Linux system. apt-get update apt-get upgrade root@nodemcu:~# uname -a Linux nodemcu 2.6.32-39-pve #1 SMP Fri May 8 11:27:35 CEST 2015 x86_64 GNU/Linux root@nodemcu:~# cat /etc/issue Debian GNU/Linux 8 \\n \\l","title":"0. Step"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#1-install-the-necessary-packages","text":"apt-get install git gcc make libtool-bin gperf bison flex build-essential texinfo automake libtool cvs autoconf libncurses5-dev help2man wget bzip2 python-dev python-serial python3-serial You may need to install: apt-get install gawk","title":"1. Install the necessary packages"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#2-clone-packages-from-git","text":"First we have to create a new user, because esp-open-sdk can not be compiled with root user. useradd nodemcu Next create a working directory for nodemcu user: mkdir /opt/nodemcu Give all permission: chown -R nodemcu:nodemcu /opt/nodemcu/ chmod u+rwx /opt/nodemcu/ Change to nodemcu user sudo su - nodemcu Clone the neccessary packages form Git. cd /opt/nodemcu git clone https://github.com/nodemcu/nodemcu-firmware git clone --recursive https://github.com/pfalcon/esp-open-sdk","title":"2. Clone packages from git"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#3-compile-esp-open-sdk","text":"This package is needed to compile the NodeMCU firmware. Esp-open-sdk contains some tools which may will be useful in the future, for example tools to flash you ESP8266 board. Steps: Change directory to /opt/nodemcu/esp-open-sdk cd /opt/nodemcu/esp-open-sdk Run make command make STANDALONE=y If the compilation is successfully finished you should get something like that: make[1]: Leaving directory '/opt/nodemcu/esp-open-sdk/esp-open-lwip' cp -a esp-open-lwip/include/arch esp-open-lwip/include/lwip esp-open-lwip/include/netif \\ esp-open-lwip/include/lwipopts.h \\ /opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/xtensa-lx106-elf/sysroot/usr/include/ Xtensa toolchain is built, to use it: export PATH=/opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/bin:$PATH Espressif ESP8266 SDK is installed, its libraries and headers are merged with the toolchain The point is the export line. Each time you want to compile NodeMCU firmware you have to run this export.","title":"3. Compile esp-open-sdk"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#4-configure-and-compile-nodemcu-firmware","text":"","title":"4. Configure and Compile NodeMCU firmware"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#41-configuration","text":"Before you run make command there are some configuration to do. Configuration files: /opt/nodemcu/nodemcu-firmware/app/include/user_config.h /opt/nodemcu/nodemcu-firmware/app/include/user_modules.h /opt/nodemcu/nodemcu-firmware/app/include/user_version.h user_version.h In this file you can configure version related properties. Example: #define NODE_VERSION \"NodeMCU 1.5.4.1 - custom bild by jvincze\" #ifndef BUILD_DATE #define BUILD_DATE \"2016-09-23\" user_modules.h Here you can configure that which modules will be included in the firmware. //#define LUA_USE_MODULES_AM2320 --> commented out, won't be compiled //#define LUA_USE_MODULES_APA102 #define LUA_USE_MODULES_BIT --> will be compiled //#define LUA_USE_MODULES_BMP085 user_config.h In this file there are some board related configuration, for example: memory size. NOTE: If you can not connect to your ESP after flashing it try to modify this value in user_config.h : From: #define BIT_RATE_DEFAULT BIT_RATE_115200 To: #define BIT_RATE_DEFAULT BIT_RATE_9600 And re-flash your ESP.","title":"4.1. Configuration"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#42-compilation","text":"Now everything is ready to build our first NodeMCU firmware: export PATH=/opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/bin:$PATH cd /opt/nodemcu/nodemcu-firmware make Our brand now firmware can be found here: nodemcu@nodemcu:/opt/nodemcu/nodemcu-firmware/bin$ ls -al total 392 drwxr-xr-x 2 nodemcu nodemcu 4096 Sep 23 14:02 . drwxr-xr-x 15 nodemcu nodemcu 4096 Sep 23 13:58 .. -rw-r--r-- 1 nodemcu nodemcu 79 Sep 23 09:54 .gitignore -rw-r--r-- 1 nodemcu nodemcu 27808 Sep 23 14:02 0x00000.bin -rw-r--r-- 1 nodemcu nodemcu 354899 Sep 23 14:02 0x10000.bin","title":"4.2. Compilation"},{"location":"old/nodemcu/How_To_Compile_Nodemcu_Firmware/#references","text":"https://github.com/nodemcu/nodemcu-firmware http://nodemcu.readthedocs.io/en/master/en/build/#linux-build-environment https://nodemcu-build.com/ http://www.esp8266.com/wiki/doku.php?id=toolchain https://github.com/pfalcon/esp-open-sdk","title":"References"},{"location":"old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! How To Unbrick ESP8266 (Blinking Blue Led) \u00b6 TL;DR \u00b6 I have some ESP8266 (NodeMCU DEV kit, ESP-01 and ESP07). Two of them bricked during firmware upgrade (Flashing). The first was my mistake because I mistyped the memory address, and the other was a power outage. So I had two bricked ESP. After I powered up the board, the blue led was blinking continuously and rapidly. I tried to update the firmware with custom builds and with builds from https://nodemcu-build.com/ , but all my tries was unsuccessful. I was a bit upset and was thinking about getting rid of them, but I never give up anything so easily. After doing some google search I found some articles about the memory map of the ESP, and some article on \"how to update boot loader\". Honestly, I'm not a developer, and I couldn't say everything I read was clear to me, but finally I successfully brought two ESP back from the death. :) I'm writing this post just because I think it will be useful for someone, some time. I absolutely do not guarantee that this method will work in any cases, the only think I can suggest: do not give up trying, and googleing. :) And note that maybe your ESP has a different memory layout, so first do some search to find as many details about your ESP as you can. 1. Check your EPS8266 Symptoms \u00b6 As I mentioned above the blue led on my esp was continuously blinking, when connected it to my computer. I used an USB to serial converter, and saw this message repeating endlessly: ets Jan 8 2013,rst cause:2, boot mode:(3,6) load 0x40100000, len 25952, room 16 tail 0 chksum 0x9a load 0x3ffe8000, len 2276, room 8 tail 12 chksum 0x03 ho 0 tail 12 room 4 load 0x3ffe88e4, len 8, room 12 tail 8 chksum 0x3f csum 0x3f rf cal sector: 251 rf[112] I think this means that the ESP was restarting continuously, maybe because it couldn't find the boot loader or any of the necessary files. I could upload the firmware, so I think one of the following files must have been missing, corrupt or overwritten: blank.bin boot_v1.5.bin esp_init_data_default.bin 2. Collect The Necessary Files \u00b6 The easiest way to do is download from this link . Or you can download the SDK from Gitub as well. If the links become broken or unavailable, you have two options: 1. Do some search (Google is your best friend. :)) 2. Compile the Nodemcu firmware (with SDK). To do this you can follow my previous post . I don't want to write the whole process again, so after you successfully compiled NodeMCU firmware on your own, you should have the necessary files. Actually you only have to compile \"esp-open-sdk\" , the NodeMCU firmware isn't definitely needed to unbrick, if you have a pre-compiled firmware, you can use it. NOTE: If you use some downloaded firmware instead of compiling one, please check which SDK was used for compiling! The files you will need can be located here: nodemcu@openhab:~/esp-open-sdk/ESP8266_NONOS_SDK_V1.5.4_16_05_20/bin$ find . ./boot_v1.2.bin ./boot_v1.5.bin ./upgrade ./esp_init_data_default.bin ./at ./at/512+512 ./at/512+512/user1.1024.new.2.bin ./at/512+512/user2.1024.new.2.bin ./at/README.md ./at/1024+1024 ./at/1024+1024/user2.2048.new.5.bin ./at/1024+1024/user1.2048.new.5.bin ./at/noboot ./at/noboot/eagle.irom0text.bin ./at/noboot/eagle.flash.bin ./blank.bin 3. Flashing the ESP \u00b6 Before you flash the files to the ESP, double-check the size of its flash. Based on the following tables [^1] upload the files to the appropriate memory address. I used nodemcu-flasher to flash my ESP with these settings: blank.bin --> 0x7E000 esp_init_data_default.bin --> 0x3FC000 user1.1024.new.2.bin --> 0x01000 boot_v1.5.bin --> 0x00000 The last step is to flash your firmware, for example with a custom build one: nodemcu@openhab:~/nodemcu-firmware/bin$ ls -al total 444 drwxr-xr-x 2 nodemcu nodemcu 4096 Oct 24 18:26 . drwxr-xr-x 15 nodemcu nodemcu 4096 Nov 1 11:28 .. -rw-r--r-- 1 nodemcu nodemcu 28160 Nov 1 11:29 0x00000.bin -rw-r--r-- 1 nodemcu nodemcu 413495 Nov 1 11:29 0x10000.bin -rw-r--r-- 1 nodemcu nodemcu 79 Oct 24 18:20 .gitignore I hope this post will be useful, and you will be able to unbrick your ESPs. REFERENCES: http://jasiek.me/2015/04/28/unbricking-an-esp8266-with-flashing-led.html http://www.electrodragon.com/w/ESP8266_AT_Commands https://nodemcu.readthedocs.io/en/master/en/flash/ (Upgrading Firmware) https://github.com/esp8266/esp8266-wiki/tree/master/sdk https://github.com/esp8266/esp8266-wiki [^1]: * https://espressif.com/sites/default/files/documentation/2a-esp8266-sdk_getting_started_guide_en.pdf","title":"How To Unbrick ESP8266 (Blinking Blue Led)"},{"location":"old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#how-to-unbrick-esp8266-blinking-blue-led","text":"","title":"How To Unbrick ESP8266 (Blinking Blue Led)"},{"location":"old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#tldr","text":"I have some ESP8266 (NodeMCU DEV kit, ESP-01 and ESP07). Two of them bricked during firmware upgrade (Flashing). The first was my mistake because I mistyped the memory address, and the other was a power outage. So I had two bricked ESP. After I powered up the board, the blue led was blinking continuously and rapidly. I tried to update the firmware with custom builds and with builds from https://nodemcu-build.com/ , but all my tries was unsuccessful. I was a bit upset and was thinking about getting rid of them, but I never give up anything so easily. After doing some google search I found some articles about the memory map of the ESP, and some article on \"how to update boot loader\". Honestly, I'm not a developer, and I couldn't say everything I read was clear to me, but finally I successfully brought two ESP back from the death. :) I'm writing this post just because I think it will be useful for someone, some time. I absolutely do not guarantee that this method will work in any cases, the only think I can suggest: do not give up trying, and googleing. :) And note that maybe your ESP has a different memory layout, so first do some search to find as many details about your ESP as you can.","title":"TL;DR"},{"location":"old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#1-check-your-eps8266-symptoms","text":"As I mentioned above the blue led on my esp was continuously blinking, when connected it to my computer. I used an USB to serial converter, and saw this message repeating endlessly: ets Jan 8 2013,rst cause:2, boot mode:(3,6) load 0x40100000, len 25952, room 16 tail 0 chksum 0x9a load 0x3ffe8000, len 2276, room 8 tail 12 chksum 0x03 ho 0 tail 12 room 4 load 0x3ffe88e4, len 8, room 12 tail 8 chksum 0x3f csum 0x3f rf cal sector: 251 rf[112] I think this means that the ESP was restarting continuously, maybe because it couldn't find the boot loader or any of the necessary files. I could upload the firmware, so I think one of the following files must have been missing, corrupt or overwritten: blank.bin boot_v1.5.bin esp_init_data_default.bin","title":"1. Check your EPS8266 Symptoms"},{"location":"old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#2-collect-the-necessary-files","text":"The easiest way to do is download from this link . Or you can download the SDK from Gitub as well. If the links become broken or unavailable, you have two options: 1. Do some search (Google is your best friend. :)) 2. Compile the Nodemcu firmware (with SDK). To do this you can follow my previous post . I don't want to write the whole process again, so after you successfully compiled NodeMCU firmware on your own, you should have the necessary files. Actually you only have to compile \"esp-open-sdk\" , the NodeMCU firmware isn't definitely needed to unbrick, if you have a pre-compiled firmware, you can use it. NOTE: If you use some downloaded firmware instead of compiling one, please check which SDK was used for compiling! The files you will need can be located here: nodemcu@openhab:~/esp-open-sdk/ESP8266_NONOS_SDK_V1.5.4_16_05_20/bin$ find . ./boot_v1.2.bin ./boot_v1.5.bin ./upgrade ./esp_init_data_default.bin ./at ./at/512+512 ./at/512+512/user1.1024.new.2.bin ./at/512+512/user2.1024.new.2.bin ./at/README.md ./at/1024+1024 ./at/1024+1024/user2.2048.new.5.bin ./at/1024+1024/user1.2048.new.5.bin ./at/noboot ./at/noboot/eagle.irom0text.bin ./at/noboot/eagle.flash.bin ./blank.bin","title":"2. Collect The Necessary Files"},{"location":"old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#3-flashing-the-esp","text":"Before you flash the files to the ESP, double-check the size of its flash. Based on the following tables [^1] upload the files to the appropriate memory address. I used nodemcu-flasher to flash my ESP with these settings: blank.bin --> 0x7E000 esp_init_data_default.bin --> 0x3FC000 user1.1024.new.2.bin --> 0x01000 boot_v1.5.bin --> 0x00000 The last step is to flash your firmware, for example with a custom build one: nodemcu@openhab:~/nodemcu-firmware/bin$ ls -al total 444 drwxr-xr-x 2 nodemcu nodemcu 4096 Oct 24 18:26 . drwxr-xr-x 15 nodemcu nodemcu 4096 Nov 1 11:28 .. -rw-r--r-- 1 nodemcu nodemcu 28160 Nov 1 11:29 0x00000.bin -rw-r--r-- 1 nodemcu nodemcu 413495 Nov 1 11:29 0x10000.bin -rw-r--r-- 1 nodemcu nodemcu 79 Oct 24 18:20 .gitignore I hope this post will be useful, and you will be able to unbrick your ESPs. REFERENCES: http://jasiek.me/2015/04/28/unbricking-an-esp8266-with-flashing-led.html http://www.electrodragon.com/w/ESP8266_AT_Commands https://nodemcu.readthedocs.io/en/master/en/flash/ (Upgrading Firmware) https://github.com/esp8266/esp8266-wiki/tree/master/sdk https://github.com/esp8266/esp8266-wiki [^1]: * https://espressif.com/sites/default/files/documentation/2a-esp8266-sdk_getting_started_guide_en.pdf","title":"3. Flashing the ESP"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/","text":"Install OpenALPR on Raspberry PI 3 \u00b6 Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Warning It seems there are some problems with using the latest Tesseract code base. For mor details please see the comments! Info I wrote a now post about this topic. \"Install OpenALPR on Raspberry PI 3 (Part 2)\" Before you start installing OpenALPR I suggest you to go through this and the mentioned post first. They may contain a lot of useful information. In this tutorial I will show how can you install OpenALPR on you Raspberry PI 3. From its home page: OpenALPR is an open source Automatic License Plate Recognition library written in C++ with bindings in C#, Java, Node.js, Go, and Python. The library analyzes images and video streams to identify license plates. The output is the text representation of any license plate characters. So after successfully installation of OpenALPR you Raspberry will be able to recognize License Plates from a single photo or from live stream. Please note that in your country maybe illegal to use this tool on public or even for private use, therefore I use it only for my entertainment. OK. Lets Begin. :) What is needed? \u00b6 Raspberry Image: 2016-05-27-raspbian-jessie.img https://www.raspberrypi.org/downloads/ At least 8GB 16GB microSD card to flash the image. Raspberry PI 2 or 3 (I do not advise RPI 1 because I think it is too slow, and image processing will also be slow, and the compiling process will take much longer) Update & Upgrade \u00b6 Before you start installing alpr and its dependencies update your Raspbian. I use a vanilla image so it is must to update. Comment I will do evry step with root access. (Without root access you are lost, or at least the install process will be much harder.) This Raspberry is only for this project thus I don't have to care about loosing anything, or installing packages which overrides other projects. Run the following commands: apt-get update apt-get upgrade Install the necessary packages \u00b6 Previously I gathered all of the packages are needed to compile alpr and all of its dependencies. apt-get install autoconf automake libtool apt-get install libleptonica-dev apt-get install libicu-dev libpango1.0-dev libcairo2-dev apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev apt-get install python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev apt-get install virtualenvwrapper apt-get install liblog4cplus-dev apt-get install libcurl4-openssl-dev In one line: apt-get install autoconf automake libtool libleptonica-dev libicu-dev libpango1.0-dev \\ libcairo2-dev cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev \\ libswscale-dev python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev \\ libjasper-dev libdc1394-22-dev virtualenvwrapper liblog4cplus-dev libcurl4-openssl-dev Based on another tutorial I know It has a chance that apt-get isntall will fail with \"no package found\". In this case you have to manually find the missing package using apt-cache search .... . It may happen that in the meanwhile the package name or version has been changed therefore apt won't find it. Dependencies \u00b6 I think this chapter will be the hardest to be done. Offical github Install documentation: https://github.com/openalpr/openalpr/wiki/Compilation-instructions-(Ubuntu-Linux) https://github.com/openalpr/openalpr OpenALPR requires the following additional libraries: Tesseract OCR v3.0.4 OpenCV v2.4.8+ And these have them own dependencies. :( Tesseract OCR \u00b6 Download (clone) the package from git. Reference: https://github.com/tesseract-ocr/tesseract/wiki/Compiling cd /usr/local/src/ git clone https://github.com/tesseract-ocr/tesseract If you want to use the exactly same version I used please checkout 3.05.00dev-380-g2660647 . Currently this is the master. Follow these steps: cd /usr/local/src/tesseract ./autogen.sh If you are lucky you will get this message: All done. To build the software now, do something like: $ ./configure [--enable-debug] [...other options] Next step is configure the package, as suggested in the message above run ./configure . :) If you want to know what \"other options\" are available first run ./configure --help . Now I don't want to override the default configuration. If you compile without root access or you want to specify the install location please use this option: Installation directories: --prefix=PREFIX install architecture-independent files in PREFIX [/usr/local] As you can see by default Tesseract will be installed in /usr/local/ . It is OK for me. After configuring run: make - It will take long time. If you want make it faster use -j2 option or if you are brave enough -j4 . :) -j [jobs], --jobs[=jobs] Specifies the number of jobs (commands) to run simultaneously. If there is more than one -j option, the last one is effective. If the -j option is given without an argu\u00e2\u20ac\u0090 ment, make will not limit the number of jobs that can run simultaneously. I have tried with -j4 but it leads to \"segmentation fault\". So I advise you to run max 2 jobs simultaneously. Finish the install with make install command. You can check if the compilation was successfully or not by: root@raspberrypi:/usr/local/src/tesseract# tesseract tesseract: error while loading shared libraries: libtesseract.so.3: cannot open shared object file: No such file or directory If you get the error below try to run ldconfig . Now you can check again: root@raspberrypi:/usr/local/src/tesseract# tesseract -v tesseract 3.05.00dev leptonica-1.71 libgif 4.1.6(?) : libjpeg 6b : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.1 : libopenjp2 2.1.0 Optionally you can install tesseract training: make training sudo make training-install OpenCV \u00b6 Home page Download page Current latest version Install Documentation Pre-Steps: Download the latest version. cd /usr/local/src wget https://github.com/Itseez/opencv/archive/2.4.13.zip mv 2.4.13.zip OpenCV-2.4.13.zip Unzip it unzip -q OpenCV-2.4.13.zip This will create a directory: /usr/local/src/opencv-2.4.13 Run cmake cd /usr/local/src/opencv-2.4.13 mkdir release cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_NEW_PYTHON_SUPPORT=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON .. This comamnd will configure the project. If you are lucky again you will get this message: -- Configuring done -- Generating done -- Build files have been written to: /usr/local/src/opencv-2.4.13/release Run make I strongly recommend to use -j2 option, because this step takes the most of time: root@raspberrypi:/usr/local/src/opencv-2.4.13/release# make -j2 Unfortunately at the first time my make command died with this message: [ 47%] Building CXX object modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o c++: internal compiler error: Segmentation fault (program cc1plus) Please submit a full bug report, with preprocessed source if appropriate. See <file:///usr/share/doc/gcc-4.9/README.Bugs> for instructions. modules/ocl/CMakeFiles/opencv_ocl.dir/build.make:719: recipe for target 'modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o' failed make[2]: *** [modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o] Error 4 CMakeFiles/Makefile2:4734: recipe for target 'modules/ocl/CMakeFiles/opencv_ocl.dir/all' failed make[1]: *** [modules/ocl/CMakeFiles/opencv_ocl.dir/all] Error 2 make[1]: *** Waiting for unfinished jobs.... As I wrote above you may get this error. It may be caused by because you run out of memory. I don't know exact solution for this issue on Raspberry, but I have some suggestion: Reboot you RPI Try not to specify -j option or use -j1 Run make clean and make again Increase swap space. fallocate --length 2GiB /root/2G.swap chmod 0600 /root/2G.swap mkswap /root/2G.swap swapon /root/2G.swap After 'n' (re)tries make -j2 command finished successfully. The next (and final) step with OpenCV is run: make install Install OpenALPR \u00b6 Finally we can continue with OpenALPR. :) Clone the git repository: cd /usr/local/src git clone https://github.com/openalpr/openalpr.git (optional) Check version cd openalpr git describe --tags v2.1.0-513-gcd2aab0 Now this is the master branch. If the master branch is not this version, you can checkout v2.1.0 if you want to use exact same version. git checkout v2.1.0 Run cmake root@raspberrypi:/usr/local/src/openalpr# cd src/ root@raspberrypi:/usr/local/src/openalpr/src# cmake ./ run make root@raspberrypi:/usr/local/src/openalpr/src# make The situation is the same, if make fails try to follow the steps are described earlier. Please remember to run ldconfig . Now you can see that compiling OpenALPR is much easier than installing its dependencies. And please note that I have gathered the necessary packages. At the very first time I installed these packages It took 6 or more hours. Despite the lot of good articles there were a lot of dependencies I had to find manually. And first time I tried to install OpenCV and Tesseract into a custom directories for example /opt/OpenCV and /opt/Tesseract . If you try this you have to manually define these libraries to OpenALPR in CMakeLists.txt . Just for demonstration I tried these settings: SET(OpenCV_DIR \"/opt/opencv-2.4.13/share/OpenCV/\") SET(Tesseract_DIR \"/usr/src/tesseract\") SET(Tesseract_LIB \"/opt/tesseract/lib/\") SET(Tesseract_INCLUDE_DIRS \"/opt/tesseract/include/\") SET(Tesseract_INCLUDE_BASEAPI_DIR \"/opt/tesseract/include\") SET(Tesseract_PKGCONF_INCLUDE_DIRS \"/opt/tesseract/include/tesseract\") But for some reason make always failed because of tesseract. After some hours I was fed up with it and installed tesseract to its default location. I had to recompile only tesseract to successfully compile OpenALPR, OpenCV remained in /opt/ . In this case this line is must inserted to CMakeLists.txt: SET(OpenCV_DIR \"/opt/opencv-2.4.13/share/OpenCV/\") . Ok. Test the newly install alpr system. cd /usr/local/src/openalpr/src root@raspberrypi:/usr/local/src# alpr ea7the.jpg plate0: 10 results - EA7THE confidence: 91.0578 - EA7TBE confidence: 84.133 - EA7T8E confidence: 83.0083 - EA7TRE confidence: 82.7869 - EA7TE confidence: 82.5961 - EA7TME confidence: 80.2908 - EA7TH6 confidence: 77.0045 - EA7THB confidence: 75.5779 - EA7TH confidence: 74.6576 - EA7TB6 confidence: 70.0797 Wow. It is working. :) You can find configuration examples in /usr/local/share/openalpr/config directory. REFERENCES: http://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/ http://docs.opencv.org/2.4/doc/tutorials/introduction/linux_install/linux_install.html#linux-installation https://virtualenv.pypa.io/en/stable/ https://gist.github.com/amstanley/9da7febc9a3e3c2228ee","title":"Install OpenALPR on Raspberry PI 3"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#install-openalpr-on-raspberry-pi-3","text":"Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Warning It seems there are some problems with using the latest Tesseract code base. For mor details please see the comments! Info I wrote a now post about this topic. \"Install OpenALPR on Raspberry PI 3 (Part 2)\" Before you start installing OpenALPR I suggest you to go through this and the mentioned post first. They may contain a lot of useful information. In this tutorial I will show how can you install OpenALPR on you Raspberry PI 3. From its home page: OpenALPR is an open source Automatic License Plate Recognition library written in C++ with bindings in C#, Java, Node.js, Go, and Python. The library analyzes images and video streams to identify license plates. The output is the text representation of any license plate characters. So after successfully installation of OpenALPR you Raspberry will be able to recognize License Plates from a single photo or from live stream. Please note that in your country maybe illegal to use this tool on public or even for private use, therefore I use it only for my entertainment. OK. Lets Begin. :)","title":"Install OpenALPR on Raspberry PI 3"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#what-is-needed","text":"Raspberry Image: 2016-05-27-raspbian-jessie.img https://www.raspberrypi.org/downloads/ At least 8GB 16GB microSD card to flash the image. Raspberry PI 2 or 3 (I do not advise RPI 1 because I think it is too slow, and image processing will also be slow, and the compiling process will take much longer)","title":"What is needed?"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#update-upgrade","text":"Before you start installing alpr and its dependencies update your Raspbian. I use a vanilla image so it is must to update. Comment I will do evry step with root access. (Without root access you are lost, or at least the install process will be much harder.) This Raspberry is only for this project thus I don't have to care about loosing anything, or installing packages which overrides other projects. Run the following commands: apt-get update apt-get upgrade","title":"Update &amp; Upgrade"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#install-the-necessary-packages","text":"Previously I gathered all of the packages are needed to compile alpr and all of its dependencies. apt-get install autoconf automake libtool apt-get install libleptonica-dev apt-get install libicu-dev libpango1.0-dev libcairo2-dev apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev apt-get install python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev apt-get install virtualenvwrapper apt-get install liblog4cplus-dev apt-get install libcurl4-openssl-dev In one line: apt-get install autoconf automake libtool libleptonica-dev libicu-dev libpango1.0-dev \\ libcairo2-dev cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev \\ libswscale-dev python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev \\ libjasper-dev libdc1394-22-dev virtualenvwrapper liblog4cplus-dev libcurl4-openssl-dev Based on another tutorial I know It has a chance that apt-get isntall will fail with \"no package found\". In this case you have to manually find the missing package using apt-cache search .... . It may happen that in the meanwhile the package name or version has been changed therefore apt won't find it.","title":"Install the necessary packages"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#dependencies","text":"I think this chapter will be the hardest to be done. Offical github Install documentation: https://github.com/openalpr/openalpr/wiki/Compilation-instructions-(Ubuntu-Linux) https://github.com/openalpr/openalpr OpenALPR requires the following additional libraries: Tesseract OCR v3.0.4 OpenCV v2.4.8+ And these have them own dependencies. :(","title":"Dependencies"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#tesseract-ocr","text":"Download (clone) the package from git. Reference: https://github.com/tesseract-ocr/tesseract/wiki/Compiling cd /usr/local/src/ git clone https://github.com/tesseract-ocr/tesseract If you want to use the exactly same version I used please checkout 3.05.00dev-380-g2660647 . Currently this is the master. Follow these steps: cd /usr/local/src/tesseract ./autogen.sh If you are lucky you will get this message: All done. To build the software now, do something like: $ ./configure [--enable-debug] [...other options] Next step is configure the package, as suggested in the message above run ./configure . :) If you want to know what \"other options\" are available first run ./configure --help . Now I don't want to override the default configuration. If you compile without root access or you want to specify the install location please use this option: Installation directories: --prefix=PREFIX install architecture-independent files in PREFIX [/usr/local] As you can see by default Tesseract will be installed in /usr/local/ . It is OK for me. After configuring run: make - It will take long time. If you want make it faster use -j2 option or if you are brave enough -j4 . :) -j [jobs], --jobs[=jobs] Specifies the number of jobs (commands) to run simultaneously. If there is more than one -j option, the last one is effective. If the -j option is given without an argu\u00e2\u20ac\u0090 ment, make will not limit the number of jobs that can run simultaneously. I have tried with -j4 but it leads to \"segmentation fault\". So I advise you to run max 2 jobs simultaneously. Finish the install with make install command. You can check if the compilation was successfully or not by: root@raspberrypi:/usr/local/src/tesseract# tesseract tesseract: error while loading shared libraries: libtesseract.so.3: cannot open shared object file: No such file or directory If you get the error below try to run ldconfig . Now you can check again: root@raspberrypi:/usr/local/src/tesseract# tesseract -v tesseract 3.05.00dev leptonica-1.71 libgif 4.1.6(?) : libjpeg 6b : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.1 : libopenjp2 2.1.0 Optionally you can install tesseract training: make training sudo make training-install","title":"Tesseract OCR"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#opencv","text":"Home page Download page Current latest version Install Documentation Pre-Steps: Download the latest version. cd /usr/local/src wget https://github.com/Itseez/opencv/archive/2.4.13.zip mv 2.4.13.zip OpenCV-2.4.13.zip Unzip it unzip -q OpenCV-2.4.13.zip This will create a directory: /usr/local/src/opencv-2.4.13 Run cmake cd /usr/local/src/opencv-2.4.13 mkdir release cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_NEW_PYTHON_SUPPORT=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON .. This comamnd will configure the project. If you are lucky again you will get this message: -- Configuring done -- Generating done -- Build files have been written to: /usr/local/src/opencv-2.4.13/release Run make I strongly recommend to use -j2 option, because this step takes the most of time: root@raspberrypi:/usr/local/src/opencv-2.4.13/release# make -j2 Unfortunately at the first time my make command died with this message: [ 47%] Building CXX object modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o c++: internal compiler error: Segmentation fault (program cc1plus) Please submit a full bug report, with preprocessed source if appropriate. See <file:///usr/share/doc/gcc-4.9/README.Bugs> for instructions. modules/ocl/CMakeFiles/opencv_ocl.dir/build.make:719: recipe for target 'modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o' failed make[2]: *** [modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o] Error 4 CMakeFiles/Makefile2:4734: recipe for target 'modules/ocl/CMakeFiles/opencv_ocl.dir/all' failed make[1]: *** [modules/ocl/CMakeFiles/opencv_ocl.dir/all] Error 2 make[1]: *** Waiting for unfinished jobs.... As I wrote above you may get this error. It may be caused by because you run out of memory. I don't know exact solution for this issue on Raspberry, but I have some suggestion: Reboot you RPI Try not to specify -j option or use -j1 Run make clean and make again Increase swap space. fallocate --length 2GiB /root/2G.swap chmod 0600 /root/2G.swap mkswap /root/2G.swap swapon /root/2G.swap After 'n' (re)tries make -j2 command finished successfully. The next (and final) step with OpenCV is run: make install","title":"OpenCV"},{"location":"old/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#install-openalpr","text":"Finally we can continue with OpenALPR. :) Clone the git repository: cd /usr/local/src git clone https://github.com/openalpr/openalpr.git (optional) Check version cd openalpr git describe --tags v2.1.0-513-gcd2aab0 Now this is the master branch. If the master branch is not this version, you can checkout v2.1.0 if you want to use exact same version. git checkout v2.1.0 Run cmake root@raspberrypi:/usr/local/src/openalpr# cd src/ root@raspberrypi:/usr/local/src/openalpr/src# cmake ./ run make root@raspberrypi:/usr/local/src/openalpr/src# make The situation is the same, if make fails try to follow the steps are described earlier. Please remember to run ldconfig . Now you can see that compiling OpenALPR is much easier than installing its dependencies. And please note that I have gathered the necessary packages. At the very first time I installed these packages It took 6 or more hours. Despite the lot of good articles there were a lot of dependencies I had to find manually. And first time I tried to install OpenCV and Tesseract into a custom directories for example /opt/OpenCV and /opt/Tesseract . If you try this you have to manually define these libraries to OpenALPR in CMakeLists.txt . Just for demonstration I tried these settings: SET(OpenCV_DIR \"/opt/opencv-2.4.13/share/OpenCV/\") SET(Tesseract_DIR \"/usr/src/tesseract\") SET(Tesseract_LIB \"/opt/tesseract/lib/\") SET(Tesseract_INCLUDE_DIRS \"/opt/tesseract/include/\") SET(Tesseract_INCLUDE_BASEAPI_DIR \"/opt/tesseract/include\") SET(Tesseract_PKGCONF_INCLUDE_DIRS \"/opt/tesseract/include/tesseract\") But for some reason make always failed because of tesseract. After some hours I was fed up with it and installed tesseract to its default location. I had to recompile only tesseract to successfully compile OpenALPR, OpenCV remained in /opt/ . In this case this line is must inserted to CMakeLists.txt: SET(OpenCV_DIR \"/opt/opencv-2.4.13/share/OpenCV/\") . Ok. Test the newly install alpr system. cd /usr/local/src/openalpr/src root@raspberrypi:/usr/local/src# alpr ea7the.jpg plate0: 10 results - EA7THE confidence: 91.0578 - EA7TBE confidence: 84.133 - EA7T8E confidence: 83.0083 - EA7TRE confidence: 82.7869 - EA7TE confidence: 82.5961 - EA7TME confidence: 80.2908 - EA7TH6 confidence: 77.0045 - EA7THB confidence: 75.5779 - EA7TH confidence: 74.6576 - EA7TB6 confidence: 70.0797 Wow. It is working. :) You can find configuration examples in /usr/local/share/openalpr/config directory. REFERENCES: http://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/ http://docs.opencv.org/2.4/doc/tutorials/introduction/linux_install/linux_install.html#linux-installation https://virtualenv.pypa.io/en/stable/ https://gist.github.com/amstanley/9da7febc9a3e3c2228ee","title":"Install OpenALPR"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Logging MQTT data (subscription) to MySQL with Shell Script \u00b6 I have a several ESPs which are continuously logging humidity and temperature values. I've decided to save all data to a Mysql database for later use or analysis. I found two different way to do this: OpenHAB persistence Shell script Both have advantages and disadvantages as well, but you can use both at the same time, and after a while you can choose he best for you. Or you can use NodeRed to logging data to DB if you don't like my solutions. 0. My Data Model \u00b6 All of my ESPs are sending data to a mqtt broker (mosquitto) using this topic format: \"NodeMCU/[NODEID]/status/[MEASSURE]\" and the data (value). Here is an example about what kind of messages are sent by one of my ESP: NodeMCU/585548/status/nodeid 585548 NodeMCU/585548/status/contacts/5 0 NodeMCU/585548/status/humidity 58 NodeMCU/585548/status/sta_macaddr 60:01:94:08:ef:4c NodeMCU/585548/status/contacts/1 0 NodeMCU/585548/status/temperature -1.5 NodeMCU/585548/status/ap_macaddr 62:01:94:08:ef:4c NodeMCU/585548/status/reboot 168457 NodeMCU/585548/status/uptime 1485351907 NodeMCU/585548/status/heap 19376 NodeMCU/585548/status/ipaddr 172.31.0.168 NodeMCU/585548/status/epoch 1485520364 NodeMCU/585548/status/rssi -57 NodeMCU/585548/status/voltage 3.531 NodeMCU/585548/status/publicip 46.1*7.*3.15* How do I collect the data? function module.collectData() -- GENERAL STATUS UPDATE local table_status = { [\"nodeid\"] = node.chipid(), [\"sta_macaddr\"] = wifi.sta.getmac(), [\"ap_macaddr\"] = wifi.ap.getmac(), [\"ipaddr\"] = wifi.sta.getip(), [\"rssi\"] = wifi.sta.getrssi(), [\"epoch\"] = rtctime.get(), [\"reboot\"] = tmr.time(), [\"uptime\"] = rtctime.get() - tmr.time(), [\"publicip\"] = ipaddr, [\"heap\"] = node.heap(), [\"voltage\"] = adc.readvdd33(0)/1000, } -- ############# (optional) - DHT local status, temperature, humidity, temp_dec, humi_dec = dht.read(config.dhtPins) table_status[\"temperature\"]=temperature table_status[\"humidity\"]=humidity return table_status end -- End of collectData How do I publish these data? function module.publishData(mqtt,toPublishTable) -- PUBLISH DATA for st,va in pairs(toPublishTable) do m:publish(config.mqtt.publishTopicStatus..st,va,0,1) end end I call this function (publishData()) by using a timer in every 60 secs. 1. OpenHAB persistence \u00b6 OpenHAB supports saving data to MySQL database by setting up the mysql persistence. 1.1. Turn on MySQL persistance \u00b6 Unzip org.openhab.persistence.mysql-1.8.3.jar to your addons directory. In my case: /opt/openhab/runtime/distribution-1.8.3-runtime/addons You can download all addons from the OpenHAB official web page: OpenHAB downloads Edit openhab.cfg file. Set the following properties: persistence:default=mysql mysql:url=jdbc:mysql://172.18.0.105:3306/openhab mysql:user=openhab mysql:password=openhab mysql:localtime=true OpenHAB will create all necessary table. 1.2. Configuration \u00b6 Create configuration file for MySQL persistence: configurations/persistence/mysql.persist Example: Strategies { default = everyChange } Items { Temperatures* -> \"Temperatures\" Humidities* -> \"Humidities\" prd_batt_349307_voltage -> \"Battery Voltage\" } For the better understand here is my Temperatures and Humidities group config and \"prd_batt_349307_voltage\" configuration: Temperature: Number prd_347920_temp \"Shaft Temperature [%.1f \u00c3\u201a\u00c2\u00b0C]\" <temperature> (GroupShed,Temperatures) {mqtt=\"<[banana:NodeMCU/347920/status/temperature:state:default\"], autoupdate=\"true\" } Humidity: Number prd_347920_humi \"Shaft Humidity [%.1f %%]\" <humi> (GroupShed,Humidities) {mqtt=\"<[banana:NodeMCU/347920/status/humidity:state:default\"], autoupdate=\"true\" } prd_batt_349307_voltage: This is a battery powered DHT22 sensor with ESP01. Number prd_batt_349307_voltage \"Batt Voltage: [%.1f mV]\" <info> (BattDHT_1) {mqtt=\"<[banana:NodeMCU/349307/status/voltage:state:default\"], autoupdate=\"true\" } OpenHAB will log all temperature and humidity values to the MySQL database when the value has changed. 1.3. The Database and Tables \u00b6 OpenHAB creates a table for all logged items, and there is another table which contains the ID and the name of the items: mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"show tables;\" openhab Item1 Item2 Item3 ... ... Items Items table: mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"select * from Items;\" openhab 1 prd_81425_humi 2 prd_80100_humi 3 test_384849_humi 4 prd_batt_349307_temp ... ... ... For example the values of prd_batt_349307_temp can be queried from the Item4 table. test_384849_humi can be found in the Item3 table. You can query the minimum temperature since 2017.01.25: mysql -h 172.18.0.105 -u openhab -popenhab -e \"select min(value) from Item4 where Time >= '2017-01-25';\" openhab +------------+ | min(value) | +------------+ | 18.1 | +------------+ 1.4. Remove incorrect data from the database \u00b6 Unfortunately sometimes the temperature and humidity values are incorrect. This means a very low or very high values (>100 ; <-100), so I remove these entries from the database with a simple shell script which runs on every hour from crontab: #!/bin/bash IFS=' ' LOG=\"/opt/openhab/custom_scripts/clean_logs/logfile.log\" exec >> $LOG 2>&1 echo \"#################### $( date +%F\\ %T ) ####################\" for TABLE in $( mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e 'show tables;' openhab | grep -v Items ) do ITEMID=$( echo $TABLE | sed 's#[^0-9]##g' ) ITEM_NAME=$( mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"select ItemName from Items where ItemId=$ITEMID;\" openhab ) echo \"---------------- $TABLE\" echo \"Item: $ITEM_NAME\" echo \"$ITEM_NAME\" | egrep -q '(temp|humi)' TO_CLEAN=$? if [ $TO_CLEAN -eq 0 ] then mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"select * from $TABLE where abs(Value) > 100 ;\" openhab mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"delete from $TABLE where abs(Value) > 100 ;\" openhab else echo \"NO Humi or temp\" fi echo done OpenHAB persistence can work with all bindings, not only for MQTT. So if you are already using OpneHAB this method maybe the most suitable for you. 2. Shell (bash) Script \u00b6 My another solution to log mqtt data to MySQL data is writing a simple shell script which subscribe to one or more topics, and INSERT data to the DB right after the message is received from the MQTT broker. 2.1. Create The Database \u00b6 Unfortunately nobody will create the database and tables for you, so you have to do this on your own. You are lucky because I share mine with you. Create database CREATE DATABASE IF NOT EXISTS `nodemcu` DEFAULT CHARACTER SET latin1 COLLATE latin1_swedish_ci; -- Optional: USE `nodemcu`; Create table DROP TABLE IF EXISTS `esps`; CREATE TABLE IF NOT EXISTS `esps` ( `_id` int(11) NOT NULL, `timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `nodeid` int(11) NOT NULL, `measure` text NOT NULL, `value` float NOT NULL, `comment` text NOT NULL ) ENGINE=InnoDB AUTO_INCREMENT=49334 DEFAULT CHARSET=latin1; Add indexes: ALTER TABLE `esps` ADD PRIMARY KEY (`_id`), ADD KEY `nodeid` (`nodeid`), ADD KEY `value` (`value`); Create user and GRANT accees: CREATE USER 'nodemcu'@'%' IDENTIFIED BY 'nodemcu'; GRANT USAGE ON *.* TO 'nodemcu'@'%' IDENTIFIED BY 'nodemcu' WITH MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0; 2.2. Shell Script \u00b6 #!/bin/bash IFS=' ' mosquitto_sub -R -v -h 172.16.0.250 -u vinyo -P ***** -t 'NodeMCU/+/status/temperature' -t 'NodeMCU/+/status/humidity' -t 'NodeMCU/+/status/voltage' | while read RAW_DATA do NODEID=$( echo $RAW_DATA | cut -f 2 -d\"/\" ) MEASURE=$( echo $RAW_DATA | cut -f 4 -d\"/\" | cut -f1 -d\" \" ) VALUE=$( echo $RAW_DATA | cut -f 2 -d\" \" ) LAST_VALUE=$( mysql -h 172.18.0.105 -u nodemcu -pnodemcu -N -s -e \"select value from esps where nodeid='$NODEID' and measure='$MEASURE' order by _id DESC LIMIT 1;\" nodemcu ) [ -z $LAST_VALUE ] && LAST_VALUE=0 if [ $LAST_VALUE != $VALUE ] then echo \"INSERT (NodeId: $NODEID; MEASURE: $MEASURE ( $LAST_VALUE --> $VALUE )\" mysql -h 172.18.0.105 -u nodemcu -pnodemcu -e \"insert into esps(nodeid,measure,value) VALUES('$NODEID','$MEASURE','$VALUE');\" nodemcu else echo \"Not Changed: (NodeId: $NODEID; MEASURE: $MEASURE ( $LAST_VALUE --> $VALUE )\" fi done Explanation: The script inserts data only when it differs from the previous value ($LAST_VALUE). It is important because ESPs send messages very frequently, and without this the db would grow fast. At the first start (when there is no data in the db) 0 will be used as the $LAST_VALUE . [ -z $LAST_VALUE ] && LAST_VALUE=0 Without this, at the first start the \"if\" statement would run into an error. It logs \"only\" the temperature, humidity and voltage values by subscripting these topics: mosquitto_sub -R -v -h 172.16.0.250 -u v*n*y*a -P ***** -t 'NodeMCU/+/status/temperature' -t 'NodeMCU/+/status/humidity' -t 'NodeMCU/+/status/voltage' This scripts log to the standard output. Of course you can redirect all output to a log file by putting the following line to script (before the mosquitto_sub command). exec >> /path/of/the/log/file 2>&1 How to run in the background? Simply use the well-known method: nohup + command + $ nohup ./script-name-sh &","title":"Logging MQTT data (subscription) to MySQL with Shell Script"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#logging-mqtt-data-subscription-to-mysql-with-shell-script","text":"I have a several ESPs which are continuously logging humidity and temperature values. I've decided to save all data to a Mysql database for later use or analysis. I found two different way to do this: OpenHAB persistence Shell script Both have advantages and disadvantages as well, but you can use both at the same time, and after a while you can choose he best for you. Or you can use NodeRed to logging data to DB if you don't like my solutions.","title":"Logging MQTT data (subscription) to MySQL with Shell Script"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#0-my-data-model","text":"All of my ESPs are sending data to a mqtt broker (mosquitto) using this topic format: \"NodeMCU/[NODEID]/status/[MEASSURE]\" and the data (value). Here is an example about what kind of messages are sent by one of my ESP: NodeMCU/585548/status/nodeid 585548 NodeMCU/585548/status/contacts/5 0 NodeMCU/585548/status/humidity 58 NodeMCU/585548/status/sta_macaddr 60:01:94:08:ef:4c NodeMCU/585548/status/contacts/1 0 NodeMCU/585548/status/temperature -1.5 NodeMCU/585548/status/ap_macaddr 62:01:94:08:ef:4c NodeMCU/585548/status/reboot 168457 NodeMCU/585548/status/uptime 1485351907 NodeMCU/585548/status/heap 19376 NodeMCU/585548/status/ipaddr 172.31.0.168 NodeMCU/585548/status/epoch 1485520364 NodeMCU/585548/status/rssi -57 NodeMCU/585548/status/voltage 3.531 NodeMCU/585548/status/publicip 46.1*7.*3.15* How do I collect the data? function module.collectData() -- GENERAL STATUS UPDATE local table_status = { [\"nodeid\"] = node.chipid(), [\"sta_macaddr\"] = wifi.sta.getmac(), [\"ap_macaddr\"] = wifi.ap.getmac(), [\"ipaddr\"] = wifi.sta.getip(), [\"rssi\"] = wifi.sta.getrssi(), [\"epoch\"] = rtctime.get(), [\"reboot\"] = tmr.time(), [\"uptime\"] = rtctime.get() - tmr.time(), [\"publicip\"] = ipaddr, [\"heap\"] = node.heap(), [\"voltage\"] = adc.readvdd33(0)/1000, } -- ############# (optional) - DHT local status, temperature, humidity, temp_dec, humi_dec = dht.read(config.dhtPins) table_status[\"temperature\"]=temperature table_status[\"humidity\"]=humidity return table_status end -- End of collectData How do I publish these data? function module.publishData(mqtt,toPublishTable) -- PUBLISH DATA for st,va in pairs(toPublishTable) do m:publish(config.mqtt.publishTopicStatus..st,va,0,1) end end I call this function (publishData()) by using a timer in every 60 secs.","title":"0. My Data Model"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#1-openhab-persistence","text":"OpenHAB supports saving data to MySQL database by setting up the mysql persistence.","title":"1. OpenHAB persistence"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#11-turn-on-mysql-persistance","text":"Unzip org.openhab.persistence.mysql-1.8.3.jar to your addons directory. In my case: /opt/openhab/runtime/distribution-1.8.3-runtime/addons You can download all addons from the OpenHAB official web page: OpenHAB downloads Edit openhab.cfg file. Set the following properties: persistence:default=mysql mysql:url=jdbc:mysql://172.18.0.105:3306/openhab mysql:user=openhab mysql:password=openhab mysql:localtime=true OpenHAB will create all necessary table.","title":"1.1. Turn on MySQL persistance"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#12-configuration","text":"Create configuration file for MySQL persistence: configurations/persistence/mysql.persist Example: Strategies { default = everyChange } Items { Temperatures* -> \"Temperatures\" Humidities* -> \"Humidities\" prd_batt_349307_voltage -> \"Battery Voltage\" } For the better understand here is my Temperatures and Humidities group config and \"prd_batt_349307_voltage\" configuration: Temperature: Number prd_347920_temp \"Shaft Temperature [%.1f \u00c3\u201a\u00c2\u00b0C]\" <temperature> (GroupShed,Temperatures) {mqtt=\"<[banana:NodeMCU/347920/status/temperature:state:default\"], autoupdate=\"true\" } Humidity: Number prd_347920_humi \"Shaft Humidity [%.1f %%]\" <humi> (GroupShed,Humidities) {mqtt=\"<[banana:NodeMCU/347920/status/humidity:state:default\"], autoupdate=\"true\" } prd_batt_349307_voltage: This is a battery powered DHT22 sensor with ESP01. Number prd_batt_349307_voltage \"Batt Voltage: [%.1f mV]\" <info> (BattDHT_1) {mqtt=\"<[banana:NodeMCU/349307/status/voltage:state:default\"], autoupdate=\"true\" } OpenHAB will log all temperature and humidity values to the MySQL database when the value has changed.","title":"1.2. Configuration"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#13-the-database-and-tables","text":"OpenHAB creates a table for all logged items, and there is another table which contains the ID and the name of the items: mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"show tables;\" openhab Item1 Item2 Item3 ... ... Items Items table: mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"select * from Items;\" openhab 1 prd_81425_humi 2 prd_80100_humi 3 test_384849_humi 4 prd_batt_349307_temp ... ... ... For example the values of prd_batt_349307_temp can be queried from the Item4 table. test_384849_humi can be found in the Item3 table. You can query the minimum temperature since 2017.01.25: mysql -h 172.18.0.105 -u openhab -popenhab -e \"select min(value) from Item4 where Time >= '2017-01-25';\" openhab +------------+ | min(value) | +------------+ | 18.1 | +------------+","title":"1.3. The Database and Tables"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#14-remove-incorrect-data-from-the-database","text":"Unfortunately sometimes the temperature and humidity values are incorrect. This means a very low or very high values (>100 ; <-100), so I remove these entries from the database with a simple shell script which runs on every hour from crontab: #!/bin/bash IFS=' ' LOG=\"/opt/openhab/custom_scripts/clean_logs/logfile.log\" exec >> $LOG 2>&1 echo \"#################### $( date +%F\\ %T ) ####################\" for TABLE in $( mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e 'show tables;' openhab | grep -v Items ) do ITEMID=$( echo $TABLE | sed 's#[^0-9]##g' ) ITEM_NAME=$( mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"select ItemName from Items where ItemId=$ITEMID;\" openhab ) echo \"---------------- $TABLE\" echo \"Item: $ITEM_NAME\" echo \"$ITEM_NAME\" | egrep -q '(temp|humi)' TO_CLEAN=$? if [ $TO_CLEAN -eq 0 ] then mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"select * from $TABLE where abs(Value) > 100 ;\" openhab mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e \"delete from $TABLE where abs(Value) > 100 ;\" openhab else echo \"NO Humi or temp\" fi echo done OpenHAB persistence can work with all bindings, not only for MQTT. So if you are already using OpneHAB this method maybe the most suitable for you.","title":"1.4. Remove incorrect data from the database"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#2-shell-bash-script","text":"My another solution to log mqtt data to MySQL data is writing a simple shell script which subscribe to one or more topics, and INSERT data to the DB right after the message is received from the MQTT broker.","title":"2. Shell (bash) Script"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#21-create-the-database","text":"Unfortunately nobody will create the database and tables for you, so you have to do this on your own. You are lucky because I share mine with you. Create database CREATE DATABASE IF NOT EXISTS `nodemcu` DEFAULT CHARACTER SET latin1 COLLATE latin1_swedish_ci; -- Optional: USE `nodemcu`; Create table DROP TABLE IF EXISTS `esps`; CREATE TABLE IF NOT EXISTS `esps` ( `_id` int(11) NOT NULL, `timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, `nodeid` int(11) NOT NULL, `measure` text NOT NULL, `value` float NOT NULL, `comment` text NOT NULL ) ENGINE=InnoDB AUTO_INCREMENT=49334 DEFAULT CHARSET=latin1; Add indexes: ALTER TABLE `esps` ADD PRIMARY KEY (`_id`), ADD KEY `nodeid` (`nodeid`), ADD KEY `value` (`value`); Create user and GRANT accees: CREATE USER 'nodemcu'@'%' IDENTIFIED BY 'nodemcu'; GRANT USAGE ON *.* TO 'nodemcu'@'%' IDENTIFIED BY 'nodemcu' WITH MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0;","title":"2.1. Create The Database"},{"location":"old/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#22-shell-script","text":"#!/bin/bash IFS=' ' mosquitto_sub -R -v -h 172.16.0.250 -u vinyo -P ***** -t 'NodeMCU/+/status/temperature' -t 'NodeMCU/+/status/humidity' -t 'NodeMCU/+/status/voltage' | while read RAW_DATA do NODEID=$( echo $RAW_DATA | cut -f 2 -d\"/\" ) MEASURE=$( echo $RAW_DATA | cut -f 4 -d\"/\" | cut -f1 -d\" \" ) VALUE=$( echo $RAW_DATA | cut -f 2 -d\" \" ) LAST_VALUE=$( mysql -h 172.18.0.105 -u nodemcu -pnodemcu -N -s -e \"select value from esps where nodeid='$NODEID' and measure='$MEASURE' order by _id DESC LIMIT 1;\" nodemcu ) [ -z $LAST_VALUE ] && LAST_VALUE=0 if [ $LAST_VALUE != $VALUE ] then echo \"INSERT (NodeId: $NODEID; MEASURE: $MEASURE ( $LAST_VALUE --> $VALUE )\" mysql -h 172.18.0.105 -u nodemcu -pnodemcu -e \"insert into esps(nodeid,measure,value) VALUES('$NODEID','$MEASURE','$VALUE');\" nodemcu else echo \"Not Changed: (NodeId: $NODEID; MEASURE: $MEASURE ( $LAST_VALUE --> $VALUE )\" fi done Explanation: The script inserts data only when it differs from the previous value ($LAST_VALUE). It is important because ESPs send messages very frequently, and without this the db would grow fast. At the first start (when there is no data in the db) 0 will be used as the $LAST_VALUE . [ -z $LAST_VALUE ] && LAST_VALUE=0 Without this, at the first start the \"if\" statement would run into an error. It logs \"only\" the temperature, humidity and voltage values by subscripting these topics: mosquitto_sub -R -v -h 172.16.0.250 -u v*n*y*a -P ***** -t 'NodeMCU/+/status/temperature' -t 'NodeMCU/+/status/humidity' -t 'NodeMCU/+/status/voltage' This scripts log to the standard output. Of course you can redirect all output to a log file by putting the following line to script (before the mosquitto_sub command). exec >> /path/of/the/log/file 2>&1 How to run in the background? Simply use the well-known method: nohup + command + $ nohup ./script-name-sh &","title":"2.2. Shell Script"},{"location":"old/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Reliable MQTT conenction with NodeMCU \u00b6 TL;DR \u00b6 I want to use ESP modules for my brand new project: **H**ome **A**utomatization **W**ith **O**penHAB (HAWO). :) To realise this I want to connect some ESP module to my OpenHAB server using MQTT broker (mosquitto). Building a complete Smart Home is not my goal (at least not now), I only want to control some lights / stuff with my phone (or tablet) and place some DHT22 (Temperature and Humidity) sensors in my house, workshop and garden. I started working with NodeMCU some months ago and I ran into several problems and bugs during my coding. I cannot write all my experience to this post, but I hope this will be helpful for you. :) Steps to be done before you can connect to mqtt broker: Install and configure mosquitto mqtt broker Connect ESP8266 module to your Wi-fi network Maybe later I will write a posts about these steps, but now I want to give more details only about mqtt connection in this thread. So I need a reliable connection to my mqtt broker which can handle network or other errors. Unfortunately for some reason mqtt module can not re-connect to broker for example if Wi-fi disconnect and reconnect. To better understand here is a basic example to connect to the broker. m = mqtt.Client(\"ClienID\", 60, \"test\", \"test123\") m:connect(\"192.168.10.10\", 1883, 0, function(client) print(\"connected\") end, function(client, reason) print(\"failed reason: \"..reason) end) When you specify mqtt.client:connect() you have on option to turn on/off auto-reconnect. mqtt:connect(host[, port[, secure[, autoreconnect]]][, function(client)[, function(client, reason)]]) What kind of problem can occurr? \u00b6 Wi-fi disconnection MQTT broker become unavailable or is being restarted. Other network issues (eg.: DNS error) Misconfiguration: bad username, password, etc. My biggest problem is that the connection between ESP and the broker cannot be tested in any way. Based on my experiences I ran into these problems: Using autoreconnect true If I restart mosquitto the clients reconnect to it successfully. But if I disconnect and connect to Wi-fi, the clients can not reconnect. This is a big problem because you can not reconnect to the broker. If you try mqtt.client:connect() again ESP will give you \"Already Connected\" error message. If you try firstly mqtt.client:close() the ESP will be restarted. I do not know if this behavior is a bug or a feature but it is really annoying. I tried to check if the connection is established or not with this if condition: if m:publish(config.mqttLwtTopic,\"Active\",0,1) then DO SOMETHING else DO SOMETHING ELSE end But if you use autoreconnect=true it will return with true in any case, regardless whether the message has been delivered or not. Using autoreconnect false It can be a good option but in this case we have to check the connection manually, and reconnect if something happens. My final solution is two functions and a timer combined with each other: local function checkLwt() if m:publish(config.mqttLwtTopic,\"Active\",0,1) then return true else return false end end local function connectReconnect() m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) print(\"MQTT connected to: \"..config.mqttHost) subsribe() onMessage() tmr.start(config.mqttCheckTimerId) end, function(client, reason) print(\"failed reason: \"..reason) print(\"Sleep for 10 secs\") tmr.alarm(6,10000,tmr.ALARM_SINGLE, function() connectReconnect() end) end) end tmr.register(config.mqttCheckTimerId,config.mqttTmrDelay, tmr.ALARM_AUTO, function() local status, err = pcall(checkLwt) if status == true and err == true then print(\"LWT OK\") updateStatus() elseif status == true and err == false then print(\"LWT FAILED\") connectReconnect() end if status == false and err ~= true then print(\"LWT faild with notconnected...\") tmr.stop(config.mqttCheckTimerId) end end) checkLwt() \u00b6 There are three scenarios: Message is successfully delivered. Return true. Message is failed to deliver. Return false. And the last one is the worst. :( If this function is called while the client is trying to connect to the broker it will fail with \"Not Connected\" exception and ESP will be restarted. That's why I use pcall (Protected Call) in the timer. local status, err = pcall(checkLwt) if status == true and err == true ... The status is true when the function returns without exception. It prevents the ESP from restarting if the function throws an exception (Not Connected). Status can be true or false. The err can be true or the error message of the exception. Based on that 3 scenario exists: status == true and err == true The function successfully finished. Everything is OK. status == true and err == false In this situation probably there is something with mqtt server. For example it stopped or is unreachable. status == false and err ~= true Please not that I use \"not equal\" ~= because err can be an error message as well. So in this situation it is likely that we get \"not connected\" exception, and maybe something happened with the Wi-fi connection. bonus: status == false and err == true O.K. This is not possible. If status is false it means that we got an exception thus err could not be true. connectReconnect() & tmr() \u00b6 This is the main part of my code. What will happen when connectReconnect function is called? If the connection to the broker is successful the timer will be started and some other functions will be called. If the client failed to connect to the broker Wait 10 secs and the function will call itself. As you can see this is a recursive function which continuously call itself until the connection is established. The timer check whether the publication is working or not. Why is it necessary to use 2 different error conditions? If the checkLwt fails, we have to know why, because if it fails due to the broker unavailability , callback events of mqtt.client:connect() won't work, neither the success nor the failed function will run. At first status == true and err == false condition will be always true, and connectReconnect() function will be called. From that point there are two error case: There is some network error, for example \"DNS failure!\". In this case the function will call itself. BUT! The timer is still running and will run into the status == false and err ~= true condition (because of \"Not Connected\" exception). We have to stop the timer because it is unnecessary. The recursive function will call itself until the connection is not ready, and if we call mqtt.client:connect() multiple times, we will get \"Already Connected\" exception. The broker is shut down: status == true and err == false As I mentioned in this case the callback event of mqtt.client:connect() won't work that's why we have to call connectReconnect() until the connection is not ready. I've created a table for the better understanding of status and err : Two more very important things: Call connectReconnect() function after the network connection is successfully established. Register the \"offline\" callback event: m:on(\"offline\", function(con) print (\"Offline\") setup.resetRelay() tmr.start(config.mqttCheckTimerId) end) NOTE Always start the connection procedure by starting the timer, not by calling conenctReconnect() function. ( tmr.start(config.mqttCheckTimerId) )!!! Why? Because if you are connected to Wi-fi network and call connectReconnect() function, but MQTT broker is unavailable the timer never will be started, because mqtt.client:connect() will fail and neither the true nor the false function will be called (because of previously mentioned bug). Summary: Maybe this is not the best solution but I failed to find better one. I tried uncountable variations of functions and timers and their combinations. The most important for me that the connection has to be reliable, and in case of any error ESP has to be reconnected to the broker. Maybe when the mentioned bugs will be fixed in the future, it will be enough to use only the reconnect=true option. You can download my full example from this link . References: https://docs.coronalabs.com/api/library/index.html https://nodemcu.readthedocs.io/en/master/ https://www.lua.org/pil/8.5.html UPDATE: Reliable MQTT connection with NodeMCU (part 2)","title":"Reliable MQTT conenction with NodeMCU"},{"location":"old/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#reliable-mqtt-conenction-with-nodemcu","text":"","title":"Reliable MQTT conenction with NodeMCU"},{"location":"old/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#tldr","text":"I want to use ESP modules for my brand new project: **H**ome **A**utomatization **W**ith **O**penHAB (HAWO). :) To realise this I want to connect some ESP module to my OpenHAB server using MQTT broker (mosquitto). Building a complete Smart Home is not my goal (at least not now), I only want to control some lights / stuff with my phone (or tablet) and place some DHT22 (Temperature and Humidity) sensors in my house, workshop and garden. I started working with NodeMCU some months ago and I ran into several problems and bugs during my coding. I cannot write all my experience to this post, but I hope this will be helpful for you. :) Steps to be done before you can connect to mqtt broker: Install and configure mosquitto mqtt broker Connect ESP8266 module to your Wi-fi network Maybe later I will write a posts about these steps, but now I want to give more details only about mqtt connection in this thread. So I need a reliable connection to my mqtt broker which can handle network or other errors. Unfortunately for some reason mqtt module can not re-connect to broker for example if Wi-fi disconnect and reconnect. To better understand here is a basic example to connect to the broker. m = mqtt.Client(\"ClienID\", 60, \"test\", \"test123\") m:connect(\"192.168.10.10\", 1883, 0, function(client) print(\"connected\") end, function(client, reason) print(\"failed reason: \"..reason) end) When you specify mqtt.client:connect() you have on option to turn on/off auto-reconnect. mqtt:connect(host[, port[, secure[, autoreconnect]]][, function(client)[, function(client, reason)]])","title":"TL;DR"},{"location":"old/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#what-kind-of-problem-can-occurr","text":"Wi-fi disconnection MQTT broker become unavailable or is being restarted. Other network issues (eg.: DNS error) Misconfiguration: bad username, password, etc. My biggest problem is that the connection between ESP and the broker cannot be tested in any way. Based on my experiences I ran into these problems: Using autoreconnect true If I restart mosquitto the clients reconnect to it successfully. But if I disconnect and connect to Wi-fi, the clients can not reconnect. This is a big problem because you can not reconnect to the broker. If you try mqtt.client:connect() again ESP will give you \"Already Connected\" error message. If you try firstly mqtt.client:close() the ESP will be restarted. I do not know if this behavior is a bug or a feature but it is really annoying. I tried to check if the connection is established or not with this if condition: if m:publish(config.mqttLwtTopic,\"Active\",0,1) then DO SOMETHING else DO SOMETHING ELSE end But if you use autoreconnect=true it will return with true in any case, regardless whether the message has been delivered or not. Using autoreconnect false It can be a good option but in this case we have to check the connection manually, and reconnect if something happens. My final solution is two functions and a timer combined with each other: local function checkLwt() if m:publish(config.mqttLwtTopic,\"Active\",0,1) then return true else return false end end local function connectReconnect() m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) print(\"MQTT connected to: \"..config.mqttHost) subsribe() onMessage() tmr.start(config.mqttCheckTimerId) end, function(client, reason) print(\"failed reason: \"..reason) print(\"Sleep for 10 secs\") tmr.alarm(6,10000,tmr.ALARM_SINGLE, function() connectReconnect() end) end) end tmr.register(config.mqttCheckTimerId,config.mqttTmrDelay, tmr.ALARM_AUTO, function() local status, err = pcall(checkLwt) if status == true and err == true then print(\"LWT OK\") updateStatus() elseif status == true and err == false then print(\"LWT FAILED\") connectReconnect() end if status == false and err ~= true then print(\"LWT faild with notconnected...\") tmr.stop(config.mqttCheckTimerId) end end)","title":"What kind of problem can occurr?"},{"location":"old/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#checklwt","text":"There are three scenarios: Message is successfully delivered. Return true. Message is failed to deliver. Return false. And the last one is the worst. :( If this function is called while the client is trying to connect to the broker it will fail with \"Not Connected\" exception and ESP will be restarted. That's why I use pcall (Protected Call) in the timer. local status, err = pcall(checkLwt) if status == true and err == true ... The status is true when the function returns without exception. It prevents the ESP from restarting if the function throws an exception (Not Connected). Status can be true or false. The err can be true or the error message of the exception. Based on that 3 scenario exists: status == true and err == true The function successfully finished. Everything is OK. status == true and err == false In this situation probably there is something with mqtt server. For example it stopped or is unreachable. status == false and err ~= true Please not that I use \"not equal\" ~= because err can be an error message as well. So in this situation it is likely that we get \"not connected\" exception, and maybe something happened with the Wi-fi connection. bonus: status == false and err == true O.K. This is not possible. If status is false it means that we got an exception thus err could not be true.","title":"checkLwt()"},{"location":"old/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#connectreconnect-tmr","text":"This is the main part of my code. What will happen when connectReconnect function is called? If the connection to the broker is successful the timer will be started and some other functions will be called. If the client failed to connect to the broker Wait 10 secs and the function will call itself. As you can see this is a recursive function which continuously call itself until the connection is established. The timer check whether the publication is working or not. Why is it necessary to use 2 different error conditions? If the checkLwt fails, we have to know why, because if it fails due to the broker unavailability , callback events of mqtt.client:connect() won't work, neither the success nor the failed function will run. At first status == true and err == false condition will be always true, and connectReconnect() function will be called. From that point there are two error case: There is some network error, for example \"DNS failure!\". In this case the function will call itself. BUT! The timer is still running and will run into the status == false and err ~= true condition (because of \"Not Connected\" exception). We have to stop the timer because it is unnecessary. The recursive function will call itself until the connection is not ready, and if we call mqtt.client:connect() multiple times, we will get \"Already Connected\" exception. The broker is shut down: status == true and err == false As I mentioned in this case the callback event of mqtt.client:connect() won't work that's why we have to call connectReconnect() until the connection is not ready. I've created a table for the better understanding of status and err : Two more very important things: Call connectReconnect() function after the network connection is successfully established. Register the \"offline\" callback event: m:on(\"offline\", function(con) print (\"Offline\") setup.resetRelay() tmr.start(config.mqttCheckTimerId) end) NOTE Always start the connection procedure by starting the timer, not by calling conenctReconnect() function. ( tmr.start(config.mqttCheckTimerId) )!!! Why? Because if you are connected to Wi-fi network and call connectReconnect() function, but MQTT broker is unavailable the timer never will be started, because mqtt.client:connect() will fail and neither the true nor the false function will be called (because of previously mentioned bug). Summary: Maybe this is not the best solution but I failed to find better one. I tried uncountable variations of functions and timers and their combinations. The most important for me that the connection has to be reliable, and in case of any error ESP has to be reconnected to the broker. Maybe when the mentioned bugs will be fixed in the future, it will be enough to use only the reconnect=true option. You can download my full example from this link . References: https://docs.coronalabs.com/api/library/index.html https://nodemcu.readthedocs.io/en/master/ https://www.lua.org/pil/8.5.html UPDATE: Reliable MQTT connection with NodeMCU (part 2)","title":"connectReconnect() &amp; tmr()"},{"location":"old/nodemcu/Reliable_Mqtt_Connection_With_Nodemcu_%28part_2%29/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Reliable MQTT connection with NodeMCU (part 2) \u00b6 Thanks to \"Modestas\" post on my previous article now I will show you another solution to this topic, which is much easier to understand and simpler. First we will create a config file like this: --[[ File Name: config.lua ]] local module = {} module.NodeID=node.chipid() -- mqtt Related Config module.mqttHost=\"172.16.0.***\" module.mqttPort=\"1883\" module.mqttUserName=\"**************\" module.mqttpassword=\"**************\" module.mqttLwtTopic=\"NodeMCU/lwt/\"..module.NodeID module.mqttSubscibeTopics={[\"NodeMCU/\"..module.NodeID..\"/command\"]=0,[\"NodeMCU/\"..module.NodeID..\"/relayCh/+\"]=0} module.mqttPublishTopicStatus=\"NodeMCU/\"..module.NodeID..\"/status/\" module.mqttUpdateStatusInterval=60000 module.mqttUpdateStatusTimerId=2 return module You can use table object to store these parameter/value pairs instead of this module. Maybe later I will write a post about 'how to do that'. Create MQTT client and set LWT m=mqtt.Client(config.NodeID, 10, config.mqttUserName, config.mqttpassword) m:lwt(config.mqttLwtTopic, \"Inactive\", 0,1) And do not forget to set isMqttAlive value to false . We will use this variable to determine if mqtt connection is alive or not. The initial value must be false, because at the first run the ESP isn't connected to the broker. connectToMqtt() Function function connectToMqtt() m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) isMqttAlive = true print(\"Successfully Conencted to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort) -- Here you can do some useful things. Example subscribe to a topic, or set up what should happen if a message is received. (`mqtt.client:on()`) end, function(con,reason) print(\"Faild to connect to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort..\", Reason: \"..reason) isMqttAlive = false end) end This function will be used to connect to the MQTT broker, and the isMqttAlive variable is also will be set here: -If the ESP is successfully connected to the broker, isMqttAlive will be true. -If something goes wrong, this variable will be false. Set up mqtt.client:on(\"offline\"....) m:on(\"offline\", function(con) isMqttAlive = false print(\"Disconnected from MQTT\") end) If the ESP disconnects from the broker this will set isMqttAlive to false. Final steps After you call connectToMqtt() function you have to check the value of isMqttAlive before each message publication. In my case I use a DHT22 sensor to monitor temperature and humidity, and I send update every 10 minutes. To do this a timer should be used. Example: tmr.alarm(config.mqttUpdateStatusTimerId,config.mqttUpdateStatusInterval, tmr.ALARM_AUTO, function() if isMqttAlive == false then print(\"Reconnect To MQTT:\"..config.mqttHost..\" on port: \"..config.mqttPort) connectToMqtt() else print(\"MQTT is OK: \"..config.mqttHost..\" on port: \"..config.mqttPort) mqttUpdateGeneralStatus(m) end end) As you can see, first, I check if isMqttAlive is ture or false. - If it is false I call connectToMqtt() function to (re)connect to the broker. - If it is true the mqttUpdateGeneralStatus(m) function will be called, which queries the sensor and send the actual temperature and humidity values to the broker. A Complete Example local config=require(\"config\") local setup=require(\"setup\") local mqttSP=require(\"mqttSP\") m=mqtt.Client(config.NodeID, 10, config.mqttUserName, config.mqttpassword) m:lwt(config.mqttLwtTopic, \"Inactive\", 0,1) isMqttAlive=false -- Conenct Function function connectToMqtt() m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) isMqttAlive = true print(\"Successfully Conencted to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort) mqttSP.mqttSubsribe(m,config.mqttSubscibeTopics) mqttSP.mqttOnMessage(m) m:publish(config.mqttLwtTopic,\"Active\",0,1) m:on(\"offline\", function(con) isMqttAlive = false print(\"Disconnected from MQTT\") end) end, function(con,reason) print(\"Faild to connect to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort..\", Reason: \"..reason) end) end -- Connect To Broker connectToMqtt() -- Start Timer tmr.alarm(config.mqttUpdateStatusTimerId,config.mqttUpdateStatusInterval, tmr.ALARM_AUTO, function() if isMqttAlive == false then print(\"Reconnect To MQTT:\"..config.mqttHost..\" on port: \"..config.mqttPort) connectToMqtt() else print(\"MQTT is OK: \"..config.mqttHost..\" on port: \"..config.mqttPort) mqttSP.mqttUpdateGeneralStatus(m,setup.getPublicIp()) end end) REFERENCES: https://nodemcu.readthedocs.io/en/master/en/modules/mqtt/","title":"Reliable MQTT connection with NodeMCU (part 2)"},{"location":"old/nodemcu/Reliable_Mqtt_Connection_With_Nodemcu_%28part_2%29/#reliable-mqtt-connection-with-nodemcu-part-2","text":"Thanks to \"Modestas\" post on my previous article now I will show you another solution to this topic, which is much easier to understand and simpler. First we will create a config file like this: --[[ File Name: config.lua ]] local module = {} module.NodeID=node.chipid() -- mqtt Related Config module.mqttHost=\"172.16.0.***\" module.mqttPort=\"1883\" module.mqttUserName=\"**************\" module.mqttpassword=\"**************\" module.mqttLwtTopic=\"NodeMCU/lwt/\"..module.NodeID module.mqttSubscibeTopics={[\"NodeMCU/\"..module.NodeID..\"/command\"]=0,[\"NodeMCU/\"..module.NodeID..\"/relayCh/+\"]=0} module.mqttPublishTopicStatus=\"NodeMCU/\"..module.NodeID..\"/status/\" module.mqttUpdateStatusInterval=60000 module.mqttUpdateStatusTimerId=2 return module You can use table object to store these parameter/value pairs instead of this module. Maybe later I will write a post about 'how to do that'. Create MQTT client and set LWT m=mqtt.Client(config.NodeID, 10, config.mqttUserName, config.mqttpassword) m:lwt(config.mqttLwtTopic, \"Inactive\", 0,1) And do not forget to set isMqttAlive value to false . We will use this variable to determine if mqtt connection is alive or not. The initial value must be false, because at the first run the ESP isn't connected to the broker. connectToMqtt() Function function connectToMqtt() m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) isMqttAlive = true print(\"Successfully Conencted to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort) -- Here you can do some useful things. Example subscribe to a topic, or set up what should happen if a message is received. (`mqtt.client:on()`) end, function(con,reason) print(\"Faild to connect to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort..\", Reason: \"..reason) isMqttAlive = false end) end This function will be used to connect to the MQTT broker, and the isMqttAlive variable is also will be set here: -If the ESP is successfully connected to the broker, isMqttAlive will be true. -If something goes wrong, this variable will be false. Set up mqtt.client:on(\"offline\"....) m:on(\"offline\", function(con) isMqttAlive = false print(\"Disconnected from MQTT\") end) If the ESP disconnects from the broker this will set isMqttAlive to false. Final steps After you call connectToMqtt() function you have to check the value of isMqttAlive before each message publication. In my case I use a DHT22 sensor to monitor temperature and humidity, and I send update every 10 minutes. To do this a timer should be used. Example: tmr.alarm(config.mqttUpdateStatusTimerId,config.mqttUpdateStatusInterval, tmr.ALARM_AUTO, function() if isMqttAlive == false then print(\"Reconnect To MQTT:\"..config.mqttHost..\" on port: \"..config.mqttPort) connectToMqtt() else print(\"MQTT is OK: \"..config.mqttHost..\" on port: \"..config.mqttPort) mqttUpdateGeneralStatus(m) end end) As you can see, first, I check if isMqttAlive is ture or false. - If it is false I call connectToMqtt() function to (re)connect to the broker. - If it is true the mqttUpdateGeneralStatus(m) function will be called, which queries the sensor and send the actual temperature and humidity values to the broker. A Complete Example local config=require(\"config\") local setup=require(\"setup\") local mqttSP=require(\"mqttSP\") m=mqtt.Client(config.NodeID, 10, config.mqttUserName, config.mqttpassword) m:lwt(config.mqttLwtTopic, \"Inactive\", 0,1) isMqttAlive=false -- Conenct Function function connectToMqtt() m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) isMqttAlive = true print(\"Successfully Conencted to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort) mqttSP.mqttSubsribe(m,config.mqttSubscibeTopics) mqttSP.mqttOnMessage(m) m:publish(config.mqttLwtTopic,\"Active\",0,1) m:on(\"offline\", function(con) isMqttAlive = false print(\"Disconnected from MQTT\") end) end, function(con,reason) print(\"Faild to connect to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort..\", Reason: \"..reason) end) end -- Connect To Broker connectToMqtt() -- Start Timer tmr.alarm(config.mqttUpdateStatusTimerId,config.mqttUpdateStatusInterval, tmr.ALARM_AUTO, function() if isMqttAlive == false then print(\"Reconnect To MQTT:\"..config.mqttHost..\" on port: \"..config.mqttPort) connectToMqtt() else print(\"MQTT is OK: \"..config.mqttHost..\" on port: \"..config.mqttPort) mqttSP.mqttUpdateGeneralStatus(m,setup.getPublicIp()) end end) REFERENCES: https://nodemcu.readthedocs.io/en/master/en/modules/mqtt/","title":"Reliable MQTT connection with NodeMCU (part 2)"},{"location":"old/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/","text":"Very Simple Way to Send Email Using NodeMCU firmware \u00b6 Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! SMTP Server \u00b6 Maybe some of you have already thought about sending email from NodeMCU powered module (ESP8266 ESP-01, ESP-07, etc). Maybe some of you have successfully written a code to send email from this little board. Now I want to show you a very simple way to implement email sending. I think there are a lot of way to do it, but instead of writing a code to send email directly using an smtp server I will use third party tool for it. The name of the service is MailGun . I have read an article which described a method using Mailgun for sending email. When I installed this Ghost Blog Engine, I got a warning message which showed that email service had not been configured properly, and gave me a link . In this article you can read about ghost email configuration, and it has a part about Mailgun configuration. Now I'm using this solution in my Ghost instance. But it is actually not important. While I was configuring Mailgun, I found an example api call using curl to send email via Mailgun. Link. The API example: curl -s --user 'api:YOUR_API_KEY' \\ https://api.mailgun.net/v3/YOUR_DOMAIN_NAME/messages \\ -F from='Excited User <mailgun@YOUR_DOMAIN_NAME>' \\ -F to=YOU@YOUR_DOMAIN_NAME \\ -F to=bar@example.com \\ -F subject='Hello' \\ -F text='Testing some Mailgun awesomness!' Some days later I was thinking. This curl example is a very simple and NodeMCU firmware is able to send HTTP POST messages. OK, maybe at the first sight it is not clear, but this curl example is actually a POST message. To come to the NodeMCU firmware, you have to build your firmware with HTTP module . To see what kind of messages are sent over the network after this curl example I created a tcpdump. It is important to send this request without SSL. Because with SSL connection you won't see anything in the dump. Example: curl -s --user 'api:key-2bdec103ac5dea85b9378ab2541faecf' \\ http://blog.vinczejanos.info/v3/blog.vinczejanos.info/messages \\ -F from='blog@blog.vinczejanos.info' \\ -F to=janos.vincze@vodafone.com \\ -F subject='Hello' \\ -F text='Testing some Mailgun awesomness!' This request returns with HTTP 404, but we are interested in only the request, not the response. Command: sudo tcpdump -s0 host api.mailgun.net -w pcap.pcap This will create a pcap.pcap file, which can be opened in Wireshark . We need the POST message to follow: As I have written above we need only the request: POST /v3/blog.vinczejanos.info/messages HTTP/1.1 Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== User-Agent: curl/7.38.0 Host: blog.vinczejanos.info Accept: */* Content-Length: 507 Expect: 100-continue Content-Type: multipart/form-data; boundary=------------------------40dcfa2d67b56270 HTTP/1.1 100 Continue --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"from\" blog@blog.vinczejanos.info --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"to\" janos.vincze@vodafone.com --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"subject\" Hello --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"text\" Testing some Mailgun awesomness! --------------------------40dcfa2d67b56270-- Ok. The first thing to do is figure out what is the Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== line. This very simple because HTTP Basic auth is base64 encoded so we can decrypt it by using tihs command: echo -n \"YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\" | base64 -d The result: api:key-2bdec103ac5dea85b9378ab2541faecf So the basic auth HTTP header contains your MailGun API key. This step is not necessary, I was only curious. If you do not want to create tcpdump to find out your basic auth key, simply use base64 command: echo -n \"api:key-2bdec103ac5dea85b9378ab2541faecf\" | base64 YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== After that we have the first line of the header: Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== Based on the tcpdump we can assemble the HTTP header. You can see that MailGun API is using multipart/form-data MIME format, thus we have to use uniq boundary. If you want to know more about boundary or form-data MIME format please read the rfc2388 documentation. I will use dynamic generated boundary to avoid occurrence of it in the other part of the message. I will use table.insert and table.concat to concatenate strings, because this method consume less memory then the simple .. (two dots). So insted of this: variable=\"string\"..var..\"another string\" I will use: test_table={} table.insert(test_table, \"string\") table.insert(test_table, var) table.insert(test_table, \"another string\") string_result=table.concat(test_table) test_table=nil OK. Maybe it is a bit longer but it's worth it. Please remember to nil the table after concat. 1. Generate Boundary \u00b6 boundary_table={} for i=1,15 do table.insert(boundary_table, string.char(math.random(65, 90))) -- A-Z table.insert(boundary_table, string.char(math.random(48, 57))) -- 0-9 table.insert(boundary_table, string.char(math.random(97, 122))) -- a-z end boundary=table.concat(boundary_table) boundary_table=nil This will create something like this: B2bO4oH6gL2iZ7oK6jG2zA6vK3zO1wI9dG1gP0wV0tE3p OK. I know this is a bit stupid way to generate random string, but working and just enough for us. Assemble the Header Part \u00b6 header_table={} table.insert(header_table, 'Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\\r\\n') table.insert(header_table, 'Host: api.mailgun.net\\r\\n') table.insert(header_table, 'User-Agent: NodeMCU/testAg\\r\\n') table.insert(header_table, 'Content-Type: multipart/form-data; boundary='..boundary..'\\r\\n') header=table.concat(header_table) As you can see I inserted only the minimal necessary lines to the header. Now we have the header variable with these lines: Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== Host: api.mailgun.net User-Agent: NodeMCU/testAg Content-Type: multipart/form-data; boundary=V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Assemble The \"Body\" Part (POST message) \u00b6 data_table={} table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"from\"\\r\\n\\r\\n') table.insert(data_table, 'admin@blog.vinczejanos.info\\r\\n') table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"to\"\\r\\n\\r\\n') table.insert(data_table, 'jvincze84@gmail.com\\r\\n') table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"subject\"\\r\\n\\r\\n') table.insert(data_table, 'Hello\\r\\n') table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"text\"\\r\\n\\r\\n') table.insert(data_table, 'Congratulations Vincze Janos, you just sent an email with Mailgun! You are truly awesome!\\r\\n\\r\\n') table.insert(data_table, '--'..boundary..'--\\r\\n') data=table.concat(data_table) data_table=nil data values contains the following: --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"from\" admin@blog.vinczejanos.info --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"to\" jvincze84@gmail.com --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"subject\" Hello --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"text\" Congratulations Vincze Janos, you just sent an email with Mailgun! You are truly awesome! --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u-- There are some very important things: In the header part use your \"raw\" boundary value: Content-Type: multipart/form-data; boundary=V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u In the body part You have to place two - sign before each boundaries: --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"from\" BUT In case of the last boundary you have to add two - to the beginning and to the end of the boundary.: --**V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u --** Send An E-Mail :) \u00b6 Finally we can now send the email. http.post('https://api.mailgun.net/v3/blog.vinczejanos.info/messages', header, data, function(code, data) if (code < 0) then print(\"HTTP request failed\") else print(code, data) end end) This message should be returned HTTP/200 OK after http.post . > http.post('https://api.mailgun.net/v3/blog.vinczejanos.info/messages', header, post_data, >> function(code, data) >> if (code < 0) then >> print(\"HTTP request failed\") >> else >> print(code, data) >> end >> end) > 200 { \"id\": \"<20160826110622.24301.92312.4561825D@blog.vinczejanos.info>\", \"message\": \"Queued. Thank you.\" } And my message is delivered to my mailbox. :) If you don't like this method, you can find many other ways to implement email sending, or you can write a code on your own. For example on github there is implementation which uses smtp communication with NodeMCU net Module . Honestly I don't like sending email directly from the ESP9266 modules because it has very limited resources, but there are some cases when you can implement this code. For example if your code has little footprint. So If you write a rather \"big\" and complex code it is possible that not enough memory will be left to assemble the header and body part and call http.post(). Rather than sending email directly from esp8266 I advise to use NodeRED .","title":"Very Simple Way to Send Email Using NodeMCU firmware"},{"location":"old/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#very-simple-way-to-send-email-using-nodemcu-firmware","text":"Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!","title":"Very Simple Way to Send Email Using NodeMCU firmware"},{"location":"old/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#smtp-server","text":"Maybe some of you have already thought about sending email from NodeMCU powered module (ESP8266 ESP-01, ESP-07, etc). Maybe some of you have successfully written a code to send email from this little board. Now I want to show you a very simple way to implement email sending. I think there are a lot of way to do it, but instead of writing a code to send email directly using an smtp server I will use third party tool for it. The name of the service is MailGun . I have read an article which described a method using Mailgun for sending email. When I installed this Ghost Blog Engine, I got a warning message which showed that email service had not been configured properly, and gave me a link . In this article you can read about ghost email configuration, and it has a part about Mailgun configuration. Now I'm using this solution in my Ghost instance. But it is actually not important. While I was configuring Mailgun, I found an example api call using curl to send email via Mailgun. Link. The API example: curl -s --user 'api:YOUR_API_KEY' \\ https://api.mailgun.net/v3/YOUR_DOMAIN_NAME/messages \\ -F from='Excited User <mailgun@YOUR_DOMAIN_NAME>' \\ -F to=YOU@YOUR_DOMAIN_NAME \\ -F to=bar@example.com \\ -F subject='Hello' \\ -F text='Testing some Mailgun awesomness!' Some days later I was thinking. This curl example is a very simple and NodeMCU firmware is able to send HTTP POST messages. OK, maybe at the first sight it is not clear, but this curl example is actually a POST message. To come to the NodeMCU firmware, you have to build your firmware with HTTP module . To see what kind of messages are sent over the network after this curl example I created a tcpdump. It is important to send this request without SSL. Because with SSL connection you won't see anything in the dump. Example: curl -s --user 'api:key-2bdec103ac5dea85b9378ab2541faecf' \\ http://blog.vinczejanos.info/v3/blog.vinczejanos.info/messages \\ -F from='blog@blog.vinczejanos.info' \\ -F to=janos.vincze@vodafone.com \\ -F subject='Hello' \\ -F text='Testing some Mailgun awesomness!' This request returns with HTTP 404, but we are interested in only the request, not the response. Command: sudo tcpdump -s0 host api.mailgun.net -w pcap.pcap This will create a pcap.pcap file, which can be opened in Wireshark . We need the POST message to follow: As I have written above we need only the request: POST /v3/blog.vinczejanos.info/messages HTTP/1.1 Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== User-Agent: curl/7.38.0 Host: blog.vinczejanos.info Accept: */* Content-Length: 507 Expect: 100-continue Content-Type: multipart/form-data; boundary=------------------------40dcfa2d67b56270 HTTP/1.1 100 Continue --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"from\" blog@blog.vinczejanos.info --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"to\" janos.vincze@vodafone.com --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"subject\" Hello --------------------------40dcfa2d67b56270 Content-Disposition: form-data; name=\"text\" Testing some Mailgun awesomness! --------------------------40dcfa2d67b56270-- Ok. The first thing to do is figure out what is the Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== line. This very simple because HTTP Basic auth is base64 encoded so we can decrypt it by using tihs command: echo -n \"YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\" | base64 -d The result: api:key-2bdec103ac5dea85b9378ab2541faecf So the basic auth HTTP header contains your MailGun API key. This step is not necessary, I was only curious. If you do not want to create tcpdump to find out your basic auth key, simply use base64 command: echo -n \"api:key-2bdec103ac5dea85b9378ab2541faecf\" | base64 YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== After that we have the first line of the header: Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== Based on the tcpdump we can assemble the HTTP header. You can see that MailGun API is using multipart/form-data MIME format, thus we have to use uniq boundary. If you want to know more about boundary or form-data MIME format please read the rfc2388 documentation. I will use dynamic generated boundary to avoid occurrence of it in the other part of the message. I will use table.insert and table.concat to concatenate strings, because this method consume less memory then the simple .. (two dots). So insted of this: variable=\"string\"..var..\"another string\" I will use: test_table={} table.insert(test_table, \"string\") table.insert(test_table, var) table.insert(test_table, \"another string\") string_result=table.concat(test_table) test_table=nil OK. Maybe it is a bit longer but it's worth it. Please remember to nil the table after concat.","title":"SMTP Server"},{"location":"old/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#assemble-the-header-part","text":"header_table={} table.insert(header_table, 'Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\\r\\n') table.insert(header_table, 'Host: api.mailgun.net\\r\\n') table.insert(header_table, 'User-Agent: NodeMCU/testAg\\r\\n') table.insert(header_table, 'Content-Type: multipart/form-data; boundary='..boundary..'\\r\\n') header=table.concat(header_table) As you can see I inserted only the minimal necessary lines to the header. Now we have the header variable with these lines: Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg== Host: api.mailgun.net User-Agent: NodeMCU/testAg Content-Type: multipart/form-data; boundary=V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u","title":"Assemble the Header Part"},{"location":"old/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#assemble-the-body-part-post-message","text":"data_table={} table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"from\"\\r\\n\\r\\n') table.insert(data_table, 'admin@blog.vinczejanos.info\\r\\n') table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"to\"\\r\\n\\r\\n') table.insert(data_table, 'jvincze84@gmail.com\\r\\n') table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"subject\"\\r\\n\\r\\n') table.insert(data_table, 'Hello\\r\\n') table.insert(data_table, '--'..boundary..'\\r\\n') table.insert(data_table, 'Content-Disposition: form-data; name=\"text\"\\r\\n\\r\\n') table.insert(data_table, 'Congratulations Vincze Janos, you just sent an email with Mailgun! You are truly awesome!\\r\\n\\r\\n') table.insert(data_table, '--'..boundary..'--\\r\\n') data=table.concat(data_table) data_table=nil data values contains the following: --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"from\" admin@blog.vinczejanos.info --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"to\" jvincze84@gmail.com --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"subject\" Hello --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"text\" Congratulations Vincze Janos, you just sent an email with Mailgun! You are truly awesome! --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u-- There are some very important things: In the header part use your \"raw\" boundary value: Content-Type: multipart/form-data; boundary=V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u In the body part You have to place two - sign before each boundaries: --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"from\" BUT In case of the last boundary you have to add two - to the beginning and to the end of the boundary.: --**V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u --**","title":"Assemble The \"Body\" Part (POST message)"},{"location":"old/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#send-an-e-mail","text":"Finally we can now send the email. http.post('https://api.mailgun.net/v3/blog.vinczejanos.info/messages', header, data, function(code, data) if (code < 0) then print(\"HTTP request failed\") else print(code, data) end end) This message should be returned HTTP/200 OK after http.post . > http.post('https://api.mailgun.net/v3/blog.vinczejanos.info/messages', header, post_data, >> function(code, data) >> if (code < 0) then >> print(\"HTTP request failed\") >> else >> print(code, data) >> end >> end) > 200 { \"id\": \"<20160826110622.24301.92312.4561825D@blog.vinczejanos.info>\", \"message\": \"Queued. Thank you.\" } And my message is delivered to my mailbox. :) If you don't like this method, you can find many other ways to implement email sending, or you can write a code on your own. For example on github there is implementation which uses smtp communication with NodeMCU net Module . Honestly I don't like sending email directly from the ESP9266 modules because it has very limited resources, but there are some cases when you can implement this code. For example if your code has little footprint. So If you write a rather \"big\" and complex code it is possible that not enough memory will be left to assemble the header and body part and call http.post(). Rather than sending email directly from esp8266 I advise to use NodeRED .","title":"Send An E-Mail :)"},{"location":"old/raspberry/Compile_Go_Language_On_Raspberry_Pi/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Compile GO language on Raspberry PI \u00b6 There are many ways to install GO language, eg. you can compile from source or use the pre-compiled binaries. Here I want to show you how you can compile GO from source. 1. PreRequirements: \u00b6 Raspberry PI 2 or 3 Raspbian I used this version: Linux raspberrypi 4.4.13-v7+ #894 SMP Mon Jun 13 13:13:27 BST 2016 armv7l GNU/Linux 2016-05-27-raspbian-jessie.zip can be downloaded from the official RPI page . gcc And you check these links: System Requirements And Install Installing Go from source Official GO git repo GitHub repo 2. Install from source using git \u00b6 In order to install the latest go you have to install 1.4.x version first. I followed this useful link. create a dedicated user: gcp root@raspberrypi:~# useradd gcp I set the home dir to /opt/google-cloud-print cat /etc/passwd|grep gcp gcp:x:1002:1002::/opt/google-cloud-print:/bin/bash Clone the repository from GIT: git clone https://github.com/golang/go Rename and copy \u00e2\u20ac\u0153go\u00e2\u20ac directory for different versions: mv go go1.4.3 cp -r go1.4.3 go1.5.2 cp -r go1.4.3 go1.6.3 Checkout the versions cd go1.4.3/ git checkout go1.4.3 cd .. cd go1.5.2/ git checkout go1.5.2 cd .. cd go1.6.3/ git checkout go1.6.3 As you can see I will compile 3 different versions: 1.4.3, 1.5.2 and the latest one: 1.6.3. First of all we will compile 1.4.3: cd /opt/google-cloud-print/git/go1.4.3/src ./all.bash When the compilation is successfully finished, you have to set some system environments: export GOROOT_BOOTSTRAP=/opt/google-cloud-print/git/go1.4.3 export GOROOT=\"/opt/google-cloud-print/git/go1.4.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" Next compile the newer versions. I move forward with 1.6.x: cd /opt/google-cloud-print/git/go1.6.3/src ./all.bash Note You can run into \"not enough\" memory issue while package is being tested, but despite the errors go will work properly. If everything was fine you use different versions of GO, by exporting system environments. I created 3 files to manage different versions: setGoEnv1.4.3.sh #!/bin/bash export GOROOT=\"/opt/google-cloud-print/git/go1.4.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" setGoEnv1.5.2.sh #!/bin/bash export GOROOT=\"/opt/google-cloud-print/git/go1.5.2\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" setGoEnv1.6.3.sh #!/bin/bash export GOROOT=\"/opt/google-cloud-print/git/go1.6.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" The final step is to check if GO working correctly or not. Directory Structure: gcp@raspberrypi:~/git$ pwd /opt/google-cloud-print/git gcp@raspberrypi:~/git$ ls -al total 32 drwxr-xr-x 5 gcp gcp 4096 Aug 11 14:35 . drwxr-xr-x 4 gcp gcp 4096 Aug 11 11:55 .. drwxr-xr-x 12 gcp gcp 4096 Aug 11 10:49 go1.4.3 drwxr-xr-x 11 gcp gcp 4096 Aug 11 11:17 go1.5.2 drwxr-xr-x 11 gcp gcp 4096 Aug 11 13:57 go1.6.3 -rwxr-xr-x 1 gcp gcp 158 Aug 11 14:34 setGoEnv1.4.3.sh -rwxr-xr-x 1 gcp gcp 158 Aug 11 14:35 setGoEnv1.5.2.sh -rwxr-xr-x 1 gcp gcp 158 Aug 11 14:35 setGoEnv1.6.3.sh Check versions: gcp@raspberrypi:~/git$ . setGoEnv1.5.2.sh gcp@raspberrypi:~/git$ go version go version go1.5.2 linux/arm gcp@raspberrypi:~/git$ source setGoEnv1.4.3.sh gcp@raspberrypi:~/git$ go version go version go1.4.3 linux/arm Note Please keep in mind that you have to log out from the current shell after using one of the installed go, because the scripts I\u00e2\u20ac\u2122m using to set the environment which will concatenate the PATH after each run. For example you set the sysenv to use 1.4.3, the script will add /opt/google-cloud-print/git/go1.4.3/bin/ directory to the PATH env. Or you have to remove the current go path from system PATH before sourceing another environment. 3. Install from downloaded source \u00b6 You can download source code form the offical web page: https://golang.org/dl/ Note As you can see there are precompiled binary file for ARM CPUs. But only for ARMv6! Be sure which type of CPU you are using before downloading GO. (Raspberry 2 has ARMv7 CPU!) The install steps are the same as in case of using GIT. Just for fun I will show you how easy to use these official source codes. Download the source code of version 1.4.3. gcp@raspberrypi:~/sources$ wget https://storage.googleapis.com/golang/go1.4.3.src.tar.gz --2016-08-12 16:19:08-- https://storage.googleapis.com/golang/go1.4.3.src.tar.gz Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.208.48, 2a00:1450:4001:817::2010 Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.208.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10875170 (10M) [application/octet-stream] Saving to: \u00e2\u20ac\u02dcgo1.4.3.src.tar.gz.1\u00e2\u20ac\u2122 go1.4.3.src.tar.gz.1 100%[=======================================================================================================>] 10.37M 5.26MB/s in 2.0s 2016-08-12 16:19:10 (5.26 MB/s) - \u00e2\u20ac\u02dcgo1.4.3.src.tar.gz.1\u00e2\u20ac\u2122 saved [10875170/10875170] Extract gcp@raspberrypi:~/sources$ tar xf go1.4.3.src.tar.gz gcp@raspberrypi:~/sources$ mv go go1.4.3 gcp@raspberrypi:~/sources$ Compile gcp@raspberrypi:~/sources/go1.4.3/src$ ./all.bash After the compilation is done you can compile later versions of go . Don't forget to set system environments before compiling.","title":"Compile GO language on Raspberry PI"},{"location":"old/raspberry/Compile_Go_Language_On_Raspberry_Pi/#compile-go-language-on-raspberry-pi","text":"There are many ways to install GO language, eg. you can compile from source or use the pre-compiled binaries. Here I want to show you how you can compile GO from source.","title":"Compile GO language on Raspberry PI"},{"location":"old/raspberry/Compile_Go_Language_On_Raspberry_Pi/#1-prerequirements","text":"Raspberry PI 2 or 3 Raspbian I used this version: Linux raspberrypi 4.4.13-v7+ #894 SMP Mon Jun 13 13:13:27 BST 2016 armv7l GNU/Linux 2016-05-27-raspbian-jessie.zip can be downloaded from the official RPI page . gcc And you check these links: System Requirements And Install Installing Go from source Official GO git repo GitHub repo","title":"1. PreRequirements:"},{"location":"old/raspberry/Compile_Go_Language_On_Raspberry_Pi/#2-install-from-source-using-git","text":"In order to install the latest go you have to install 1.4.x version first. I followed this useful link. create a dedicated user: gcp root@raspberrypi:~# useradd gcp I set the home dir to /opt/google-cloud-print cat /etc/passwd|grep gcp gcp:x:1002:1002::/opt/google-cloud-print:/bin/bash Clone the repository from GIT: git clone https://github.com/golang/go Rename and copy \u00e2\u20ac\u0153go\u00e2\u20ac directory for different versions: mv go go1.4.3 cp -r go1.4.3 go1.5.2 cp -r go1.4.3 go1.6.3 Checkout the versions cd go1.4.3/ git checkout go1.4.3 cd .. cd go1.5.2/ git checkout go1.5.2 cd .. cd go1.6.3/ git checkout go1.6.3 As you can see I will compile 3 different versions: 1.4.3, 1.5.2 and the latest one: 1.6.3. First of all we will compile 1.4.3: cd /opt/google-cloud-print/git/go1.4.3/src ./all.bash When the compilation is successfully finished, you have to set some system environments: export GOROOT_BOOTSTRAP=/opt/google-cloud-print/git/go1.4.3 export GOROOT=\"/opt/google-cloud-print/git/go1.4.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" Next compile the newer versions. I move forward with 1.6.x: cd /opt/google-cloud-print/git/go1.6.3/src ./all.bash Note You can run into \"not enough\" memory issue while package is being tested, but despite the errors go will work properly. If everything was fine you use different versions of GO, by exporting system environments. I created 3 files to manage different versions: setGoEnv1.4.3.sh #!/bin/bash export GOROOT=\"/opt/google-cloud-print/git/go1.4.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" setGoEnv1.5.2.sh #!/bin/bash export GOROOT=\"/opt/google-cloud-print/git/go1.5.2\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" setGoEnv1.6.3.sh #!/bin/bash export GOROOT=\"/opt/google-cloud-print/git/go1.6.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" The final step is to check if GO working correctly or not. Directory Structure: gcp@raspberrypi:~/git$ pwd /opt/google-cloud-print/git gcp@raspberrypi:~/git$ ls -al total 32 drwxr-xr-x 5 gcp gcp 4096 Aug 11 14:35 . drwxr-xr-x 4 gcp gcp 4096 Aug 11 11:55 .. drwxr-xr-x 12 gcp gcp 4096 Aug 11 10:49 go1.4.3 drwxr-xr-x 11 gcp gcp 4096 Aug 11 11:17 go1.5.2 drwxr-xr-x 11 gcp gcp 4096 Aug 11 13:57 go1.6.3 -rwxr-xr-x 1 gcp gcp 158 Aug 11 14:34 setGoEnv1.4.3.sh -rwxr-xr-x 1 gcp gcp 158 Aug 11 14:35 setGoEnv1.5.2.sh -rwxr-xr-x 1 gcp gcp 158 Aug 11 14:35 setGoEnv1.6.3.sh Check versions: gcp@raspberrypi:~/git$ . setGoEnv1.5.2.sh gcp@raspberrypi:~/git$ go version go version go1.5.2 linux/arm gcp@raspberrypi:~/git$ source setGoEnv1.4.3.sh gcp@raspberrypi:~/git$ go version go version go1.4.3 linux/arm Note Please keep in mind that you have to log out from the current shell after using one of the installed go, because the scripts I\u00e2\u20ac\u2122m using to set the environment which will concatenate the PATH after each run. For example you set the sysenv to use 1.4.3, the script will add /opt/google-cloud-print/git/go1.4.3/bin/ directory to the PATH env. Or you have to remove the current go path from system PATH before sourceing another environment.","title":"2. Install from source using git"},{"location":"old/raspberry/Compile_Go_Language_On_Raspberry_Pi/#3-install-from-downloaded-source","text":"You can download source code form the offical web page: https://golang.org/dl/ Note As you can see there are precompiled binary file for ARM CPUs. But only for ARMv6! Be sure which type of CPU you are using before downloading GO. (Raspberry 2 has ARMv7 CPU!) The install steps are the same as in case of using GIT. Just for fun I will show you how easy to use these official source codes. Download the source code of version 1.4.3. gcp@raspberrypi:~/sources$ wget https://storage.googleapis.com/golang/go1.4.3.src.tar.gz --2016-08-12 16:19:08-- https://storage.googleapis.com/golang/go1.4.3.src.tar.gz Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.208.48, 2a00:1450:4001:817::2010 Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.208.48|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10875170 (10M) [application/octet-stream] Saving to: \u00e2\u20ac\u02dcgo1.4.3.src.tar.gz.1\u00e2\u20ac\u2122 go1.4.3.src.tar.gz.1 100%[=======================================================================================================>] 10.37M 5.26MB/s in 2.0s 2016-08-12 16:19:10 (5.26 MB/s) - \u00e2\u20ac\u02dcgo1.4.3.src.tar.gz.1\u00e2\u20ac\u2122 saved [10875170/10875170] Extract gcp@raspberrypi:~/sources$ tar xf go1.4.3.src.tar.gz gcp@raspberrypi:~/sources$ mv go go1.4.3 gcp@raspberrypi:~/sources$ Compile gcp@raspberrypi:~/sources/go1.4.3/src$ ./all.bash After the compilation is done you can compile later versions of go . Don't forget to set system environments before compiling.","title":"3. Install from downloaded source"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Google Cloud Print With Orange PI (or RPI) \u00b6 -1. Update your system & Install Requirements \u00b6 apt-get update apt-ge upgrade apt-get install gcc make # If they aren't installed (for golang) apt-get install build-essential libcups2-dev libavahi-client-dev git bzr # for cloud connector 0. Install Go Language \u00b6 Install go lang. You can follow \"Compile GO language on Raspberry PI\" to complete this step. I don't want to write detailed install steps here, instead of I paste only the commands to this chapter without any / detailed comments. (Maybe it will helpful for someone). 1. Clone Go \u00b6 mkdir /opt/go/ cd /opt/go/ git clone https://github.com/golang/go` 2. Checkout Go \u00b6 cd /opt/go/go git tag -l cd .. mv go go1.4.3 cp -r go1.4.3 go1.6.3 cd go1.4.3/ git checkout go1.4.3 cd /opt/go/go1.6.3 git checkout go1.6.3 3. Compile GO 1.4.3 \u00b6 export GOARCH=\"arm\" export GOOS=\"linux\" export GOARM=\"7\" cd /opt/go/go1.4.3/src /opt/go/go1.4.3/src ./all.bash | tee -a output.log Sample Output: Link 4. Set System environments \u00b6 export GOROOT_BOOTSTRAP=\"/opt/go/go1.4.3\" export GOROOT=\"/opt/go/go1.4.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" export GOARCH=\"arm\" export GOOS=\"linux\" export GOARM=\"7\" 5. Some Performance tuning \u00b6 fallocate --length 2GiB /root/2G.swap chmod 0600 /root/2G.swap mkswap /root/2G.swap swapon /root/2G.swap ulimit -s 65536 6. Compile GO 1.6.3 \u00b6 cd /opt/go/go1.6.3/src ./all.bash | tee -a output.log Sample output: Link 7. Set System Environments to use go1.6.3. \u00b6 You should unset all GO related ENV, or logout from current shell and create new shell. export GOROOT_BOOTSTRAP=\"/opt/go/go1.4.3\" export GOROOT=\"/opt/go/go1.6.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" export GOARCH=\"arm\" export GOOS=\"linux\" export GOARM=\"7\" Check: go version go version go1.6.3 linux/arm Your GO is ready to use. :) Install Google Cloud Print connector \u00b6 Simply run the following command: go get github.com/google/cloud-print-connector/... Go will download and install packages and dependencies. Q: What means ... ? A: if you want all packages in that repository, use ... Reference: https://github.com/google/cloud-print-connector/wiki/Build-from-source Run automatically on system boot (systemd): https://github.com/google/cloud-print-connector/wiki/Run-Connector-Automatically-on-Boot This will create two new files to /opt/go/go1.6.3/src/bin directory: root@OrangePI:/opt/go/go1.6.3/src/bin# ls -al total 17504 drwxr-xr-x 2 root root 4096 Sep 6 13:11 . drwxr-xr-x 47 root root 4096 Sep 6 13:11 .. -rwxr-xr-x 1 root root 8358992 Sep 6 13:11 gcp-connector-util -rwxr-xr-x 1 root root 9554384 Sep 6 13:11 gcp-cups-connector And the source is available int /opt/go/go1.6.3/src/src/github.com/google/cloud-print-connector Configure Google Cloud Print connector \u00b6 To configure Google Cloud Print (GCP) you simply run gcp-connector-util from /opt/go/go1.6.3/src/bin directory, answer some question. root@OrangePI:/opt/go/go1.6.3/src/bin# ./gcp-connector-util init \"Local printing\" means that clients print directly to the connector via local subnet, and that an Internet connection is neither necessary nor used. Enable local printing? yes \"Cloud printing\" means that clients can print from anywhere on the Internet, and that printers must be explicitly shared with users. Enable cloud printing? yes Retain the user OAuth token to enable automatic sharing? no Proxy name for this connector: opi_sam_ml1510 Visit https://www.google.com/device, and enter this code. I'll wait for you. GGVY-**** At this point you have to login to your google account, visit https://www.google.com/device and give your code. After you enter your code you will get the following message on linux box: The config file /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json is ready to rock. Keep it somewhere safe, as it contains an OAuth refresh token. The next step is checking the configuration: Start GCP connector: ./gcp-cups-connector --config-filename gcp-cups-connector.config.json --log-to-console Check your printer by visiting https://www.google.com/cloudprint/#printers website.If everything went fine you should see your printer. The last step is change log file location by editing gcp-cups-connector.config.json , and modify log_file_name entry according to where you want to save logs. Run in the background: nohup ./gcp-cups-connector --config-filename gcp-cups-connector.config.json & Run Automatically On boot \u00b6 If you want to start GCP at system boot follow these steps. Reference: https://github.com/google/cloud-print-connector/wiki/Run-Connector-Automatically-on-Boot You can find a sample config file in /opt/go/go1.6.3/src/src/github.com/google/cloud-print-connector/systemd directory. Make a backup: cp cloud-print-connector.service cloud-print-connector.service-orig Edit this file. Modify ExecStart and User property to this: ExecStart=/opt/go/go1.6.3/src/bin/gcp-cups-connector -config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json User=root Install systemd service: install -o root -m 0660 cloud-print-connector.service /etc/systemd/system Enable the service: systemctl enable cloud-print-connector.service Start service: systemctl start cloud-print-connector.service Check status: systemctl status cloud-print-connector.service Done. :) Now everytime you restart your OPI GCP will start automatically. Update Unfortunately GCP can't start at boot time because of the following error: systemctl status cloud-print-connector.service \u00e2\u2014\u008f cloud-print-connector.service - Google Cloud Print Connector Loaded: loaded (/etc/systemd/system/cloud-print-connector.service; enabled) Active: failed (Result: start-limit) since Thu 1970-01-01 00:24:04 UTC; 46 years 8 months ago Docs: https://github.com/google/cloud-print-connector Process: 867 ExecStart=/opt/go/go1.6.3/src/bin/gcp-cups-connector --config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json (code=exited, status=1/FAILURE) Main PID: 867 (code=exited, status=1/FAILURE) Jan 01 00:24:03 OrangePI systemd[1]: Unit cloud-print-connector.service entered failed state. Jan 01 00:24:04 OrangePI systemd[1]: cloud-print-connector.service holdoff time over, scheduling restart. Jan 01 00:24:04 OrangePI systemd[1]: Stopping Google Cloud Print Connector... Jan 01 00:24:04 OrangePI systemd[1]: Starting Google Cloud Print Connector... Jan 01 00:24:04 OrangePI systemd[1]: cloud-print-connector.service start request repeated too quickly, refusing to start. Jan 01 00:24:04 OrangePI systemd[1]: Failed to start Google Cloud Print Connector. Jan 01 00:24:04 OrangePI systemd[1]: Unit cloud-print-connector.service entered failed state. I've checked the log files and I saw that after boot the log file was accessed in 1970, and the log entries was also from 1970: I [01/Jan/1970:00:24:03 +0000] Using config file /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json I [01/Jan/1970:00:24:03 +0000] Cloud Print Connector for CUPS version DEV-linux X [01/Jan/1970:00:24:03 +0000] While starting XMPP, failed to get access token (password): Post https://accounts.google.com/o/oauth2/token: x509: certificate has expired or is not yet valid Based on these errors I tried to modify \"After\" in cloud print service file to start service after ntp.service and some other services: After=cups.service avahi-daemon.service network-online.target ntp.service networking.service exim4.service ssh.service NetworkManager.service wpa_supplicant.service But It did not help. After some tries I gave up and wrote a shell script to start this service: #!/bin/bash IFS=' ' ps aux| grep -v grep | grep -q gcp-cups-connector if [ $? -eq 0 ] then echo \"Already running\" exit 0 fi OK=0 while [ $OK -eq 0 ] do YEAR=$( date +%Y ) if [ $YEAR -gt 2015 ] then echo ok OK=1 /opt/go/go1.6.3/src/bin/gcp-cups-connector --config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json else echo \"Sleep for 1 secs\" sleep 1 fi done This little script check the year in every sec and if it is grater then 2015 the GCP will be started. The problem was: OrangePI has no built in (hw) clock, thus before the ntp service start it doesn't know the current time. GCP was started before the network service came up that's why it could not connect to google service (this happened despite my modification on service file (after property)).","title":"Google Cloud Print With Orange PI (or RPI)"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#google-cloud-print-with-orange-pi-or-rpi","text":"","title":"Google Cloud Print With Orange PI (or RPI)"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#-1-update-your-system-install-requirements","text":"apt-get update apt-ge upgrade apt-get install gcc make # If they aren't installed (for golang) apt-get install build-essential libcups2-dev libavahi-client-dev git bzr # for cloud connector","title":"-1. Update your system &amp; Install Requirements"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#0-install-go-language","text":"Install go lang. You can follow \"Compile GO language on Raspberry PI\" to complete this step. I don't want to write detailed install steps here, instead of I paste only the commands to this chapter without any / detailed comments. (Maybe it will helpful for someone).","title":"0. Install Go Language"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#1-clone-go","text":"mkdir /opt/go/ cd /opt/go/ git clone https://github.com/golang/go`","title":"1. Clone Go"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#2-checkout-go","text":"cd /opt/go/go git tag -l cd .. mv go go1.4.3 cp -r go1.4.3 go1.6.3 cd go1.4.3/ git checkout go1.4.3 cd /opt/go/go1.6.3 git checkout go1.6.3","title":"2. Checkout Go"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#3-compile-go-143","text":"export GOARCH=\"arm\" export GOOS=\"linux\" export GOARM=\"7\" cd /opt/go/go1.4.3/src /opt/go/go1.4.3/src ./all.bash | tee -a output.log Sample Output: Link","title":"3. Compile GO 1.4.3"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#4-set-system-environments","text":"export GOROOT_BOOTSTRAP=\"/opt/go/go1.4.3\" export GOROOT=\"/opt/go/go1.4.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" export GOARCH=\"arm\" export GOOS=\"linux\" export GOARM=\"7\"","title":"4. Set System environments"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#5-some-performance-tuning","text":"fallocate --length 2GiB /root/2G.swap chmod 0600 /root/2G.swap mkswap /root/2G.swap swapon /root/2G.swap ulimit -s 65536","title":"5. Some Performance tuning"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#6-compile-go-163","text":"cd /opt/go/go1.6.3/src ./all.bash | tee -a output.log Sample output: Link","title":"6. Compile GO 1.6.3"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#7-set-system-environments-to-use-go163","text":"You should unset all GO related ENV, or logout from current shell and create new shell. export GOROOT_BOOTSTRAP=\"/opt/go/go1.4.3\" export GOROOT=\"/opt/go/go1.6.3\" export GOROOT_BOOTSTRAP=\"$GOROOT\" export GOPATH=\"$GOROOT/src\" export PATH=\"$PATH:$GOROOT/bin\" export GOARCH=\"arm\" export GOOS=\"linux\" export GOARM=\"7\" Check: go version go version go1.6.3 linux/arm Your GO is ready to use. :)","title":"7. Set System Environments to use go1.6.3."},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#install-google-cloud-print-connector","text":"Simply run the following command: go get github.com/google/cloud-print-connector/... Go will download and install packages and dependencies. Q: What means ... ? A: if you want all packages in that repository, use ... Reference: https://github.com/google/cloud-print-connector/wiki/Build-from-source Run automatically on system boot (systemd): https://github.com/google/cloud-print-connector/wiki/Run-Connector-Automatically-on-Boot This will create two new files to /opt/go/go1.6.3/src/bin directory: root@OrangePI:/opt/go/go1.6.3/src/bin# ls -al total 17504 drwxr-xr-x 2 root root 4096 Sep 6 13:11 . drwxr-xr-x 47 root root 4096 Sep 6 13:11 .. -rwxr-xr-x 1 root root 8358992 Sep 6 13:11 gcp-connector-util -rwxr-xr-x 1 root root 9554384 Sep 6 13:11 gcp-cups-connector And the source is available int /opt/go/go1.6.3/src/src/github.com/google/cloud-print-connector","title":"Install Google Cloud Print connector"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#configure-google-cloud-print-connector","text":"To configure Google Cloud Print (GCP) you simply run gcp-connector-util from /opt/go/go1.6.3/src/bin directory, answer some question. root@OrangePI:/opt/go/go1.6.3/src/bin# ./gcp-connector-util init \"Local printing\" means that clients print directly to the connector via local subnet, and that an Internet connection is neither necessary nor used. Enable local printing? yes \"Cloud printing\" means that clients can print from anywhere on the Internet, and that printers must be explicitly shared with users. Enable cloud printing? yes Retain the user OAuth token to enable automatic sharing? no Proxy name for this connector: opi_sam_ml1510 Visit https://www.google.com/device, and enter this code. I'll wait for you. GGVY-**** At this point you have to login to your google account, visit https://www.google.com/device and give your code. After you enter your code you will get the following message on linux box: The config file /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json is ready to rock. Keep it somewhere safe, as it contains an OAuth refresh token. The next step is checking the configuration: Start GCP connector: ./gcp-cups-connector --config-filename gcp-cups-connector.config.json --log-to-console Check your printer by visiting https://www.google.com/cloudprint/#printers website.If everything went fine you should see your printer. The last step is change log file location by editing gcp-cups-connector.config.json , and modify log_file_name entry according to where you want to save logs. Run in the background: nohup ./gcp-cups-connector --config-filename gcp-cups-connector.config.json &","title":"Configure Google Cloud Print connector"},{"location":"old/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#run-automatically-on-boot","text":"If you want to start GCP at system boot follow these steps. Reference: https://github.com/google/cloud-print-connector/wiki/Run-Connector-Automatically-on-Boot You can find a sample config file in /opt/go/go1.6.3/src/src/github.com/google/cloud-print-connector/systemd directory. Make a backup: cp cloud-print-connector.service cloud-print-connector.service-orig Edit this file. Modify ExecStart and User property to this: ExecStart=/opt/go/go1.6.3/src/bin/gcp-cups-connector -config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json User=root Install systemd service: install -o root -m 0660 cloud-print-connector.service /etc/systemd/system Enable the service: systemctl enable cloud-print-connector.service Start service: systemctl start cloud-print-connector.service Check status: systemctl status cloud-print-connector.service Done. :) Now everytime you restart your OPI GCP will start automatically. Update Unfortunately GCP can't start at boot time because of the following error: systemctl status cloud-print-connector.service \u00e2\u2014\u008f cloud-print-connector.service - Google Cloud Print Connector Loaded: loaded (/etc/systemd/system/cloud-print-connector.service; enabled) Active: failed (Result: start-limit) since Thu 1970-01-01 00:24:04 UTC; 46 years 8 months ago Docs: https://github.com/google/cloud-print-connector Process: 867 ExecStart=/opt/go/go1.6.3/src/bin/gcp-cups-connector --config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json (code=exited, status=1/FAILURE) Main PID: 867 (code=exited, status=1/FAILURE) Jan 01 00:24:03 OrangePI systemd[1]: Unit cloud-print-connector.service entered failed state. Jan 01 00:24:04 OrangePI systemd[1]: cloud-print-connector.service holdoff time over, scheduling restart. Jan 01 00:24:04 OrangePI systemd[1]: Stopping Google Cloud Print Connector... Jan 01 00:24:04 OrangePI systemd[1]: Starting Google Cloud Print Connector... Jan 01 00:24:04 OrangePI systemd[1]: cloud-print-connector.service start request repeated too quickly, refusing to start. Jan 01 00:24:04 OrangePI systemd[1]: Failed to start Google Cloud Print Connector. Jan 01 00:24:04 OrangePI systemd[1]: Unit cloud-print-connector.service entered failed state. I've checked the log files and I saw that after boot the log file was accessed in 1970, and the log entries was also from 1970: I [01/Jan/1970:00:24:03 +0000] Using config file /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json I [01/Jan/1970:00:24:03 +0000] Cloud Print Connector for CUPS version DEV-linux X [01/Jan/1970:00:24:03 +0000] While starting XMPP, failed to get access token (password): Post https://accounts.google.com/o/oauth2/token: x509: certificate has expired or is not yet valid Based on these errors I tried to modify \"After\" in cloud print service file to start service after ntp.service and some other services: After=cups.service avahi-daemon.service network-online.target ntp.service networking.service exim4.service ssh.service NetworkManager.service wpa_supplicant.service But It did not help. After some tries I gave up and wrote a shell script to start this service: #!/bin/bash IFS=' ' ps aux| grep -v grep | grep -q gcp-cups-connector if [ $? -eq 0 ] then echo \"Already running\" exit 0 fi OK=0 while [ $OK -eq 0 ] do YEAR=$( date +%Y ) if [ $YEAR -gt 2015 ] then echo ok OK=1 /opt/go/go1.6.3/src/bin/gcp-cups-connector --config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json else echo \"Sleep for 1 secs\" sleep 1 fi done This little script check the year in every sec and if it is grater then 2015 the GCP will be started. The problem was: OrangePI has no built in (hw) clock, thus before the ntp service start it doesn't know the current time. GCP was started before the network service came up that's why it could not connect to google service (this happened despite my modification on service file (after property)).","title":"Run Automatically On boot"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! How To Install NodeRED on Raspberry PI \u00b6 What is NodeRed? Node-RED is a tool for wiring together hardware devices, APIs and online services in new and interesting ways. 0. Update Your System \u00b6 As always we start with updating our system. Run this with root: apt-get update ; apt-get upgrade 1. Create User For NodeRed & NodeJS \u00b6 We will run node-red with a non-root user. Add new user: useradd nodered Move user's home to /opt/nodered: usermod -m -d /opt/nodered nodered Change home directory's owner: chown -R nodered:nodered /opt/nodered/ Change user's shell: usermod -s /bin/bash nodered Login to user nodemcu: sudo su - nodemcu 2. Get The Necessary Packages \u00b6 NodeRed is written in NodeJS, so the first step is to build a NodeJS runtime environment. We will build it from source, but the binaries are also available for multiple platforms on the NodeJS official page. To compile from source we node some packages on our Linux system. ^1 In my case I had to install only gcc, g++, clang and make packages with this command: apt-get install gcc g++ clang make 2.1. Download & Compile NodeJS \u00b6 Create a directory to store the source codes: mkdir sources Change to \"sources\" directory: cd sources Download the latest source code: wget https://nodejs.org/dist/v7.2.0/node-v7.2.0.tar.gz Untar: tar xf sources/node-v7.2.0.tar.gz Check the available options of the 'configure' script: cd /opt/nodered/sources/node-v7.2.0 ./configure --help If you are using 'root' during the install you don't have to bother with options, simply run ./configure . But now I'm using nodered user to compile nodered, so the 'prefix' option have to be used. Most of the configure script has this option, with this you can specify the directory you want to install the software (make install). The default is /usr/local , but nodered user do not have write access to this direcrory, thus we will specify a custom install localtion. Run the configure script: ./configure --prefix=/opt/nodered/node-v7.2.0 Run make: make This will take a long time (~2-3 hours). Please be patient. :) The last step is to run: make install After the make install finished you will have the NodeJS runtime environment here: ls -al /opt/nodered/node-v7.2.0/ total 24 drwxr-xr-x 6 nodered nodered 4096 Nov 25 07:12 . drwxr-xr-x 4 nodered nodered 4096 Nov 25 07:13 .. drwxr-xr-x 2 nodered nodered 4096 Nov 25 07:12 bin drwxr-xr-x 3 nodered nodered 4096 Nov 25 07:12 include drwxr-xr-x 3 nodered nodered 4096 Nov 25 07:12 lib drwxr-xr-x 5 nodered nodered 4096 Nov 25 07:12 share 3. Install NodeRED \u00b6 Now we are over the hump, there are a few commands left. Add NodeJS bin directory to PATH: export PATH=$PATH:/opt/nodered/node-v7.2.0/bin/ Set up npm prefix system env: export NPM_CONFIG_PREFIX=/opt/nodered/node-v7.2.0/lib/node_modules/ Unless this environment npm will try to install node-red to /usr/local, but 'nodered' user has no write access to this. Run: npm install -g node-red This command will install Node-Red, and after this process you can start your Node-Red instance by typing /opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red . 3.1. Run Node-Red In The Background \u00b6 In order to run node-red in a the background we need one more package called forever . ^3 Do not forget to set up the system environments before install 'forever': export NPM_CONFIG_PREFIX=/opt/nodered/node-v7.2.0/lib/node_modules/ export PATH=$PATH:/opt/nodered/node-v7.2.0/bin/ After that simply run: npm -g install forever You can get help about forever by typing: /opt/nodered/node-v7.2.0/lib/node_modules/bin/forever --help Start Node-Red in the Background: /opt/nodered/node-v7.2.0/lib/node_modules/bin/forever start /opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red You may want to specify --userDir /opt/node/.node-red . Check running 'forever' processes: /opt/nodered/node-v7.2.0/lib/node_modules/bin/forever list Stop 'forever' process: /opt/nodered/node-v7.2.0/lib/node_modules/bin/forever stop /opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red You can use 'Id|Uid|Pid|Index|Script' to stop the application. (For example UID can be check with 'list' option.) 3.2. Add Extra Nodes To Node-Red \u00b6 You can browse more than 1000 extra nodes and flows on the http://flows.nodered.org. For example: Install mysql node: nodered@raspberrypi:~/.node-red$ npm install node-red-node-mysql /opt/nodered/.node-red \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u00ac node-red-node-mysql@0.0.11 \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u00ac mysql@2.11.1 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac bignumber.js@2.3.0 \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u00ac readable-stream@1.1.14 \u00e2\u201d\u201a \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac core-util-is@1.0.2 \u00e2\u201d\u201a \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac inherits@2.0.3 \u00e2\u201d\u201a \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac isarray@0.0.1 \u00e2\u201d\u201a \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac string_decoder@0.10.31 \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac sqlstring@2.0.1 npm WARN enoent ENOENT, open '/opt/nodered/.node-red/package.json' npm WARN .node-red No description npm WARN .node-red No repository field. npm WARN .node-red No README data npm WARN .node-red No license field. **REFERENCES: ** https://github.com/node-red/node-red https://github.com/nodejs/node http://nodered.org/ https://nodejs.org/","title":"How To Install NodeRED on Raspberry PI"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#how-to-install-nodered-on-raspberry-pi","text":"What is NodeRed? Node-RED is a tool for wiring together hardware devices, APIs and online services in new and interesting ways.","title":"How To Install NodeRED on Raspberry PI"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#0-update-your-system","text":"As always we start with updating our system. Run this with root: apt-get update ; apt-get upgrade","title":"0. Update Your System"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#1-create-user-for-nodered-nodejs","text":"We will run node-red with a non-root user. Add new user: useradd nodered Move user's home to /opt/nodered: usermod -m -d /opt/nodered nodered Change home directory's owner: chown -R nodered:nodered /opt/nodered/ Change user's shell: usermod -s /bin/bash nodered Login to user nodemcu: sudo su - nodemcu","title":"1. Create User For NodeRed &amp; NodeJS"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#2-get-the-necessary-packages","text":"NodeRed is written in NodeJS, so the first step is to build a NodeJS runtime environment. We will build it from source, but the binaries are also available for multiple platforms on the NodeJS official page. To compile from source we node some packages on our Linux system. ^1 In my case I had to install only gcc, g++, clang and make packages with this command: apt-get install gcc g++ clang make","title":"2. Get The Necessary Packages"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#21-download-compile-nodejs","text":"Create a directory to store the source codes: mkdir sources Change to \"sources\" directory: cd sources Download the latest source code: wget https://nodejs.org/dist/v7.2.0/node-v7.2.0.tar.gz Untar: tar xf sources/node-v7.2.0.tar.gz Check the available options of the 'configure' script: cd /opt/nodered/sources/node-v7.2.0 ./configure --help If you are using 'root' during the install you don't have to bother with options, simply run ./configure . But now I'm using nodered user to compile nodered, so the 'prefix' option have to be used. Most of the configure script has this option, with this you can specify the directory you want to install the software (make install). The default is /usr/local , but nodered user do not have write access to this direcrory, thus we will specify a custom install localtion. Run the configure script: ./configure --prefix=/opt/nodered/node-v7.2.0 Run make: make This will take a long time (~2-3 hours). Please be patient. :) The last step is to run: make install After the make install finished you will have the NodeJS runtime environment here: ls -al /opt/nodered/node-v7.2.0/ total 24 drwxr-xr-x 6 nodered nodered 4096 Nov 25 07:12 . drwxr-xr-x 4 nodered nodered 4096 Nov 25 07:13 .. drwxr-xr-x 2 nodered nodered 4096 Nov 25 07:12 bin drwxr-xr-x 3 nodered nodered 4096 Nov 25 07:12 include drwxr-xr-x 3 nodered nodered 4096 Nov 25 07:12 lib drwxr-xr-x 5 nodered nodered 4096 Nov 25 07:12 share","title":"2.1. Download &amp; Compile NodeJS"},{"location":"old/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#3-install-nodered","text":"Now we are over the hump, there are a few commands left. Add NodeJS bin directory to PATH: export PATH=$PATH:/opt/nodered/node-v7.2.0/bin/ Set up npm prefix system env: export NPM_CONFIG_PREFIX=/opt/nodered/node-v7.2.0/lib/node_modules/ Unless this environment npm will try to install node-red to /usr/local, but 'nodered' user has no write access to this. Run: npm install -g node-red This command will install Node-Red, and after this process you can start your Node-Red instance by typing /opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red .","title":"3. Install NodeRED"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/","text":"Install Debian Jessie to Orange PI Plus 2 \u00b6 Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Maybe you heard about Orange PI, and maybe you are interested in it. I bought this little board because it's specification, I thought it has to be faster then Raspberry PI2, and it has built-in wifi chip, etc. I love all of my RPI very much and I wanted to try OPI as well, but buying OPI wasn't my best choice of my life. To say the least Orange PI is a little bit neglected and lack of support compared with Raspberry which has a very great and big community. Of course there are a lot of article on Internet about OPI, but sometimes you can't find what you want and you have to do it on your own. The whole story began with I wanted to install Jessie to my OPI. 0. Zero-Step \u00b6 The first thing I have to realize that Debian Jessie image cannot be downloaded from Orange PIs web site . If you don't want to spend (waste) time with build Jessie image, please visit OPI home page and choose from the ready-to-use distros. Maybe after a while Jessie will be also available for download. But now we have to build it manually. (Maybe it can be downloaded from another website, I didn't do detailed search for it.) Yes, it can be downloaded from here . I strongly recommend this forum for everyone who wants to deal with OrangePI. In this post I will follow this guide. Important Most part of this post is by ctrl+c + ctrl+v from various other posts, I wrote this just for collecting things and give more help to others. First I installed a (clean) Debian (8.5) system on a Virtualbox machine. 1. Installing dependencies \u00b6 As in case of any other installation we start with updating the OS. sudo apt-get update sudo apt-get upgrade Installing the necessary packages apt-get install gcc make debootstrap qemu-user-static git Clone the git repository cd /usr/src git clone https://github.com/loboris/OrangePi-BuildLinux 2. Setting up param.sh and create the image \u00b6 cd /usr/src/OrangePi-BuildLinux vi vi params.sh After the setting is done,my param.sh look like this (without commend and blank lines): root@debian:/usr/src/OrangePi-BuildLinux# cat params.sh | grep -v \\# | grep -v ^$ ONLY_BASE=\"no\" HOSTNAME=\"OrangePI\" USER=\"orangepi\" ROOTPASS=\"orangepi\" USERPASS=\"orangepi\" _timezone=\"Etc/UTC\" LANGUAGE=\"en\" LANG=\"en_US.UTF-8\" image_name=\"jessie\" _format=\"ext4\" fatsize=64 linuxsize=1800 distro=\"jessie\" repo=\"http://ftp.hu.debian.org/debian\" raspbian=\"no\" _compress=\"yes\" _boot_on_ext4=\"no\" run create_image Sample output: https://drive.google.com/file/d/0B4xTxuaiVCZyV3dUVEppTnZQeWs After the script has been successfully finished we have jessie.img file: -rw-r--r-- 1 root root 1975517184 Sep 5 20:10 jessie.img 3. Mount and set up the new image \u00b6 I want to show you multiple ways to properly set up these files. 3.1 With kpartx \u00b6 To mount the image file use the following commands: losetup -f losetup /dev/loop0 jessie.img kpartx -av /dev/loop0 For some reason the last command kpartx will fail , but the first partition can be mounted. add map loop0p1 (254:0): 0 131072 linear /dev/loop0 40960 device-mapper: resume ioctl on loop0p2 failed: Invalid argument create/reload failed on loop0p2 add map loop0p2 (0:0): 0 3686401 linear /dev/loop0 172032 root@debian:/usr/src/OrangePi-BuildLinux# blkid | grep \"/dev/mapper/\" /dev/mapper/loop0p1: LABEL=\"BOOT\" UUID=\"DC73-B07C\" TYPE=\"vfat\" PARTUUID=\"8e7a5a1e-01\" mount -t vfat /dev/mapper/loop0p1 /mnt/ Copy uImage and script.bin according to your OPI version. In my case: cd /mnt/ cp uImage_OPI-PLUS uImage cp script.bin.OPI-PLUS_1080p60_hdmi uImage Umount the partition and loop device: sync umount /mnt kpartx -dv /dev/loop0 losetup -d /dev/loop0 3.2 Without kpartx \u00b6 This way is only different from the first one in mounting the image. First determine the start and and position of partitions inside the image file. To do this run parted jessie.img Type u b . This will change the unit type to byte . Type print If you get Error: Can't have a partition outside the disk! error, type Ignore and continue. You will show something like this: Number Start End Size Type File system Flags 1 20971520B 88080383B 67108864B primary fat32 2 88080384B 1975517695B 1887437312B primary ext4 Create loop devices losetup -f jessie.img -o 20971520 --sizelimit 67108864 losetup -f jessie.img -o 88080384 --sizelimit 1887437312 Check loop devices root@debian:/usr/src/OrangePi-BuildLinux# blkid /dev/loop* /dev/loop0: LABEL=\"BOOT\" UUID=\"DC73-B07C\" TYPE=\"vfat\" /dev/loop1: LABEL=\"linux\" UUID=\"bb12e03a-254f-426a-be34-58fd5a9abb94\" TYPE=\"ext4\" Mount them mkdir /mnt/tmp_loop0 mkdir /mnt/tmp_loop1 mount /dev/loop0 /mnt/tmp_loop0 mount /dev/loop1 /mnt/tmp_loop1 You can check tho mounted partitions: ls -al /mnt/tmp_loop0 ls -al /mnt/tmp_loop1 Now you can modify uImage and script.bin. After that unmount the image: umount /mnt/tmp_loop0 /mnt/tmp_loop1 losetup -d /dev/loop0 /dev/loop1 Now you can write this image to your SD card (or flashdrive), and boot you OPI. You can use Win32DiskImager or dd command to do this. 3.3 Do it on your Orange PI \u00b6 The last method is the easiest. Without any modification on the image file just write it to your SD card, and boot your Orange PI. In this case the board have to be connected to a keyboard and a monitor, because without properly configured uImage and script.bin it has a chance that ETH port won't work. After the OPI boot the boot partition should be mounted on /boot or /media/boot , here you can set up the uImage and script.bin. If you are done with it reboot your OPI, and check if ETH port & WIFI adapter working or not. Advantage of using the first and second method is that you shouldn't connect your OPI to display, after flashing the image everything should be working. REFERENCES & LINKS \u00b6 https://github.com/google/cloud-print-connector/wiki/Install https://github.com/google/cloud-print-connector/wiki/Build-from-source http://www.orangepi.cn/orangepibbsen/forum.php?mod=viewthread&tid=342 http://vosse.blogspot.hu/2015/10/installing-linux-img-files-on-orange-pi.html http://www.orangepi.org/Docs/Building.html https://linux-sunxi.org/Main_Page https://github.com/allwinner-zh/linux-3.4-sunxi https://github.com/loboris/OrangePI-Kernel","title":"Install Debian Jessie to Orange PI Plus 2"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#install-debian-jessie-to-orange-pi-plus-2","text":"Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Maybe you heard about Orange PI, and maybe you are interested in it. I bought this little board because it's specification, I thought it has to be faster then Raspberry PI2, and it has built-in wifi chip, etc. I love all of my RPI very much and I wanted to try OPI as well, but buying OPI wasn't my best choice of my life. To say the least Orange PI is a little bit neglected and lack of support compared with Raspberry which has a very great and big community. Of course there are a lot of article on Internet about OPI, but sometimes you can't find what you want and you have to do it on your own. The whole story began with I wanted to install Jessie to my OPI.","title":"Install Debian Jessie to Orange PI Plus 2"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#0-zero-step","text":"The first thing I have to realize that Debian Jessie image cannot be downloaded from Orange PIs web site . If you don't want to spend (waste) time with build Jessie image, please visit OPI home page and choose from the ready-to-use distros. Maybe after a while Jessie will be also available for download. But now we have to build it manually. (Maybe it can be downloaded from another website, I didn't do detailed search for it.) Yes, it can be downloaded from here . I strongly recommend this forum for everyone who wants to deal with OrangePI. In this post I will follow this guide. Important Most part of this post is by ctrl+c + ctrl+v from various other posts, I wrote this just for collecting things and give more help to others. First I installed a (clean) Debian (8.5) system on a Virtualbox machine.","title":"0. Zero-Step"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#1-installing-dependencies","text":"As in case of any other installation we start with updating the OS. sudo apt-get update sudo apt-get upgrade Installing the necessary packages apt-get install gcc make debootstrap qemu-user-static git Clone the git repository cd /usr/src git clone https://github.com/loboris/OrangePi-BuildLinux","title":"1. Installing dependencies"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#2-setting-up-paramsh-and-create-the-image","text":"cd /usr/src/OrangePi-BuildLinux vi vi params.sh After the setting is done,my param.sh look like this (without commend and blank lines): root@debian:/usr/src/OrangePi-BuildLinux# cat params.sh | grep -v \\# | grep -v ^$ ONLY_BASE=\"no\" HOSTNAME=\"OrangePI\" USER=\"orangepi\" ROOTPASS=\"orangepi\" USERPASS=\"orangepi\" _timezone=\"Etc/UTC\" LANGUAGE=\"en\" LANG=\"en_US.UTF-8\" image_name=\"jessie\" _format=\"ext4\" fatsize=64 linuxsize=1800 distro=\"jessie\" repo=\"http://ftp.hu.debian.org/debian\" raspbian=\"no\" _compress=\"yes\" _boot_on_ext4=\"no\" run create_image Sample output: https://drive.google.com/file/d/0B4xTxuaiVCZyV3dUVEppTnZQeWs After the script has been successfully finished we have jessie.img file: -rw-r--r-- 1 root root 1975517184 Sep 5 20:10 jessie.img","title":"2. Setting up param.sh and create the image"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#3-mount-and-set-up-the-new-image","text":"I want to show you multiple ways to properly set up these files.","title":"3. Mount and set up the new image"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#31-with-kpartx","text":"To mount the image file use the following commands: losetup -f losetup /dev/loop0 jessie.img kpartx -av /dev/loop0 For some reason the last command kpartx will fail , but the first partition can be mounted. add map loop0p1 (254:0): 0 131072 linear /dev/loop0 40960 device-mapper: resume ioctl on loop0p2 failed: Invalid argument create/reload failed on loop0p2 add map loop0p2 (0:0): 0 3686401 linear /dev/loop0 172032 root@debian:/usr/src/OrangePi-BuildLinux# blkid | grep \"/dev/mapper/\" /dev/mapper/loop0p1: LABEL=\"BOOT\" UUID=\"DC73-B07C\" TYPE=\"vfat\" PARTUUID=\"8e7a5a1e-01\" mount -t vfat /dev/mapper/loop0p1 /mnt/ Copy uImage and script.bin according to your OPI version. In my case: cd /mnt/ cp uImage_OPI-PLUS uImage cp script.bin.OPI-PLUS_1080p60_hdmi uImage Umount the partition and loop device: sync umount /mnt kpartx -dv /dev/loop0 losetup -d /dev/loop0","title":"3.1 With kpartx"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#32-without-kpartx","text":"This way is only different from the first one in mounting the image. First determine the start and and position of partitions inside the image file. To do this run parted jessie.img Type u b . This will change the unit type to byte . Type print If you get Error: Can't have a partition outside the disk! error, type Ignore and continue. You will show something like this: Number Start End Size Type File system Flags 1 20971520B 88080383B 67108864B primary fat32 2 88080384B 1975517695B 1887437312B primary ext4 Create loop devices losetup -f jessie.img -o 20971520 --sizelimit 67108864 losetup -f jessie.img -o 88080384 --sizelimit 1887437312 Check loop devices root@debian:/usr/src/OrangePi-BuildLinux# blkid /dev/loop* /dev/loop0: LABEL=\"BOOT\" UUID=\"DC73-B07C\" TYPE=\"vfat\" /dev/loop1: LABEL=\"linux\" UUID=\"bb12e03a-254f-426a-be34-58fd5a9abb94\" TYPE=\"ext4\" Mount them mkdir /mnt/tmp_loop0 mkdir /mnt/tmp_loop1 mount /dev/loop0 /mnt/tmp_loop0 mount /dev/loop1 /mnt/tmp_loop1 You can check tho mounted partitions: ls -al /mnt/tmp_loop0 ls -al /mnt/tmp_loop1 Now you can modify uImage and script.bin. After that unmount the image: umount /mnt/tmp_loop0 /mnt/tmp_loop1 losetup -d /dev/loop0 /dev/loop1 Now you can write this image to your SD card (or flashdrive), and boot you OPI. You can use Win32DiskImager or dd command to do this.","title":"3.2 Without kpartx"},{"location":"old/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#33-do-it-on-your-orange-pi","text":"The last method is the easiest. Without any modification on the image file just write it to your SD card, and boot your Orange PI. In this case the board have to be connected to a keyboard and a monitor, because without properly configured uImage and script.bin it has a chance that ETH port won't work. After the OPI boot the boot partition should be mounted on /boot or /media/boot , here you can set up the uImage and script.bin. If you are done with it reboot your OPI, and check if ETH port & WIFI adapter working or not. Advantage of using the first and second method is that you shouldn't connect your OPI to display, after flashing the image everything should be working.","title":"3.3 Do it on your Orange PI"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Install OpenALPR on Raspberry PI 3 (Part 2) \u00b6 I'm writing this post because it was reported that there are some issues with installing OpenALPR and its dependencies. You can check the comments to my old post about this topic here: Install OpenALPR on Raspberry PI 3 After the installation of OpenALPR you can get this error message: Error in fopenReadStream: file not found Error in pixaRead: stream not opened Warning in pixaGetFont: pixa of char bitmaps not found Info in bmfCreate: Generating pixa of bitmap fonts Error in fopenReadStream: file not found Error in pixRead: image file not found: /usr/local/src/openalpr/src/build/chars-14.tif Error in pixaGenerateFont: pixs not all defined Error in bmfCreate: font pixa not made I think the main problem with my first post that some people try to follow the steps without checking the dependencies. I tried to install OpenALPR by following my post, and I successfully reproduced the issue. Unfortunately I didn't have enough time to try this on my RPI3, so I created a VM with much more RAM and CPU than RPI have to decrease the compiles time. If you follow my old post you will install the latest packages which is not too good. So as the first step you have to always check the dependencies! 1. OpenALPR dependencies: Link \u00b6 Tesseract OCR v3.0.4 ( https://github.com/tesseract-ocr/tesseract ) OpenCV v2.4.8+ ( http://opencv.org/ ) 2. Tesseract dependencies: Link \u00b6 First Note: OpenALPR does NOT need the newest Tesseract! Tesseract needs Leptonica, and it can be download from here . BUT! At the moment the newest version is: Latest version: 1.74.1 (1/3/17) So we can notice that: OpenALRP needs Tesseract 3.04 Tesseract needs Leptonica 1.71 Just for testing purpose I installed the latest Leptonica, Tesseract & OpenCV. 3. OpenCV dependencies: Link \u00b6 The Latest OpenCV version: Second Note: We DO NOT need this version. If you get this error: root@opanalpr-tst02:/usr/local/src/opencv-3.2.0/release# opencv_version libdc1394 error: Failed to initialize libdc1394 Please run this: ln /dev/null /dev/raw1394 And / or install these packages: apt-get install libdc1394-22-dev apt-get install libdc1394-22 libdc1394-utils 4. Conclusion \u00b6 If you install the newest version of all dependencies you will get this error message : Error in fopenReadStream: file not found Error in pixaRead: stream not opened Warning in pixaGetFont: pixa of char bitmaps not found Info in bmfCreate: Generating pixa of bitmap fonts Error in fopenReadStream: file not found Error in pixRead: image file not found: /usr/local/src/openalpr/src/build/chars-14.tif Error in pixaGenerateFont: pixs not all defined Error in bmfCreate: font pixa not made Third Note: I suggest to everyone to try installing only the requires version of dependencies, not always the latest one. Maybe there are some ways to use OpenALPR with the latest OpenCV and Tesseract but I failed to find it. Now here is a brief summary on how to install OpenALPR, including only the key steps with some explanation and suggestion. 5. Install OpenALPR \u00b6 Install the dependencies apt-get install autoconf automake libtool libleptonica-dev libicu-dev libpango1.0-dev libcairo2-dev cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev virtualenvwrapper liblog4cplus-dev libcurl4-openssl-dev This part comes from my old post: Install OpenALPR on Raspberry PI 3 Install Leptonica cd /usr/src wget http://www.leptonica.org/source/leptonica-1.71.tar.gz tar xf leptonica-1.71.tar.gz You may need to install these packages: apt-get install libjpeg-dev libtiff5-dev libpng12-dev gcc make Compile: /usr/src/leptonica-1.71 ./configure make make install Install Tesseract You also may need to install these packages: apt-get install ca-certificates git apt-get install autoconf automake libtool apt-get install autoconf-archive apt-get install pkg-config If you plan to install the training tools, you also need the following libraries: apt-get install libicu-dev apt-get install libpango1.0-dev apt-get install libcairo2-dev Clone From GIT cd /usr/src git clone https://github.com/tesseract-ocr/tesseract.git Check available versions (tags) cd /usr/src/tesseract git tag Checkout the version which we need: git checkout 3.04.01 Run these commands: cd /usr/src/tesseract ./autogen.sh ./configure --enable-debug make make install You will get the appropriate version: root@openalpr-tst01:/usr/src/tesseract# tesseract -v tesseract 3.04.01 leptonica-1.71 libjpeg 6b : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 Install OpenCV Download and extract: cd /usr/src wget https://github.com/opencv/opencv/archive/2.4.13.zip unzip 2.4.13.zip Compile: cd opencv-2.4.13 mkdir release cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. make make install Install OpenALPR Download cd /usr/src git clone https://github.com/openalpr/openalpr.git Build: cd openalpr/src mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX:PATH=/usr -DCMAKE_INSTALL_SYSCONFDIR:PATH=/etc .. make make install If you experience some errors please try to install these packages: apt-get install cmake apt-get install liblog4cplus-dev libcurl3-dev sudo apt-get install beanstalkd apt-get install openjdk-7-jdk export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64/ Test: wget http://plates.openalpr.com/h786poj.jpg -O lp.jpg alpr lp.jpg The result must be something like this (Without any errors): plate0: 8 results - 786P0 confidence: 90.1703 - 786PO confidence: 85.579 - 786PQ confidence: 85.3442 - 786PD confidence: 84.4616 - 7B6P0 confidence: 69.4531 - 7B6PO confidence: 64.8618 - 7B6PQ confidence: 64.627 - 7B6PD confidence: 63.7444 If you get any type of missing library error at any steps, run ldconfig command. I hope this post will be useful for you, and you will be able to install OpenALPR. If you have any further question or note you can leave a Disqus comment below.","title":"Install OpenALPR on Raspberry PI 3 (Part 2)"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#install-openalpr-on-raspberry-pi-3-part-2","text":"I'm writing this post because it was reported that there are some issues with installing OpenALPR and its dependencies. You can check the comments to my old post about this topic here: Install OpenALPR on Raspberry PI 3 After the installation of OpenALPR you can get this error message: Error in fopenReadStream: file not found Error in pixaRead: stream not opened Warning in pixaGetFont: pixa of char bitmaps not found Info in bmfCreate: Generating pixa of bitmap fonts Error in fopenReadStream: file not found Error in pixRead: image file not found: /usr/local/src/openalpr/src/build/chars-14.tif Error in pixaGenerateFont: pixs not all defined Error in bmfCreate: font pixa not made I think the main problem with my first post that some people try to follow the steps without checking the dependencies. I tried to install OpenALPR by following my post, and I successfully reproduced the issue. Unfortunately I didn't have enough time to try this on my RPI3, so I created a VM with much more RAM and CPU than RPI have to decrease the compiles time. If you follow my old post you will install the latest packages which is not too good. So as the first step you have to always check the dependencies!","title":"Install OpenALPR on Raspberry PI 3 (Part 2)"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#1-openalpr-dependencies-link","text":"Tesseract OCR v3.0.4 ( https://github.com/tesseract-ocr/tesseract ) OpenCV v2.4.8+ ( http://opencv.org/ )","title":"1. OpenALPR dependencies: Link"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#2-tesseract-dependencies-link","text":"First Note: OpenALPR does NOT need the newest Tesseract! Tesseract needs Leptonica, and it can be download from here . BUT! At the moment the newest version is: Latest version: 1.74.1 (1/3/17) So we can notice that: OpenALRP needs Tesseract 3.04 Tesseract needs Leptonica 1.71 Just for testing purpose I installed the latest Leptonica, Tesseract & OpenCV.","title":"2. Tesseract dependencies: Link"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#3-opencv-dependencies-link","text":"The Latest OpenCV version: Second Note: We DO NOT need this version. If you get this error: root@opanalpr-tst02:/usr/local/src/opencv-3.2.0/release# opencv_version libdc1394 error: Failed to initialize libdc1394 Please run this: ln /dev/null /dev/raw1394 And / or install these packages: apt-get install libdc1394-22-dev apt-get install libdc1394-22 libdc1394-utils","title":"3. OpenCV dependencies: Link"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#4-conclusion","text":"If you install the newest version of all dependencies you will get this error message : Error in fopenReadStream: file not found Error in pixaRead: stream not opened Warning in pixaGetFont: pixa of char bitmaps not found Info in bmfCreate: Generating pixa of bitmap fonts Error in fopenReadStream: file not found Error in pixRead: image file not found: /usr/local/src/openalpr/src/build/chars-14.tif Error in pixaGenerateFont: pixs not all defined Error in bmfCreate: font pixa not made Third Note: I suggest to everyone to try installing only the requires version of dependencies, not always the latest one. Maybe there are some ways to use OpenALPR with the latest OpenCV and Tesseract but I failed to find it. Now here is a brief summary on how to install OpenALPR, including only the key steps with some explanation and suggestion.","title":"4. Conclusion"},{"location":"old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#5-install-openalpr","text":"Install the dependencies apt-get install autoconf automake libtool libleptonica-dev libicu-dev libpango1.0-dev libcairo2-dev cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev virtualenvwrapper liblog4cplus-dev libcurl4-openssl-dev This part comes from my old post: Install OpenALPR on Raspberry PI 3 Install Leptonica cd /usr/src wget http://www.leptonica.org/source/leptonica-1.71.tar.gz tar xf leptonica-1.71.tar.gz You may need to install these packages: apt-get install libjpeg-dev libtiff5-dev libpng12-dev gcc make Compile: /usr/src/leptonica-1.71 ./configure make make install Install Tesseract You also may need to install these packages: apt-get install ca-certificates git apt-get install autoconf automake libtool apt-get install autoconf-archive apt-get install pkg-config If you plan to install the training tools, you also need the following libraries: apt-get install libicu-dev apt-get install libpango1.0-dev apt-get install libcairo2-dev Clone From GIT cd /usr/src git clone https://github.com/tesseract-ocr/tesseract.git Check available versions (tags) cd /usr/src/tesseract git tag Checkout the version which we need: git checkout 3.04.01 Run these commands: cd /usr/src/tesseract ./autogen.sh ./configure --enable-debug make make install You will get the appropriate version: root@openalpr-tst01:/usr/src/tesseract# tesseract -v tesseract 3.04.01 leptonica-1.71 libjpeg 6b : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 Install OpenCV Download and extract: cd /usr/src wget https://github.com/opencv/opencv/archive/2.4.13.zip unzip 2.4.13.zip Compile: cd opencv-2.4.13 mkdir release cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. make make install Install OpenALPR Download cd /usr/src git clone https://github.com/openalpr/openalpr.git Build: cd openalpr/src mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX:PATH=/usr -DCMAKE_INSTALL_SYSCONFDIR:PATH=/etc .. make make install If you experience some errors please try to install these packages: apt-get install cmake apt-get install liblog4cplus-dev libcurl3-dev sudo apt-get install beanstalkd apt-get install openjdk-7-jdk export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64/ Test: wget http://plates.openalpr.com/h786poj.jpg -O lp.jpg alpr lp.jpg The result must be something like this (Without any errors): plate0: 8 results - 786P0 confidence: 90.1703 - 786PO confidence: 85.579 - 786PQ confidence: 85.3442 - 786PD confidence: 84.4616 - 7B6P0 confidence: 69.4531 - 7B6PO confidence: 64.8618 - 7B6PQ confidence: 64.627 - 7B6PD confidence: 63.7444 If you get any type of missing library error at any steps, run ldconfig command. I hope this post will be useful for you, and you will be able to install OpenALPR. If you have any further question or note you can leave a Disqus comment below.","title":"5. Install OpenALPR"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/","text":"Mount SD card image (partitioned image) w/o kpartx \u00b6 Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! If you are working with SD card image I pretty sure that you were in the situation when you had to mount the image before write it to the SD card. Particular example when you want to modify cmdline.txt in a Raspberry PI image (because you want to use different partition for booting). With kpartx utility \u00b6 If your system doesn't have kpartx utility install it for example with apt-get install kpartx command in case of Debian-Like systems. For demonstration I use 2016-05-27-raspbian-jessie-lite.img image. set up loop devices losetup -f 2016-05-27-raspbian-jessie-lite.img (optional) Check the loop device losetup NAME SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE /dev/loop0 0 0 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img Create device maps from partition tables kpartx -av /dev/loop0 add map loop0p1 (254:0): 0 129024 linear /dev/loop0 8192 add map loop0p2 (254:1): 0 2572288 linear /dev/loop0 137216 (optional) Check mapped partitions with blkid Simpliy type `blkid` and hit enter. Output: ... /dev/mapper/loop0p1: SEC_TYPE=\"msdos\" LABEL=\"boot\" UUID=\"22E0-C711\" TYPE=\"vfat\" PARTUUID=\"6fcf21f3-01\" /dev/mapper/loop0p2: UUID=\"202638e1-4ce4-45df-9a00-ad725c2537bb\" TYPE=\"ext4\" PARTUUID=\"6fcf21f3-02\" ... Mount these partitions mkdir /mnt/tmp1 mkdir /mnt/tmp2 mount -t vfat /dev/mapper/loop0p1 /mnt/tmp1 mount -t ext4 /dev/mapper/loop0p2 /mnt/tmp2 After you are done with the neccessary modifications you can dismount everything. DisMount umount /mnt/tmp1 umount /mnt/tmp2 kpartx -dv /dev/loop0 losetup -d /dev/loop0 You can check it everything is unmounted by running losetup command, it should return with \"nothing\". Without kpartx utility \u00b6 It is possible to mount partitions inside an image without kpart utility as well, but I think this way a little bit more complicated. Follow these steps: Determine the size partitions (where is it started and ended) \u00b6 1.1. Run parted 2016-05-27-raspbian-jessie-lite.img 1.2. Type u b and enter. --> This will change the display unit to bytes. 1.3. Type print to display partition layout. Sample output: parted 2016-05-27-raspbian-jessie-lite.img GNU Parted 3.2 Using /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) u b (parted) print Model: (file) Disk /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img: 1387266048B Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 4194304B 70254591B 66060288B primary fat16 lba 2 70254592B 1387266047B 1317011456B primary ext4 (parted) q Set up loopbak devices \u00b6 As you can see there are two partition inside the image file. We need to create two loopback device for them: losetup -f -o 4194304 --sizelimit 66060288 2016-05-27-raspbian-jessie-lite.img losetup -f -o 70254592 --sizelimit 1317011456 2016-05-27-raspbian-jessie-lite.img Warning Please double check the offset and sizelimit parameters! Check with losetup and blkid commands: losetup: NAME SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE /dev/loop0 66060288 4194304 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img /dev/loop1 1317011456 70254592 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img blkid: /dev/loop0: SEC_TYPE=\"msdos\" LABEL=\"boot\" UUID=\"22E0-C711\" TYPE=\"vfat\" /dev/loop1: UUID=\"202638e1-4ce4-45df-9a00-ad725c2537bb\" TYPE=\"ext4\" Mount the partitions \u00b6 mount /dev/loop0 /mnt/tmp1 mount /dev/loop1 /mnt/tmp2 Check: df -hT /dev/loop* Filesystem Type Size Used Avail Use% Mounted on /dev/loop0 vfat 63M 21M 43M 33% /mnt/tmp1 /dev/loop1 ext4 1.2G 738M 389M 66% /mnt/tmp2 udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev After you are done with you work unmount everything. Unmount the partitions \u00b6 root@debian:~# umount /mnt/tmp1 /mnt/tmp2/ root@debian:~# losetup NAME SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE /dev/loop0 66060288 4194304 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img /dev/loop1 1317011456 70254592 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img root@debian:~# losetup -d /dev/loop0 /dev/loop1 root@debian:~# losetup root@debian:~#","title":"Mount SD card image (partitioned image) w/o kpartx"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#mount-sd-card-image-partitioned-image-wo-kpartx","text":"Caution This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! If you are working with SD card image I pretty sure that you were in the situation when you had to mount the image before write it to the SD card. Particular example when you want to modify cmdline.txt in a Raspberry PI image (because you want to use different partition for booting).","title":"Mount SD card image (partitioned image) w/o kpartx"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#with-kpartx-utility","text":"If your system doesn't have kpartx utility install it for example with apt-get install kpartx command in case of Debian-Like systems. For demonstration I use 2016-05-27-raspbian-jessie-lite.img image. set up loop devices losetup -f 2016-05-27-raspbian-jessie-lite.img (optional) Check the loop device losetup NAME SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE /dev/loop0 0 0 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img Create device maps from partition tables kpartx -av /dev/loop0 add map loop0p1 (254:0): 0 129024 linear /dev/loop0 8192 add map loop0p2 (254:1): 0 2572288 linear /dev/loop0 137216 (optional) Check mapped partitions with blkid Simpliy type `blkid` and hit enter. Output: ... /dev/mapper/loop0p1: SEC_TYPE=\"msdos\" LABEL=\"boot\" UUID=\"22E0-C711\" TYPE=\"vfat\" PARTUUID=\"6fcf21f3-01\" /dev/mapper/loop0p2: UUID=\"202638e1-4ce4-45df-9a00-ad725c2537bb\" TYPE=\"ext4\" PARTUUID=\"6fcf21f3-02\" ... Mount these partitions mkdir /mnt/tmp1 mkdir /mnt/tmp2 mount -t vfat /dev/mapper/loop0p1 /mnt/tmp1 mount -t ext4 /dev/mapper/loop0p2 /mnt/tmp2 After you are done with the neccessary modifications you can dismount everything. DisMount umount /mnt/tmp1 umount /mnt/tmp2 kpartx -dv /dev/loop0 losetup -d /dev/loop0 You can check it everything is unmounted by running losetup command, it should return with \"nothing\".","title":"With kpartx utility"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#without-kpartx-utility","text":"It is possible to mount partitions inside an image without kpart utility as well, but I think this way a little bit more complicated. Follow these steps:","title":"Without kpartx utility"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#determine-the-size-partitions-where-is-it-started-and-ended","text":"1.1. Run parted 2016-05-27-raspbian-jessie-lite.img 1.2. Type u b and enter. --> This will change the display unit to bytes. 1.3. Type print to display partition layout. Sample output: parted 2016-05-27-raspbian-jessie-lite.img GNU Parted 3.2 Using /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img Welcome to GNU Parted! Type 'help' to view a list of commands. (parted) u b (parted) print Model: (file) Disk /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img: 1387266048B Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 4194304B 70254591B 66060288B primary fat16 lba 2 70254592B 1387266047B 1317011456B primary ext4 (parted) q","title":"Determine the size partitions (where is it started and ended)"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#set-up-loopbak-devices","text":"As you can see there are two partition inside the image file. We need to create two loopback device for them: losetup -f -o 4194304 --sizelimit 66060288 2016-05-27-raspbian-jessie-lite.img losetup -f -o 70254592 --sizelimit 1317011456 2016-05-27-raspbian-jessie-lite.img Warning Please double check the offset and sizelimit parameters! Check with losetup and blkid commands: losetup: NAME SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE /dev/loop0 66060288 4194304 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img /dev/loop1 1317011456 70254592 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img blkid: /dev/loop0: SEC_TYPE=\"msdos\" LABEL=\"boot\" UUID=\"22E0-C711\" TYPE=\"vfat\" /dev/loop1: UUID=\"202638e1-4ce4-45df-9a00-ad725c2537bb\" TYPE=\"ext4\"","title":"Set up loopbak devices"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#mount-the-partitions","text":"mount /dev/loop0 /mnt/tmp1 mount /dev/loop1 /mnt/tmp2 Check: df -hT /dev/loop* Filesystem Type Size Used Avail Use% Mounted on /dev/loop0 vfat 63M 21M 43M 33% /mnt/tmp1 /dev/loop1 ext4 1.2G 738M 389M 66% /mnt/tmp2 udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev udev devtmpfs 10M 0 10M 0% /dev After you are done with you work unmount everything.","title":"Mount the partitions"},{"location":"old/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#unmount-the-partitions","text":"root@debian:~# umount /mnt/tmp1 /mnt/tmp2/ root@debian:~# losetup NAME SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE /dev/loop0 66060288 4194304 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img /dev/loop1 1317011456 70254592 0 0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img root@debian:~# losetup -d /dev/loop0 /dev/loop1 root@debian:~# losetup root@debian:~#","title":"Unmount the partitions"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/","text":"Move root file system to USB storage (RPI2 & RPI3) \u00b6 Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! UPDATE: 2018-11-06 Maybe this post has lost its purpose because nowadays all modern distros support moving root file system to external device, for examples: Raspberry PIs: With help of noobs you can install the whole OS to external device. Orange & Banana PIs: Armbian has the feature of moving file system to USB ( armbian-config ). I don't think this article can be used as a complete guide, the only reason I let this post (a)live is the commands and links in it. There are a lot of very good and useful guide on the Internet which can help you to move your root file system to an external USB storage. Using external storage is highly recommended if you want to use your RPI as a server machine which runs 7/24. I had a lot of problems with SD cards, I think they are absolutely unsuitable for use in a raspberry and store the root file system on them. I pretty sure all SD card will die in some months, and you will lost all of your data. This happens to me and I decided not to use SD cards anymore. You can find some guides here: \u00b6 http://www.kupply.com/move-your-raspberry-pi-system-to-usb-in-10-steps https://www.raspberrypi.org/blog/pi-3-booting-part-i-usb-mass-storage-boot (PI3 only) https://liewdaryl.wordpress.com/2015/06/06/setting-up-raspberry-pi-2-including-moving-rootfs-to-usb-drive http://elinux.org/Transfer_system_disk_from_SD_card_to_hard_disk So why am I writing this post? Just because I spent a lot of time to do this on my own, and I want to write down my experiences. These tips can be useful when you deal with SD card, SD card images and hard drives. 1. Create Image From SD card with dd \u00b6 You can create copy of your entire SD card. For example for backup. dd if=/dev/sdc of=RPI_sdcard.img bs=4096 If something fails during working with the image try decrease \"bs\" to 1024. 2. Extract Partitions From The Image File \u00b6 2.1. Determine the size of partitions \u00b6 I will use the official raspberry image. Run this command: parted 2016-09-23-raspbian-jessie.img Inside the parted type: (parted) u b (parted) print Number Start End Size Type File system Flags 1 4194304B 70254591B 66060288B primary fat16 lba 2 70254592B 4348444671B 4278190080B primary ext4 First one is the boot partition, the second one is the root partition. 2.1. Extract the first partition from the image \u00b6 Command: dd if=2016-09-23-raspbian-jessie.img iflag=skip_bytes,count_bytes,fullblock bs=4096 count=70254591 of=boot_fs.img This will create the boot_fs.img , inside it the second partition is absolutely unnecessary, so we can delete it. To do this use the fdisk utility. Command: fdisk boot_fs.img Inside the fdisk type these commands: Command (m for help): d Partition number (1,2, default 2): 2 Partition 2 has been deleted. Command (m for help): w The partition table has been altered. Syncing disks. The created image is about 70MB, and it is more than enough to boot your PI, but be aware that you have to modify the cmdline.txt to use an external hard disk for the root partition. 2.1. Extract the second partition from the image \u00b6 Command: dd if=2016-09-23-raspbian-jessie.img iflag=skip_bytes,count_bytes,fullblock bs=4096 skip=70254592 count=4278190080 of=root_fs.img The meaning of the options: if=2016-09-23-raspbian-jessie.img --> Input file iflag --> \"read as per the comma separated symbol list\" skip_bytes --> \"treat 'skip=N' as a byte count (iflag only)\" count_bytes --> \"treat 'count=N' as a byte count (iflag only)\" fullblock --> \"accumulate full blocks of input (iflag only)\" skip=70254592 --> Skip the first 70254592 bytes from the image. The start position of the second partition inside the image file. count=4278190080 --> The size of the second partition. That's all. At the end of this step we have two files: -rw-r--r-- 1 root root 4278190080 nov 19 16:39 root_fs.img -rw-r--r-- 1 root root 70254591 nov 19 16:36 boot_fs.img We can check all of them by using fdisk/parted/gdisk/gparted utility. 3. Prepare HDDs \u00b6 3.1. Remove all partition entries from the disk \u00b6 Example: wipefs --all /dev/sdb IMPORTANT: This will destroy all data on your entire disk! 3.2. Create Partition \u00b6 If you want to copy an image (for example the root_fs.img) to your Hard Drive, first create a partition on it. Run parted: parted /dev/sdb You will see an error message: Error: /dev/sdb: unrecognised disk label So we have to create the disk label first: Command (inside parted): (parted) mktable msdos Create a partition for the image. Its size have to be at least 4278190080 Byte, but we will create a 20GB partition. (parted) mkpart Partition type? primary/extended? primary File system type? [ext2]? ext3 Start? 0% (optional) Check the partition (parted) p Model: WDC WD25 00JB-98GVA0 (scsi) Disk /dev/sdb: 250GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 20,0GB 20,0GB primary ext3 lba After that you can create as many partition as you want. To create partitions you can use gparted graphical utility, too, or any other partition utility. 3.2. Copy image to the partition \u00b6 Command: dd if=root_fs.img of=/dev/sdb1 bs=4096 If you created larger partition than the image, you have to resize to partition. (Of course if you created smaller partition than the image, dd will fail with \"no space left on device\" error message.) Before running resize2fs you have to check the partition: e2fsck -f /dev/sdb1 Resize: resize2fs /dev/sdb1 3. Configure Raspberry to use root fs on the external hard drive \u00b6 3.1. cmdline.txt \u00b6 This file can be found on the boot partition, on your SD card. Modify from: root=/dev/mmcblk0p2 To: root=/dev/sda1 3.2. etc/fstab \u00b6 This file must be located on your external hard disk root partition. If your SD card still have the root partition lest modify fstab on it, because it won't take effect. Modify from: /dev/mmcblk0p2 / ext4 defaults,noatime 0 1 To: /dev/sdaq / ext4 defaults,noatime 0 2 4. Final Thoughts \u00b6 I think running OS from an external HDD (or SSD) is highly recommended. As this post doesn't give you a step-by-step guide, I share with you the method I usually move root fs to HDD: Download the image from https://www.raspberrypi.org/downloads/ Write the image to an SD card. (Linux: dd) Start your Raspberry PI and do the initial steps. The most important the FS resize. Shutdown the Raspberry Copy the root partition to the HDD. (Linux: dd) Edit cmdline.txt and fstab. (Please aware that the cmdline.txt remains on the SD card, but the fstab must be edited on the HDD.) Take back the SD card to the Raspberry & connect the external HDD. Boot & Enjoy References: http://unix.stackexchange.com/questions/38164/create-partition-aligned-using-parted https://www.pantz.org/software/parted/parted_and_disk_alignment.html http://askubuntu.com/questions/201164/proper-alignment-of-partitions-on-an-advanced-format-hdd-using-parted http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/ http://gparted.org/h2-fix-msdos-pt.php https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/msd.md","title":"Move root file system to USB storage (RPI2 & RPI3)"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#move-root-file-system-to-usb-storage-rpi2-rpi3","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! UPDATE: 2018-11-06 Maybe this post has lost its purpose because nowadays all modern distros support moving root file system to external device, for examples: Raspberry PIs: With help of noobs you can install the whole OS to external device. Orange & Banana PIs: Armbian has the feature of moving file system to USB ( armbian-config ). I don't think this article can be used as a complete guide, the only reason I let this post (a)live is the commands and links in it. There are a lot of very good and useful guide on the Internet which can help you to move your root file system to an external USB storage. Using external storage is highly recommended if you want to use your RPI as a server machine which runs 7/24. I had a lot of problems with SD cards, I think they are absolutely unsuitable for use in a raspberry and store the root file system on them. I pretty sure all SD card will die in some months, and you will lost all of your data. This happens to me and I decided not to use SD cards anymore.","title":"Move root file system to USB storage (RPI2 &amp; RPI3)"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#1-create-image-from-sd-card-with-dd","text":"You can create copy of your entire SD card. For example for backup. dd if=/dev/sdc of=RPI_sdcard.img bs=4096 If something fails during working with the image try decrease \"bs\" to 1024.","title":"1. Create Image From SD card with dd"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#2-extract-partitions-from-the-image-file","text":"","title":"2. Extract Partitions From The Image File"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#3-prepare-hdds","text":"","title":"3. Prepare HDDs"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#3-configure-raspberry-to-use-root-fs-on-the-external-hard-drive","text":"","title":"3. Configure Raspberry to use root fs on the external hard drive"},{"location":"old/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#4-final-thoughts","text":"I think running OS from an external HDD (or SSD) is highly recommended. As this post doesn't give you a step-by-step guide, I share with you the method I usually move root fs to HDD: Download the image from https://www.raspberrypi.org/downloads/ Write the image to an SD card. (Linux: dd) Start your Raspberry PI and do the initial steps. The most important the FS resize. Shutdown the Raspberry Copy the root partition to the HDD. (Linux: dd) Edit cmdline.txt and fstab. (Please aware that the cmdline.txt remains on the SD card, but the fstab must be edited on the HDD.) Take back the SD card to the Raspberry & connect the external HDD. Boot & Enjoy References: http://unix.stackexchange.com/questions/38164/create-partition-aligned-using-parted https://www.pantz.org/software/parted/parted_and_disk_alignment.html http://askubuntu.com/questions/201164/proper-alignment-of-partitions-on-an-advanced-format-hdd-using-parted http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/ http://gparted.org/h2-fix-msdos-pt.php https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/msd.md","title":"4. Final Thoughts"},{"location":"old/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! I have an old but working Samsung ML1510 printer, and everytime I want to print something I have to turn on my old desktop PC, copy my doc(s) to it and start printing from it. So I have decided to make my old printer able to work over wireless. My goals: To print from my Home network over Wifi connection. I have an iPhone and it supports AirPrint. I have a Google account, and Google has \"Google Cloud Print\" service. It supports local printing and printing over the internet from anywhere. (I don't understand why people want to print from anywhere to home, but O.K. let's do it.) Of course I can use a Raspberry PI 2 for this project, but I recommend to use RPI3, because it has built-in WIFI. In my case RPI is connected with ETH cable to my existing network (, or you can use Wi-fi stick) Raspbian version: Linux raspberrypi 4.4.13-v7+ #894 SMP Mon Jun 13 13:13:27 BST 2016 armv7l GNU/Linux Download Link: latest 1. Samsung Driver \u00b6 If it is needed please run update & upgrade before apt-get install : sudo apt-get update sudo apt-get upgrade The next step is to find a driver for your printer. I have a Samsung ML_1510 printer, after some googling I found an article which said that \u00e2\u20ac\u0153splix\u00e2\u20ac package contains the driver for my printer (printer-driver-splix) . Unfortunately the official Samsung linux driver does not support ARM architecture. root@raspberrypi:~# apt-cache search samsung bitpim - utility to communicate with many CDMA phones bitpim-lib - architecture-dependent helper files for BitPim heimdall-flash - tool for flashing firmware on Samsung Galaxy S devices heimdall-flash-frontend - tool for flashing firmware on Samsung Galaxy S devices - Qt GUI libimage-exiftool-perl - library and program to read and write meta information in multimedia files madwimax - user-space driver for mWiMAX equipment based on Samsung CMC-730 printer-driver-splix - Driver for Samsung and Xerox SPL2 and SPLc laser printers skyeye - Embedded Hardware Simulation firmware-samsung - Binary firmware for Samsung MFC video codecs Command: sudo apt-get install printer-driver-splix 2. Installing and configuring CUPS \u00b6 Configuring CUPS is very easy. It listens on TCP/631, and you can use your favorite browser to access the configuration page. Because Raspberry has limited resources (mem, cpu), I use ssh tunnel to configure CUPS. (By default cups listen only on the loopback interface for security reason. Of course you can configure cups to listen on its private IP address, but I love tunnelling everything. \u2122\u201a If you prefer this way you should change Listen localhost:631 to Port 631 in /etc/cups/cupsd.conf .) So I created a local tunnel from another linux box in my network: ssh 172.16.0.210 -L 1631:localhost:631 Note I used port 1631 because non-root user can not bind port under 1025. Install CUPS: apt-get install cups cups-client cups-common cups-pdf Before you start administrating you should add \u00e2\u20ac\u0153pi\u00e2\u20ac (or another) user to lpadmin group (or you can use root user): usermod -a -G lpadmin pi OK. Open your browser and head to http://localhost:1631 (or http://localhost:631 without tunneling or http://[IP]:631 if you preconfigured cups to listen on all interface). Click on \u00e2\u20ac\u0153Administration\u00e2\u20ac. You will be prompted for a username and password. If you previously added your user to lpadmin group you can use it, otherwise use \u00e2\u20ac\u0153root\u00e2\u20ac. Click \u00e2\u20ac\u0153Add Printer Button\u00e2\u20ac : Select your printer from the list. (In my case: Samsung ML-1510_700 (Samsung ML-1510_700)) and click continue . Give a name to your printer, check \u00e2\u20ac\u0153Share This Printer\u00e2\u20ac and click continue . Select your printer (in my case Samsung ML_1510) and click \u00e2\u20ac\u0153Add Printer\u00e2\u20ac If your printer is not listed its driver is not properly installed. You have to do some research for linux driver. Set the default values for this printer. (I did not created a screenshot because this step is different in case of each printer. On the main page click \u00e2\u20ac\u0153Printers\u00e2\u20ac and check your newly added printer status . (Optional) You can print a test page. Click on the printer\u00e2\u20ac\u2122s name : Now you can see the printer's configuration page. Click on \u00e2\u20ac\u0153Maintenance\u00e2\u20ac drop-down list and choose \u00e2\u20ac\u0153Print Test Page\u00e2\u20ac : At the bottom of the page you can see the status of the printing but only for a few seconds. Next we check Jobs: Click on \u00e2\u20ac\u0153Job\u00e2\u20ac on the horizontal main menu, and click \u00e2\u20ac\u0153Show Competed Jobs\u00e2\u20ac: (Optional) On the \u00e2\u20ac\u0153Administration\u00e2\u20ac page check: \u00e2\u20ac\u201c Share printers connected to this system \u00e2\u20ac\u201c Allow remote administration \u00e2\u20ac\u201c Allow printing from the Internet This is a very basic setup of cups, but this is just enough at the beginning. If you want to know more about CUPS visit its official website or do some google (re)search. 2. Make you printer available for Air print \u00b6 This step is incredibly easy, just one command: sudo apt-get install avahi-discover avahi-daemon I found some articles about \u00e2\u20ac\u0153cups airprint setup\u00e2\u20ac which are much complected, but somehow in my case it works with installing only the avahi damon and discover. Links: http://iain.polevaultweb.com/2014/03/setting-raspberry-pi-print-server-airprint-support/ http://www.lynsayshepherd.com/blog/2015/10/18/wireless-printingairprint-server-via-the-raspberry-pi-updated-guide/ https://wiki.debian.org/AirPrint NOTE: Of course you have to connect your RasPI to the same network which used by your Wifi, and connect your phone to it in order to make everything work fine. 3. Add your printer to Windows 10 \u00b6 So I think if you configured a network printer you would like to use it from another system(s). If you don\u00e2\u20ac\u2122t share your printer via SAMBA you can use your printers URL to connect to it from Win10. You can check this URL by accessing the admin page of cups. Click on \u00e2\u20ac\u0153printers\u00e2\u20ac and select the printer you want to connect to: Copy your Printer URL to clipboard from your browser Address Line! For example: https://172.16.0.210:631/printers/rpi_samsung_ml-1510 4. Windows Setup: \u00b6 On Windows system click \u00e2\u20ac\u0153Device and Printers\u00e2\u20ac in the control panel. Click Add printer Click The printer that I want isnt listed Click Select a shared printer by name\u00e2\u20ac and paste your printer URL: Click Next. If Windows can not connect to you printer, try without SSL (http). Select your printer\u00e2\u20ac\u2122s driver and click finish.","title":"Raspberry PI 2 as print server + AirPrint"},{"location":"old/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#1-samsung-driver","text":"If it is needed please run update & upgrade before apt-get install : sudo apt-get update sudo apt-get upgrade The next step is to find a driver for your printer. I have a Samsung ML_1510 printer, after some googling I found an article which said that \u00e2\u20ac\u0153splix\u00e2\u20ac package contains the driver for my printer (printer-driver-splix) . Unfortunately the official Samsung linux driver does not support ARM architecture. root@raspberrypi:~# apt-cache search samsung bitpim - utility to communicate with many CDMA phones bitpim-lib - architecture-dependent helper files for BitPim heimdall-flash - tool for flashing firmware on Samsung Galaxy S devices heimdall-flash-frontend - tool for flashing firmware on Samsung Galaxy S devices - Qt GUI libimage-exiftool-perl - library and program to read and write meta information in multimedia files madwimax - user-space driver for mWiMAX equipment based on Samsung CMC-730 printer-driver-splix - Driver for Samsung and Xerox SPL2 and SPLc laser printers skyeye - Embedded Hardware Simulation firmware-samsung - Binary firmware for Samsung MFC video codecs Command: sudo apt-get install printer-driver-splix","title":"1. Samsung Driver"},{"location":"old/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#2-installing-and-configuring-cups","text":"Configuring CUPS is very easy. It listens on TCP/631, and you can use your favorite browser to access the configuration page. Because Raspberry has limited resources (mem, cpu), I use ssh tunnel to configure CUPS. (By default cups listen only on the loopback interface for security reason. Of course you can configure cups to listen on its private IP address, but I love tunnelling everything. \u2122\u201a If you prefer this way you should change Listen localhost:631 to Port 631 in /etc/cups/cupsd.conf .) So I created a local tunnel from another linux box in my network: ssh 172.16.0.210 -L 1631:localhost:631 Note I used port 1631 because non-root user can not bind port under 1025. Install CUPS: apt-get install cups cups-client cups-common cups-pdf Before you start administrating you should add \u00e2\u20ac\u0153pi\u00e2\u20ac (or another) user to lpadmin group (or you can use root user): usermod -a -G lpadmin pi OK. Open your browser and head to http://localhost:1631 (or http://localhost:631 without tunneling or http://[IP]:631 if you preconfigured cups to listen on all interface). Click on \u00e2\u20ac\u0153Administration\u00e2\u20ac. You will be prompted for a username and password. If you previously added your user to lpadmin group you can use it, otherwise use \u00e2\u20ac\u0153root\u00e2\u20ac. Click \u00e2\u20ac\u0153Add Printer Button\u00e2\u20ac : Select your printer from the list. (In my case: Samsung ML-1510_700 (Samsung ML-1510_700)) and click continue . Give a name to your printer, check \u00e2\u20ac\u0153Share This Printer\u00e2\u20ac and click continue . Select your printer (in my case Samsung ML_1510) and click \u00e2\u20ac\u0153Add Printer\u00e2\u20ac If your printer is not listed its driver is not properly installed. You have to do some research for linux driver. Set the default values for this printer. (I did not created a screenshot because this step is different in case of each printer. On the main page click \u00e2\u20ac\u0153Printers\u00e2\u20ac and check your newly added printer status . (Optional) You can print a test page. Click on the printer\u00e2\u20ac\u2122s name : Now you can see the printer's configuration page. Click on \u00e2\u20ac\u0153Maintenance\u00e2\u20ac drop-down list and choose \u00e2\u20ac\u0153Print Test Page\u00e2\u20ac : At the bottom of the page you can see the status of the printing but only for a few seconds. Next we check Jobs: Click on \u00e2\u20ac\u0153Job\u00e2\u20ac on the horizontal main menu, and click \u00e2\u20ac\u0153Show Competed Jobs\u00e2\u20ac: (Optional) On the \u00e2\u20ac\u0153Administration\u00e2\u20ac page check: \u00e2\u20ac\u201c Share printers connected to this system \u00e2\u20ac\u201c Allow remote administration \u00e2\u20ac\u201c Allow printing from the Internet This is a very basic setup of cups, but this is just enough at the beginning. If you want to know more about CUPS visit its official website or do some google (re)search.","title":"2. Installing and configuring CUPS"},{"location":"old/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#2-make-you-printer-available-for-air-print","text":"This step is incredibly easy, just one command: sudo apt-get install avahi-discover avahi-daemon I found some articles about \u00e2\u20ac\u0153cups airprint setup\u00e2\u20ac which are much complected, but somehow in my case it works with installing only the avahi damon and discover. Links: http://iain.polevaultweb.com/2014/03/setting-raspberry-pi-print-server-airprint-support/ http://www.lynsayshepherd.com/blog/2015/10/18/wireless-printingairprint-server-via-the-raspberry-pi-updated-guide/ https://wiki.debian.org/AirPrint NOTE: Of course you have to connect your RasPI to the same network which used by your Wifi, and connect your phone to it in order to make everything work fine.","title":"2. Make you printer available for Air print"},{"location":"old/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#3-add-your-printer-to-windows-10","text":"So I think if you configured a network printer you would like to use it from another system(s). If you don\u00e2\u20ac\u2122t share your printer via SAMBA you can use your printers URL to connect to it from Win10. You can check this URL by accessing the admin page of cups. Click on \u00e2\u20ac\u0153printers\u00e2\u20ac and select the printer you want to connect to: Copy your Printer URL to clipboard from your browser Address Line! For example: https://172.16.0.210:631/printers/rpi_samsung_ml-1510","title":"3. Add your printer to Windows 10"},{"location":"old/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#4-windows-setup","text":"On Windows system click \u00e2\u20ac\u0153Device and Printers\u00e2\u20ac in the control panel. Click Add printer Click The printer that I want isnt listed Click Select a shared printer by name\u00e2\u20ac and paste your printer URL: Click Next. If Windows can not connect to you printer, try without SSL (http). Select your printer\u00e2\u20ac\u2122s driver and click finish.","title":"4. Windows Setup:"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/","text":"Caution This page has been updated a long time ago. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example! Raspberry PI 3 As Wifi Range Extender \u00b6 TL;DR \u00b6 I have a Workshop in our backyard, and there are some ESPs inside. Unfortunately my router is far away from them and the Wifi connection often breaks. This is a real problem for me because the ESPs are controlling my lighting in the garden and it's a bit irritating when I cannot turn on the lights. (The ESPs automatically turn off all relay channels when the Wifi disconnects, so turning off is not an issue.) As I have wired LAN access (almost) everywhere in my house and even in my Workshop I was thinking about how to extend my Wifi range. OK. I know. There are a lot of possibilities to do it, but I wanted to choose the best and the most reliable way, and I wanted to use something I have already have. I use a Mikrotik router with a lot of switches all around the house. Since I don't like Wifi networks I'm trying to connect as much devices as I can wired to my network, but the ESPs needs Wifi connection. There is a Raspberry PI3 already running in the workshop so it is reasonable to use that. Of course the PI has Ethernet connection to my Mikrotik router. :) Update: I haven't published this post yet, and I've already had an update. To make it work was much easier than I thought. --> To make it work was much complicated then I've ever thought and took more days. :( As you will see I faced a lot of problems. But I want to believe that this post will be helpful. Install the necessary packages \u00b6 sudo apt-get install hostapd bridge-utils wicd wicd-cli wpasupplicant During the installation the wicd-daemon will ask you for the list of users who can use the wicd client. Users who should be able to run wicd clients need to be added to the group \"netdev\". You can modify these settings later by running dpkg-reconfigure wicd-daemon command. Configuring Bridge Interface \u00b6 My setup looks like that: auto lo iface lo inet loopback iface eth0 inet manual iface wlan0 inet manual auto br0 iface br0 inet dhcp bridge_ports eth0 bridge_stp off bridge_fd 0 bridge_maxwait 0 bridge_waitport 0 Now you should restart your PI, after that you can check your config: brctl show bridge name bridge id STP enabled interfaces br0 8000.b827eb26993d no eth0 Check Wifi Device & Configure Hostapd \u00b6 First it is recommended to check if your Wireless device supports AP mode or not. It is only necessary for example if you are using an RPI which is older than PI3. (RPI3 has built-n WiFi chip, which supports AP mode.) Check your interface list iw dev phy#0 Interface wlan0 ifindex 3 wdev 0x1 addr b8:27:eb:73:cc:68 type managed You can see I have only one interface: phy#0 . Here is an example when there are multiple interfaces: iw dev phy#1 Interface wlan1 ifindex 4 wdev 0x100000001 addr 00:e0:32:00:00:8b type managed channel 8 (2447 MHz), width: 20 MHz, center1: 2447 MHz phy#0 Interface wlan0 ifindex 3 wdev 0x1 addr b8:27:eb:f9:dd:be ssid Vinyo-Net type AP channel 2 (2417 MHz), width: 20 MHz, center1: 2417 MHz Check \"Supported interface modes\" iw phy phy0 info Wiphy phy0 max # scan SSIDs: 10 max scan IEs length: 2048 bytes Retry short limit: 7 Retry long limit: 4 Coverage class: 0 (up to 0m) Device supports T-DLS. Supported Ciphers: * WEP40 (00-0f-ac:1) * WEP104 (00-0f-ac:5) * TKIP (00-0f-ac:2) * CCMP (00-0f-ac:4) Available Antennas: TX 0 RX 0 Supported interface modes: * IBSS * managed * AP * P2P-client * P2P-GO * P2P-device Band 1: Capabilities: 0x1020 HT20 Static SM Power Save RX HT20 SGI No RX STBC Max AMSDU length: 3839 bytes DSSS/CCK HT40 ... ... As you can see, our built-in Wifi device supports the AP mode. (By running this command ( iw phy phy0 info ) you get a lot of information about your Wifi device.) Configure Hostapd I don't know why, but the /etc/hostapd directory does not contain default/sample configuration file, but you can find sample configuration files here: /usr/share/doc/hostapd/examples I used this command to get examples: gunzip -c /usr/share/doc/hostapd/examples/hostapd.conf.gz | less I advise you to use a different SSID firstly then you already have to test a vanilla setup. Here is working example file: ctrl_interface=/var/run/hostapd macaddr_acl=0 driver=nl80211 interface=wlan0 bridge=br0 country_code=HU hw_mode=g ieee80211n=1 channel=2 ssid=WS-TST-01 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_passphrase=12345678 wpa_key_mgmt=WPA-PSK wpa_pairwise=CCMP rsn_pairwise=CCMP Warning Use a channel which is different from the one your primary device uses. After the configuration file is created you can try to start hostapd. hostapd /etc/hostapd/hostapd.conf Configuration file: /etc/hostapd/hostapd.conf Failed to create interface mon.wlan0: -95 (Operation not supported) wlan0: interface state UNINITIALIZED->COUNTRY_UPDATE wlan0: Could not connect to kernel driver Using interface wlan0 with hwaddr b8:27:eb:73:cc:68 and ssid \"WS-TST-01\" wlan0: interface state COUNTRY_UPDATE->ENABLED wlan0: AP-ENABLED We get two error messages: Failed to create interface mon.wlan0: -95 (Operation not supported) wlan0: Could not connect to kernel driver I did a lot of Google searches, but failed to find any solution for this issue. Despite the failures the \"AP-ENABLED\" messages shows us that everything should work. Now you can try to connect your new Wifi AP: WS-TST-01 Enable Auto Start hostapd daemon By default the hostapd doesn't start at boot time. To enable it change this file: --- hostapd_orig 2017-08-15 11:14:58.673575799 +0200 +++ hostapd 2017-08-14 21:25:47.684546287 +0200 @@ -7,7 +7,7 @@ # file and hostapd will be started during system boot. An example configuration # file can be found at /usr/share/doc/hostapd/examples/hostapd.conf.gz # -DAEMON_CONF=\"\" +DAEMON_CONF=\"/etc/hostapd/hostapd.conf\" # Additional daemon options to be appended to hostapd command:- # -d show more debug messages (-dd for even more) At this point I was facing a very strange and serious issue. :( It took days to find a working solution or only a workaround instead. The problem: After restarting my PI everything seemed fine, but none of devices could see the SSID. But it could be solved with restarting the hostapd daemon. At the bottom of this post in the references section you can find some article which discuss similar issues, but without any \"real\" solution (only workarounds). (Example: control the hostapd daemon from interface config, ifup.d). Now I sharing you my experiences I got during the investigation, It may (or may not) be useful. You have to know I made uncountable tries to configure hostapd, dhcpcd, dhcp-client, bridge, etc, but none of them led to success. 1. Replace the original System V init service to systemd. By default the hostapd daemon is started by an init.d scripts (/etc/init.d/hostapd). I wanted to move the service start to the end of the boot process to make sure that every necessary service started before hostapd. So I wrote a custom systemd config file and removed it from /etc/rc3.d. My SystemD script: [Unit] Description=HOSTAPD Requires=multi-user.target network-online.target avahi-daemon.service smbd.service After=avahi-daemon.service smbd.service multi-user.target [Service] Type=forking GuessMainPID=yes ExecStart=/usr/sbin/hostapd -d -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf ExecStop=/usr/bin/kill -SIGINT $MAINPID PIDFile=/run/hostapd.pid Restart=always User=root [Install] WantedBy=multi-user.target You can check the boot order by issuing the command (after reboot): systemd-analyze plot >/tmp/plot3.svg This .svg file can be opened with any type of browser. I saw that the hostapd service was started almost at the end of the boot process (I think it was the penultimate one before systemd-update-utmp-runlevel.service and after the multi-user.target). But It did not solved my problem. 2. Restart hostapd daemon after the IP address bounded (br0) I was reading the log files a lot, and found that something happens after the boot process has been finished with the interfaces (br0,wlan0,eth0): In the log files you can see that the boot process has been finished at 11:09:11. Aug 26 11:09:11 ws-rpi3 systemd[1]: Startup finished in 3.918s (kernel) + 29.036s (userspace) = 32.954s. But after some seconds: Aug 26 11:09:14 ws-rpi3 kernel: [ 36.241794] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.242140] br0: port 1(eth0) entered disabled state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.342138] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.342435] br0: port 1(eth0) entered blocking state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.342446] br0: port 1(eth0) entered forwarding state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.474142] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.474380] br0: port 1(eth0) entered disabled state Aug 26 11:09:14 ws-rpi3 dhcpcd[1046]: dhcpcd not running Aug 26 11:09:14 ws-rpi3 kernel: [ 36.591789] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.592070] br0: port 1(eth0) entered blocking state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.592074] br0: port 1(eth0) entered forwarding state Aug 26 11:09:16 ws-rpi3 kernel: [ 38.073773] smsc95xx 1-1.1:1.0 eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1 Aug 26 11:09:16 ws-rpi3 dhcpcd[1051]: version 6.7.1 starting Aug 26 11:09:16 ws-rpi3 dhcpcd[1051]: eth0: interface not found or invalid Aug 26 11:09:16 ws-rpi3 dhcpcd[1051]: exited Aug 26 11:09:21 ws-rpi3 dhcpcd[1089]: dhcpcd not running Aug 26 11:09:21 ws-rpi3 kernel: [ 43.735165] brcmfmac: power management disabled Aug 26 11:09:22 ws-rpi3 dhcpcd[1113]: dhcpcd not running Aug 26 11:09:22 ws-rpi3 kernel: [ 43.866789] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:22 ws-rpi3 kernel: [ 43.866935] br0: port 1(eth0) entered disabled state Aug 26 11:09:22 ws-rpi3 kernel: [ 43.962974] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:22 ws-rpi3 kernel: [ 43.963452] br0: port 1(eth0) entered blocking state Aug 26 11:09:22 ws-rpi3 kernel: [ 43.963460] br0: port 1(eth0) entered forwarding state Aug 26 11:09:23 ws-rpi3 kernel: [ 45.581772] smsc95xx 1-1.1:1.0 eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1 This is the only possible reason I could find. For some reason the interfaces changed to down and up again. I run the ping command from a remote machine and during this period the packages were lost. (When I was fast I could see the SSID on my phone for some seconds before it disappeared.) Here I was facing another big issue. I could not find any errors in the log files or anything else on the RPI side with which the restart could have been triggered. This means that everything seemed fine on the RPI side, but the SSID was not visible. First I thought it is a good idea to restart hostapd daemon when the br0 get the IP address from the DHCP server (BOUND), but it wasn't. :( The dhcp client \"only\" renews the IP address. Only for information I paste here my dhcp related config ( /etc/dhcp/dhclient-enter-hooks.d/jvincze_custom ): #!/bin/sh case \"$reason\" in BOUND) echo \"[ $(date +%F\\ %T ) ] - ($0) DHCP REASON: $reason ($new_ip_address , $interface)\" >>/var/log/jvinczedhcp.log PID=$( ps aux|grep hostapd | grep -v grep | awk '{print $2}' ) if [ ! -z $PID ] then echo \"[ $(date +%F\\ %T ) ] - KILLING $PID pid\" >>/var/log/jvinczedhcp.log sleep 30 kill $PID fi ;; *) echo \"[ $(date +%F\\ %T ) ] - ($0) DHCP REASON: $reason ($new_ip_address , $interface)\" >>/var/log/jvinczedhcp.log ;; esac First I tried this (instead of killing the process): #[ ! -z $PID ] && while kill $PID 2> /dev/null; do sleep 1 ; done; #sleep 4 #/usr/sbin/hostapd -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf I did not like this, because this file contains the command line parameter of hostapd. It is not a \"real\" problem, but with this way this file has to be synchronized with the .service file. After I slept a bit I found out that I can kill the process if I use Restart=always and Type=forking in the .service file (systemd will restart the process when it's dead/killed/etc.) But nothing changed. :( It could be a solution to modify the script to restart hostapd daemon when ip address is renewed, but in case of long lease time I could lose the wifi connection for hours. (And I did not want to change a network parameter which affects my entire network and devices.) NOTE: After finding my final WA, I did not remove this script to make sure if the interface br0 get a new ip address hostapd will be restarted. (I don't know if it is necessary or not, but think this won't cause further problems.) If you have more than one interface using dhcp to assign IP address you can write an \"if\" condition to specify the interface to check. Example: if [ ! -z $PID ] || [ \"$interface\" == \"br0\" ] ; then 3. Another tries Start hostapd from systemd with this command line options: ExecStart=/usr/sbin/hostapd -d -S -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf After the RPI restarted, I stopped hostapd, backed-up/removed the hostapd.log files, and started the hostapd (again). Then I compared the two log files. You can download my diff file from here: LINK Example .svg file. LINK I tried to find something special to search in google, but all of my searches led to nothing. :( So I finally gave up searching in the log files. It is obvious that this behavior was caused by the interface changes after boot. Syslog: ... Aug 30 21:05:33 ws-rpi3 kernel: [ 44.726888] br0: port 2(wlan0) entered disabled state Aug 30 21:05:33 ws-rpi3 dhcpcd[1020]: dhcpcd not running Aug 30 21:05:33 ws-rpi3 kernel: [ 44.969033] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 30 21:05:33 ws-rpi3 kernel: [ 44.969368] br0: port 1(eth0) entered disabled state ... Of course I did a lot of search for syslog entries but I could not find any relating, or useful article. 4. Final Solution (WA) I tried a lot of things in order to make this work, but the only way is the \"delayed start\". :( I completely disabled the hostapd service and restarted the PI, waited some time (~1min) and started the hostapd daemon. Everything was fine. Now the \"only\" thing was to figuring out how to start hostapd service delayed. My first solution was this (.service file): [Service] ExecStartPre=/bin/sleep 30 It is a working but ugly workaround. With this method the whole boot process is delayed. :( And you can see in the boot order (.svg) that the hostapd starting process take +30s compared with the \"normal\" case. :( Finally I set up a systemd timer: hostapd.timer cat /lib/systemd/system/hostapd.timer [Unit] Description=Runs hostapd After=multi-user.target [Timer] OnBootSec=1min Unit=hostapd.service [Install] WantedBy=multi-user.target I think 1 minute enough delay after boot to start hostapd. hostapd.service cat /lib/systemd/system/hostapd.service [Unit] Description=HOSTAPD Requires=multi-user.target network-online.target avahi-daemon.service smbd.service After=avahi-daemon.service smbd.service multi-user.target sockets.targee [Service] Type=forking GuessMainPID=yes ExecStart=/usr/sbin/hostapd -d -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf #ExecStart=/usr/sbin/hostapd -d -S -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf #ExecStart=/usr/sbin/hostapd -t -B -f /var/log/hostapd.log /etc/hostapd/hostapd.conf ExecStop=/usr/bin/kill -SIGINT $MAINPID PIDFile=/run/hostapd.pid Restart=always User=root #[Install] #WantedBy=multi-user.target Commands: systemctl daemon-reload systemctl disable hostapd.service Removed symlink /etc/systemd/system/multi-user.target.wants/hostapd.service. systemctl enable hostapd.timer Created symlink from /etc/systemd/system/multi-user.target.wants/hostapd.timer to /lib/systemd/system/hostapd.timer. Final Thoughts \u00b6 When I started to write this post I thought configuring hostapd must consist of some easy steps. Actually it is right because configuring hostapd is pretty easy, I had(/have) problems with autostart on boot time. I have been running this setup since weeks without any problem. Maybe later when I have time for this, I'll continue the investigation, or maybe in the future there will be a new kernel and/or hostapd daemon with which this problem won't occur. If you have any idea how to solve this please let me a post on Disqus. :) Some Useful Commands \u00b6 Check Connected Devices hostapd_cli all_sta Check Connected Devices (only MAC addresses) hostapd_cli all_sta | grep -E '^([0-9|a-f|A-F]{2,2}\\:?){6,6}' Check Connected Devices & Query MAC address on MIKROTIK router for MAC in $(hostapd_cli all_sta | grep -E '^([0-9|a-f|A-F]{2,2}\\:?){6,6}'); do echo \"########## $MAC ##########\" ; ssh admin@172.16.0.1 \"ip dhcp-server lease print where mac-address=$MAC\" ; done ########## 60:01:94:08:31:48 ########## Flags: X - disabled, R - radius, D - dynamic, B - blocked # ADDRESS MAC-ADDRESS HO SER.. RA 0 172.20.0.15 60:01:94:08:31:48 NO def.. Check bridge interface brctl show bridge name bridge id STP enabled interfaces br0 8000.b827eb26993d no eth0 wlan0 NOTE: If the hostapd have been started successfully you have to see WLAN0 in the interface list. Analyze boot order systemd-analyze plot >/tmp/plot.svg You can open the created file in your browser. Enable packet forwarding by kernel echo 1 > /proc/sys/net/ipv4/ip_forward iptables --append FORWARD --in-interface eth1 -j ACCEPT Or permanently, add this line to /etc/sysctl.conf net.ipv4.ip_forward=1 Alternatively: sudo sysctl -w net.ipv4.ip_forward=1 Disable interface in dhcpcd.conf denyinterfaces wlan0 denyinterfaces br0 Disable ipv6 (sysctl.conf) net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1 Wifi Channel Setup (hostapd.conf) If you build a WiFi infrastructure you must use different channel in all devices. References: https://www.extremetech.com/computing/179344-how-to-boost-your-wifi-speed-by-choosing-the-right-channel https://www.hanselman.com/blog/ConfiguringTwoWirelessRoutersWithOneSSIDNetworkNameAtHomeForFreeRoaming.aspx From hostapd example conf: # Channel number (IEEE 802.11) # (default: 0, i.e., not set) # Please note that some drivers do not use this value from hostapd and the # channel will need to be configured separately with iwconfig. # # If CONFIG_ACS build option is enabled, the channel can be selected # automatically at run time by setting channel=acs_survey or channel=0, both of # which will enable the ACS survey based algorithm. channel=1 Bonus - Connect To Existing WiFi Network It can be useful when you have two WiFi adapters, and you want to use your raspberry to extend WiFi signal range without ETH connection. I haven't tested this, but I think based on this post and the linked articles it can be done easily. Install necessary packages apt-get install wpasupplicant wicd wicd-cli Search for Wifi Networks iwlist wlan1 scan Create Config File wpa_passphrase Wifi-NetWork 12345678 >wifi.config Test you connection wpa_supplicant -i wlan1 -D nl80211 -c wifi.config Edit Interface Config (/etc/network/interfaces) auto wlan1 allow-hotplug wlan1 iface wlan1 inet dhcp wpa-conf /root/wifi.config References: https://frillip.com/using-your-raspberry-pi-3-as-a-wifi-access-point-with-hostapd/ http://www.instructables.com/id/How-to-make-a-WiFi-Access-Point-out-of-a-Raspberry/ http://www.catonrug.net/2016/07/use-phone-tablet-as-raspberry-pi-3-wireless-screen-part-2.html https://www.raspberrypi.org/forums/viewtopic.php?t=141807 https://wiki.debian.org/BridgeNetworkConnections https://askubuntu.com/questions/617973/fatal-error-netlink-genl-genl-h-no-such-file-or-directory https://unix.stackexchange.com/questions/119209/hostapd-will-not-start-via-service-but-will-start-directly https://superuser.com/questions/676918/hostapd-requires-manual-restart-for-devices-to-connect https://learn.adafruit.com/setting-up-a-raspberry-pi-as-a-wifi-access-point/install-software","title":"Raspberry PI 3 As Wifi Range Extender"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#raspberry-pi-3-as-wifi-range-extender","text":"","title":"Raspberry PI 3 As Wifi Range Extender"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#tldr","text":"I have a Workshop in our backyard, and there are some ESPs inside. Unfortunately my router is far away from them and the Wifi connection often breaks. This is a real problem for me because the ESPs are controlling my lighting in the garden and it's a bit irritating when I cannot turn on the lights. (The ESPs automatically turn off all relay channels when the Wifi disconnects, so turning off is not an issue.) As I have wired LAN access (almost) everywhere in my house and even in my Workshop I was thinking about how to extend my Wifi range. OK. I know. There are a lot of possibilities to do it, but I wanted to choose the best and the most reliable way, and I wanted to use something I have already have. I use a Mikrotik router with a lot of switches all around the house. Since I don't like Wifi networks I'm trying to connect as much devices as I can wired to my network, but the ESPs needs Wifi connection. There is a Raspberry PI3 already running in the workshop so it is reasonable to use that. Of course the PI has Ethernet connection to my Mikrotik router. :) Update: I haven't published this post yet, and I've already had an update. To make it work was much easier than I thought. --> To make it work was much complicated then I've ever thought and took more days. :( As you will see I faced a lot of problems. But I want to believe that this post will be helpful.","title":"TL;DR"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#install-the-necessary-packages","text":"sudo apt-get install hostapd bridge-utils wicd wicd-cli wpasupplicant During the installation the wicd-daemon will ask you for the list of users who can use the wicd client. Users who should be able to run wicd clients need to be added to the group \"netdev\". You can modify these settings later by running dpkg-reconfigure wicd-daemon command.","title":"Install the necessary packages"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#configuring-bridge-interface","text":"My setup looks like that: auto lo iface lo inet loopback iface eth0 inet manual iface wlan0 inet manual auto br0 iface br0 inet dhcp bridge_ports eth0 bridge_stp off bridge_fd 0 bridge_maxwait 0 bridge_waitport 0 Now you should restart your PI, after that you can check your config: brctl show bridge name bridge id STP enabled interfaces br0 8000.b827eb26993d no eth0","title":"Configuring Bridge Interface"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#check-wifi-device-configure-hostapd","text":"First it is recommended to check if your Wireless device supports AP mode or not. It is only necessary for example if you are using an RPI which is older than PI3. (RPI3 has built-n WiFi chip, which supports AP mode.) Check your interface list iw dev phy#0 Interface wlan0 ifindex 3 wdev 0x1 addr b8:27:eb:73:cc:68 type managed You can see I have only one interface: phy#0 . Here is an example when there are multiple interfaces: iw dev phy#1 Interface wlan1 ifindex 4 wdev 0x100000001 addr 00:e0:32:00:00:8b type managed channel 8 (2447 MHz), width: 20 MHz, center1: 2447 MHz phy#0 Interface wlan0 ifindex 3 wdev 0x1 addr b8:27:eb:f9:dd:be ssid Vinyo-Net type AP channel 2 (2417 MHz), width: 20 MHz, center1: 2417 MHz Check \"Supported interface modes\" iw phy phy0 info Wiphy phy0 max # scan SSIDs: 10 max scan IEs length: 2048 bytes Retry short limit: 7 Retry long limit: 4 Coverage class: 0 (up to 0m) Device supports T-DLS. Supported Ciphers: * WEP40 (00-0f-ac:1) * WEP104 (00-0f-ac:5) * TKIP (00-0f-ac:2) * CCMP (00-0f-ac:4) Available Antennas: TX 0 RX 0 Supported interface modes: * IBSS * managed * AP * P2P-client * P2P-GO * P2P-device Band 1: Capabilities: 0x1020 HT20 Static SM Power Save RX HT20 SGI No RX STBC Max AMSDU length: 3839 bytes DSSS/CCK HT40 ... ... As you can see, our built-in Wifi device supports the AP mode. (By running this command ( iw phy phy0 info ) you get a lot of information about your Wifi device.) Configure Hostapd I don't know why, but the /etc/hostapd directory does not contain default/sample configuration file, but you can find sample configuration files here: /usr/share/doc/hostapd/examples I used this command to get examples: gunzip -c /usr/share/doc/hostapd/examples/hostapd.conf.gz | less I advise you to use a different SSID firstly then you already have to test a vanilla setup. Here is working example file: ctrl_interface=/var/run/hostapd macaddr_acl=0 driver=nl80211 interface=wlan0 bridge=br0 country_code=HU hw_mode=g ieee80211n=1 channel=2 ssid=WS-TST-01 auth_algs=1 ignore_broadcast_ssid=0 wpa=2 wpa_passphrase=12345678 wpa_key_mgmt=WPA-PSK wpa_pairwise=CCMP rsn_pairwise=CCMP Warning Use a channel which is different from the one your primary device uses. After the configuration file is created you can try to start hostapd. hostapd /etc/hostapd/hostapd.conf Configuration file: /etc/hostapd/hostapd.conf Failed to create interface mon.wlan0: -95 (Operation not supported) wlan0: interface state UNINITIALIZED->COUNTRY_UPDATE wlan0: Could not connect to kernel driver Using interface wlan0 with hwaddr b8:27:eb:73:cc:68 and ssid \"WS-TST-01\" wlan0: interface state COUNTRY_UPDATE->ENABLED wlan0: AP-ENABLED We get two error messages: Failed to create interface mon.wlan0: -95 (Operation not supported) wlan0: Could not connect to kernel driver I did a lot of Google searches, but failed to find any solution for this issue. Despite the failures the \"AP-ENABLED\" messages shows us that everything should work. Now you can try to connect your new Wifi AP: WS-TST-01 Enable Auto Start hostapd daemon By default the hostapd doesn't start at boot time. To enable it change this file: --- hostapd_orig 2017-08-15 11:14:58.673575799 +0200 +++ hostapd 2017-08-14 21:25:47.684546287 +0200 @@ -7,7 +7,7 @@ # file and hostapd will be started during system boot. An example configuration # file can be found at /usr/share/doc/hostapd/examples/hostapd.conf.gz # -DAEMON_CONF=\"\" +DAEMON_CONF=\"/etc/hostapd/hostapd.conf\" # Additional daemon options to be appended to hostapd command:- # -d show more debug messages (-dd for even more) At this point I was facing a very strange and serious issue. :( It took days to find a working solution or only a workaround instead. The problem: After restarting my PI everything seemed fine, but none of devices could see the SSID. But it could be solved with restarting the hostapd daemon. At the bottom of this post in the references section you can find some article which discuss similar issues, but without any \"real\" solution (only workarounds). (Example: control the hostapd daemon from interface config, ifup.d). Now I sharing you my experiences I got during the investigation, It may (or may not) be useful. You have to know I made uncountable tries to configure hostapd, dhcpcd, dhcp-client, bridge, etc, but none of them led to success. 1. Replace the original System V init service to systemd. By default the hostapd daemon is started by an init.d scripts (/etc/init.d/hostapd). I wanted to move the service start to the end of the boot process to make sure that every necessary service started before hostapd. So I wrote a custom systemd config file and removed it from /etc/rc3.d. My SystemD script: [Unit] Description=HOSTAPD Requires=multi-user.target network-online.target avahi-daemon.service smbd.service After=avahi-daemon.service smbd.service multi-user.target [Service] Type=forking GuessMainPID=yes ExecStart=/usr/sbin/hostapd -d -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf ExecStop=/usr/bin/kill -SIGINT $MAINPID PIDFile=/run/hostapd.pid Restart=always User=root [Install] WantedBy=multi-user.target You can check the boot order by issuing the command (after reboot): systemd-analyze plot >/tmp/plot3.svg This .svg file can be opened with any type of browser. I saw that the hostapd service was started almost at the end of the boot process (I think it was the penultimate one before systemd-update-utmp-runlevel.service and after the multi-user.target). But It did not solved my problem. 2. Restart hostapd daemon after the IP address bounded (br0) I was reading the log files a lot, and found that something happens after the boot process has been finished with the interfaces (br0,wlan0,eth0): In the log files you can see that the boot process has been finished at 11:09:11. Aug 26 11:09:11 ws-rpi3 systemd[1]: Startup finished in 3.918s (kernel) + 29.036s (userspace) = 32.954s. But after some seconds: Aug 26 11:09:14 ws-rpi3 kernel: [ 36.241794] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.242140] br0: port 1(eth0) entered disabled state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.342138] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.342435] br0: port 1(eth0) entered blocking state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.342446] br0: port 1(eth0) entered forwarding state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.474142] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.474380] br0: port 1(eth0) entered disabled state Aug 26 11:09:14 ws-rpi3 dhcpcd[1046]: dhcpcd not running Aug 26 11:09:14 ws-rpi3 kernel: [ 36.591789] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:14 ws-rpi3 kernel: [ 36.592070] br0: port 1(eth0) entered blocking state Aug 26 11:09:14 ws-rpi3 kernel: [ 36.592074] br0: port 1(eth0) entered forwarding state Aug 26 11:09:16 ws-rpi3 kernel: [ 38.073773] smsc95xx 1-1.1:1.0 eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1 Aug 26 11:09:16 ws-rpi3 dhcpcd[1051]: version 6.7.1 starting Aug 26 11:09:16 ws-rpi3 dhcpcd[1051]: eth0: interface not found or invalid Aug 26 11:09:16 ws-rpi3 dhcpcd[1051]: exited Aug 26 11:09:21 ws-rpi3 dhcpcd[1089]: dhcpcd not running Aug 26 11:09:21 ws-rpi3 kernel: [ 43.735165] brcmfmac: power management disabled Aug 26 11:09:22 ws-rpi3 dhcpcd[1113]: dhcpcd not running Aug 26 11:09:22 ws-rpi3 kernel: [ 43.866789] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:22 ws-rpi3 kernel: [ 43.866935] br0: port 1(eth0) entered disabled state Aug 26 11:09:22 ws-rpi3 kernel: [ 43.962974] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 26 11:09:22 ws-rpi3 kernel: [ 43.963452] br0: port 1(eth0) entered blocking state Aug 26 11:09:22 ws-rpi3 kernel: [ 43.963460] br0: port 1(eth0) entered forwarding state Aug 26 11:09:23 ws-rpi3 kernel: [ 45.581772] smsc95xx 1-1.1:1.0 eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1 This is the only possible reason I could find. For some reason the interfaces changed to down and up again. I run the ping command from a remote machine and during this period the packages were lost. (When I was fast I could see the SSID on my phone for some seconds before it disappeared.) Here I was facing another big issue. I could not find any errors in the log files or anything else on the RPI side with which the restart could have been triggered. This means that everything seemed fine on the RPI side, but the SSID was not visible. First I thought it is a good idea to restart hostapd daemon when the br0 get the IP address from the DHCP server (BOUND), but it wasn't. :( The dhcp client \"only\" renews the IP address. Only for information I paste here my dhcp related config ( /etc/dhcp/dhclient-enter-hooks.d/jvincze_custom ): #!/bin/sh case \"$reason\" in BOUND) echo \"[ $(date +%F\\ %T ) ] - ($0) DHCP REASON: $reason ($new_ip_address , $interface)\" >>/var/log/jvinczedhcp.log PID=$( ps aux|grep hostapd | grep -v grep | awk '{print $2}' ) if [ ! -z $PID ] then echo \"[ $(date +%F\\ %T ) ] - KILLING $PID pid\" >>/var/log/jvinczedhcp.log sleep 30 kill $PID fi ;; *) echo \"[ $(date +%F\\ %T ) ] - ($0) DHCP REASON: $reason ($new_ip_address , $interface)\" >>/var/log/jvinczedhcp.log ;; esac First I tried this (instead of killing the process): #[ ! -z $PID ] && while kill $PID 2> /dev/null; do sleep 1 ; done; #sleep 4 #/usr/sbin/hostapd -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf I did not like this, because this file contains the command line parameter of hostapd. It is not a \"real\" problem, but with this way this file has to be synchronized with the .service file. After I slept a bit I found out that I can kill the process if I use Restart=always and Type=forking in the .service file (systemd will restart the process when it's dead/killed/etc.) But nothing changed. :( It could be a solution to modify the script to restart hostapd daemon when ip address is renewed, but in case of long lease time I could lose the wifi connection for hours. (And I did not want to change a network parameter which affects my entire network and devices.) NOTE: After finding my final WA, I did not remove this script to make sure if the interface br0 get a new ip address hostapd will be restarted. (I don't know if it is necessary or not, but think this won't cause further problems.) If you have more than one interface using dhcp to assign IP address you can write an \"if\" condition to specify the interface to check. Example: if [ ! -z $PID ] || [ \"$interface\" == \"br0\" ] ; then 3. Another tries Start hostapd from systemd with this command line options: ExecStart=/usr/sbin/hostapd -d -S -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf After the RPI restarted, I stopped hostapd, backed-up/removed the hostapd.log files, and started the hostapd (again). Then I compared the two log files. You can download my diff file from here: LINK Example .svg file. LINK I tried to find something special to search in google, but all of my searches led to nothing. :( So I finally gave up searching in the log files. It is obvious that this behavior was caused by the interface changes after boot. Syslog: ... Aug 30 21:05:33 ws-rpi3 kernel: [ 44.726888] br0: port 2(wlan0) entered disabled state Aug 30 21:05:33 ws-rpi3 dhcpcd[1020]: dhcpcd not running Aug 30 21:05:33 ws-rpi3 kernel: [ 44.969033] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup Aug 30 21:05:33 ws-rpi3 kernel: [ 44.969368] br0: port 1(eth0) entered disabled state ... Of course I did a lot of search for syslog entries but I could not find any relating, or useful article. 4. Final Solution (WA) I tried a lot of things in order to make this work, but the only way is the \"delayed start\". :( I completely disabled the hostapd service and restarted the PI, waited some time (~1min) and started the hostapd daemon. Everything was fine. Now the \"only\" thing was to figuring out how to start hostapd service delayed. My first solution was this (.service file): [Service] ExecStartPre=/bin/sleep 30 It is a working but ugly workaround. With this method the whole boot process is delayed. :( And you can see in the boot order (.svg) that the hostapd starting process take +30s compared with the \"normal\" case. :( Finally I set up a systemd timer: hostapd.timer cat /lib/systemd/system/hostapd.timer [Unit] Description=Runs hostapd After=multi-user.target [Timer] OnBootSec=1min Unit=hostapd.service [Install] WantedBy=multi-user.target I think 1 minute enough delay after boot to start hostapd. hostapd.service cat /lib/systemd/system/hostapd.service [Unit] Description=HOSTAPD Requires=multi-user.target network-online.target avahi-daemon.service smbd.service After=avahi-daemon.service smbd.service multi-user.target sockets.targee [Service] Type=forking GuessMainPID=yes ExecStart=/usr/sbin/hostapd -d -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf #ExecStart=/usr/sbin/hostapd -d -S -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf #ExecStart=/usr/sbin/hostapd -t -B -f /var/log/hostapd.log /etc/hostapd/hostapd.conf ExecStop=/usr/bin/kill -SIGINT $MAINPID PIDFile=/run/hostapd.pid Restart=always User=root #[Install] #WantedBy=multi-user.target Commands: systemctl daemon-reload systemctl disable hostapd.service Removed symlink /etc/systemd/system/multi-user.target.wants/hostapd.service. systemctl enable hostapd.timer Created symlink from /etc/systemd/system/multi-user.target.wants/hostapd.timer to /lib/systemd/system/hostapd.timer.","title":"Check Wifi Device &amp; Configure Hostapd"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#final-thoughts","text":"When I started to write this post I thought configuring hostapd must consist of some easy steps. Actually it is right because configuring hostapd is pretty easy, I had(/have) problems with autostart on boot time. I have been running this setup since weeks without any problem. Maybe later when I have time for this, I'll continue the investigation, or maybe in the future there will be a new kernel and/or hostapd daemon with which this problem won't occur. If you have any idea how to solve this please let me a post on Disqus. :)","title":"Final Thoughts"},{"location":"old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#some-useful-commands","text":"Check Connected Devices hostapd_cli all_sta Check Connected Devices (only MAC addresses) hostapd_cli all_sta | grep -E '^([0-9|a-f|A-F]{2,2}\\:?){6,6}' Check Connected Devices & Query MAC address on MIKROTIK router for MAC in $(hostapd_cli all_sta | grep -E '^([0-9|a-f|A-F]{2,2}\\:?){6,6}'); do echo \"########## $MAC ##########\" ; ssh admin@172.16.0.1 \"ip dhcp-server lease print where mac-address=$MAC\" ; done ########## 60:01:94:08:31:48 ########## Flags: X - disabled, R - radius, D - dynamic, B - blocked # ADDRESS MAC-ADDRESS HO SER.. RA 0 172.20.0.15 60:01:94:08:31:48 NO def.. Check bridge interface brctl show bridge name bridge id STP enabled interfaces br0 8000.b827eb26993d no eth0 wlan0 NOTE: If the hostapd have been started successfully you have to see WLAN0 in the interface list. Analyze boot order systemd-analyze plot >/tmp/plot.svg You can open the created file in your browser. Enable packet forwarding by kernel echo 1 > /proc/sys/net/ipv4/ip_forward iptables --append FORWARD --in-interface eth1 -j ACCEPT Or permanently, add this line to /etc/sysctl.conf net.ipv4.ip_forward=1 Alternatively: sudo sysctl -w net.ipv4.ip_forward=1 Disable interface in dhcpcd.conf denyinterfaces wlan0 denyinterfaces br0 Disable ipv6 (sysctl.conf) net.ipv6.conf.all.disable_ipv6 = 1 net.ipv6.conf.default.disable_ipv6 = 1 net.ipv6.conf.lo.disable_ipv6 = 1 Wifi Channel Setup (hostapd.conf) If you build a WiFi infrastructure you must use different channel in all devices. References: https://www.extremetech.com/computing/179344-how-to-boost-your-wifi-speed-by-choosing-the-right-channel https://www.hanselman.com/blog/ConfiguringTwoWirelessRoutersWithOneSSIDNetworkNameAtHomeForFreeRoaming.aspx From hostapd example conf: # Channel number (IEEE 802.11) # (default: 0, i.e., not set) # Please note that some drivers do not use this value from hostapd and the # channel will need to be configured separately with iwconfig. # # If CONFIG_ACS build option is enabled, the channel can be selected # automatically at run time by setting channel=acs_survey or channel=0, both of # which will enable the ACS survey based algorithm. channel=1 Bonus - Connect To Existing WiFi Network It can be useful when you have two WiFi adapters, and you want to use your raspberry to extend WiFi signal range without ETH connection. I haven't tested this, but I think based on this post and the linked articles it can be done easily. Install necessary packages apt-get install wpasupplicant wicd wicd-cli Search for Wifi Networks iwlist wlan1 scan Create Config File wpa_passphrase Wifi-NetWork 12345678 >wifi.config Test you connection wpa_supplicant -i wlan1 -D nl80211 -c wifi.config Edit Interface Config (/etc/network/interfaces) auto wlan1 allow-hotplug wlan1 iface wlan1 inet dhcp wpa-conf /root/wifi.config References: https://frillip.com/using-your-raspberry-pi-3-as-a-wifi-access-point-with-hostapd/ http://www.instructables.com/id/How-to-make-a-WiFi-Access-Point-out-of-a-Raspberry/ http://www.catonrug.net/2016/07/use-phone-tablet-as-raspberry-pi-3-wireless-screen-part-2.html https://www.raspberrypi.org/forums/viewtopic.php?t=141807 https://wiki.debian.org/BridgeNetworkConnections https://askubuntu.com/questions/617973/fatal-error-netlink-genl-genl-h-no-such-file-or-directory https://unix.stackexchange.com/questions/119209/hostapd-will-not-start-via-service-but-will-start-directly https://superuser.com/questions/676918/hostapd-requires-manual-restart-for-devices-to-connect https://learn.adafruit.com/setting-up-a-raspberry-pi-as-a-wifi-access-point/install-software","title":"Some Useful Commands"}]}