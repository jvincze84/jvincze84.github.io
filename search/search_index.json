{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome Here","text":""},{"location":"#new-articles","title":"New Articles","text":"<p>{{ blog_content }}</p>"},{"location":"#old-contents-warning","title":"Old Contents Warning","text":"<p>All of the old contents can be found here under the \"Old Blog Conetent\" menu. But please aware that these contents are really old, and I keep them only for examples.  I think most of the tutorials could not be followed step by step, because a lot of things has been changed since these posts had been written.</p>"},{"location":"Tips_And_Tricks/","title":"Tips And Tricks 2024a","text":"<p>In this page I will share you some random tips and tricks I use in my daily life, and I will update this post frequently. Some of them will be commented, but not all.</p>"},{"location":"Tips_And_Tricks/#generate-random-strings","title":"Generate random strings","text":"<p><pre><code>tr -cd '[:alnum:]' &lt; /dev/./urandom | fold -w12 | head -n4\n</code></pre> You can use these string for example as random generated passwords.</p>"},{"location":"Tips_And_Tricks/#run-simple-python-http-server","title":"Run simple python http server","text":"<pre><code>`nohup python -m SimpleHTTPServer 8888 &gt;&gt;../access.log &amp;`\n</code></pre> <p>This will create a very simple http server on port 8888.  Useful for shearing files quickly and easily over http protocol.</p>"},{"location":"Tips_And_Tricks/#poor-mans-vpn-with-sshuttle","title":"Poor man's VPN with sshuttle","text":"<pre><code>sshuttle -r [USER]@[HOSTNAME] 0.0.0.0/0 --dns -v\n</code></pre>"},{"location":"Tips_And_Tricks/#split-mp3-into-equal-time-length-slices","title":"Split MP3 into equal time length slices","text":"<p>Install required package:</p> <pre><code>apt-get install poc-streamer\n</code></pre> <p>Split into 3mins slices:</p> <p><pre><code>mp3splt  -t  3.00  [MP3 FILE] -o @n\n</code></pre> Output will be: 01.mp3, 02.mp3, 04.mp3 . . . .</p>"},{"location":"Tips_And_Tricks/#convert-video-to-1280x-lower-quality","title":"Convert  Video to 1280X (lower quality)","text":"<p>It us useful when you want to convert your video files to lower quality. Command:</p> <pre><code>avconv -y -i $MOVIE -vf \"scale=1280:trunc(ow/a/2)*2\" -vcodec libx264 -acodec libmp3lame $NEWNAME\n</code></pre> <p>Where:</p> <ul> <li>-y overwrite output files</li> <li>-i input files</li> <li>-vf scale=1280:trunc(ow/a/2)*2\" --&gt; Keep original ratio, and avoid \"height not divisible by 2\" error message.</li> <li>-vcodec libx264 --&gt; video codec</li> <li>-acodec libmp3lame --&gt; audio codec</li> </ul> <p>You can replace \"1280\" to any other values which is divisible by 2. (640,480,etc...)</p>"},{"location":"Tips_And_Tricks/#lua-delay-function","title":"LUA - delay function","text":"<p>Sometimes you have to use delay before the next function, and in this situation can be useful this little function. <pre><code>function sleepAndContinueWithCommand(command)\n    print(\"sleepAndContinueWithCommand Function\")\n    tmr.alarm(6,2000,tmr.ALARM_SINGLE, function()\n        command()\n   end) \n</code></pre></p> <p>How to call it? <code>sleepAndContinueWithCommand(FUNCTION_NAME)</code></p>"},{"location":"Tips_And_Tricks/#modify-post-width-of-ghost-blog","title":"Modify Post Width of Ghost Blog","text":"<p>Change <code>max-width</code> in this section <pre><code>/* Every post, on every page, gets this style on its &lt;article&gt; tag */\n.post {\n    position: relative;\n    width: 80%;\n    max-width: 1000px;\n    margin: 4rem auto;\n    padding-bottom: 4rem;\n    border-bottom: #EBF2F6 1px solid;\n    word-wrap: break-word;\n}\n</code></pre></p>"},{"location":"Tips_And_Tricks/#limit-oprengepi-cpu-cores-enabledisable-cores","title":"Limit OprengePI CPU cores (enable/disable cores)","text":"<pre><code>echo 1 &gt;/sys/devices/system/cpu/cpu0/online\necho 0 &gt;/sys/devices/system/cpu/cpu1/online\necho 0 &gt;/sys/devices/system/cpu/cpu2/online\necho 0 &gt;/sys/devices/system/cpu/cpu3/online\n</code></pre> <p>Check CPU temperature:</p> <p><code>/sys/devices/virtual/thermal/thermal_zone0/temp</code></p>"},{"location":"Tips_And_Tricks/#tar-fromto-remote-machine","title":"tar from/to remote machine","text":"<p>tar from remote machine: <pre><code>ssh root@172.16.0.240 \"tar cfz - /etc /opt\" &gt;output.tar.gz\n</code></pre></p> <p>tar to remote machine: <pre><code>tar cvf - *.sh | ssh vinyo@172.16.0.240 \"cat  &gt;~/test.tar.gz\"\n</code></pre></p> <p>untar from remote machine <pre><code>ssh vinyo@172.16.0.240 \"cat ~/test.tar.gz\" | tar xv  #OR\nssh vinyo@172.16.0.240 \"cat ~/test.tar.gz\" | tar xvf - \n</code></pre></p> <p>untar to remote machine <pre><code>cat test.tar.gz | ssh 172.16.0.250 \"cd /home/vinyo/temp ; tar xv\" #OR\ncat test.tar.gz | ssh 172.16.0.250 \"cd /home/vinyo/temp ; tar xvf -\"\n</code></pre></p>"},{"location":"Tips_And_Tricks/#raspberry-prevent-to-sleep-wifi","title":"Raspberry - prevent to sleep Wifi","text":"<p>Find your Wifi chip module: <code>ls -la /sys/module/</code> Mine is:  <code>/sys/module/8189es/</code> Check power management status: <code>cat /sys/module/8189es/parameters/rtw_power_mgnt</code>  If it is equal to 0 then you are OK. If not, create a file something like this:</p> <pre><code>cat /etc/modprobe.d/8189es.conf \noptions 8189es rtw_power_mgnt=0 rtw_enusbss=0\n</code></pre> <p>Restart.</p>"},{"location":"Tips_And_Tricks/#rename-openvz-container","title":"Rename OpenVZ container","text":"<pre><code>vzctl set [CTID] --hostname [NEW HOSTNAME] --save\n</code></pre>"},{"location":"Tips_And_Tricks/#unzip-each-zip-file-to-separate-directory","title":"Unzip Each .zip File To Separate Directory","text":"<p>You have to be in the directory which contains the .zip files.</p> <pre><code>for Z in $( find . -type f -name '*.zip') ; do B=$( basename $Z ) ; F=${B%.*} ; mkdir $F ; unzip $Z -d $F ; done\n</code></pre>"},{"location":"Tips_And_Tricks/#read-parameter-file-in-linux-shell-bash","title":"Read Parameter File In Linux Shell (bash)","text":"<p><pre><code>while IFS='=' read -r key value\ndo\n    echo \"... $key='$value'\"\n    eval \"$key='$value'\"\ndone &lt; $PARAM_FILE\n</code></pre> Where:</p> <ul> <li><code>$PARAM_FILE</code> --&gt; File which contains the paramter and values.</li> <li><code>IFS='='</code> Internal Field Separator. Parameter names and Values are separated by <code>=</code> sign.  Example: <code>appname=weblogic</code></li> <li><code>eval \"$key='$value'\"</code> --&gt; This will create system environment. </li> </ul>"},{"location":"Tips_And_Tricks/#some-linux-console-fun","title":"Some Linux Console Fun","text":"<p>Try them out, if you are brave enough. :)</p> <pre><code>apt-get moo\napt-get install sl  \napt-get install furtune\napt-get install cowsay\napt-get install figlet\n</code></pre> <p>Examples:</p> <pre><code>sl\n\nfortune | cowsay\n\nfiglet \"Hello\"\n</code></pre>"},{"location":"Tips_And_Tricks/#run-command-without-x-display","title":"Run command without X display","text":"<p>This method can be useful when you want to run a command (which needs X11 display), but X11 display isn't running on your system.  Typical error message: <code>failed to commit changes to dconf: Cannot autolaunch D-Bus without X11 $DISPLAY</code> The solution is: xvfb-run</p> <pre><code>xvfb-run  -  run  specified  X  client or command in a virtual X server environment\n</code></pre> <p>Install:</p> <pre><code>apt-cache search xvfb-run\nxvfb - Virtual Framebuffer 'fake' X server\n\napt-get install xvfb\n</code></pre> <p>How to use it? <pre><code>xvfb-run --server-args=\"-screen 0, 1024x768x24\" /usr/bin/ssconvert --export-type=Gnumeric_Excel:excel_dsf 20161105_090001.html 20161105_090001.xls\n</code></pre></p> <p>Or from a shell script: <pre><code>xvfb-run --server-args=\"-screen 0, 1024x768x24\" /usr/bin/wkhtmltopdf $*\n</code></pre></p> <p>There are 2 typical command I'm using with xvfb-run: <code>ssconvert</code> and <code>wkhtmltopdf</code></p>"},{"location":"Tips_And_Tricks/#simple-image-viewer-for-linux","title":"Simple Image Viewer For Linux","text":"<p>feh -- image viewer and cataloguer</p>"},{"location":"Tips_And_Tricks/#failed-to-install-cisco-anyconnect-on-xubuntu-1604","title":"Failed To Install Cisco AnyConnect on Xubuntu 16.04","text":"<p>Error message: <code>Failed to start vpnagentd.service: Unit vpnagentd.service not found.</code> </p> <p>Solution: <pre><code>apt install network-manager-openconnect\nsystemctl daemon-reload\n</code></pre></p> <p>Then restart the install. REFERENCE:</p> <p>https://technicalsanctuary.wordpress.com/2016/05/28/installing-cisco-anyconnect-vpn-on-ubuntu-16-04/</p>"},{"location":"Tips_And_Tricks/#configure-extra-mouse-buttons-under-linux","title":"Configure Extra Mouse Buttons Under Linux","text":"<p>This solution is tested on Linux Mint (Sarah) and Xubuntu 16.04. So I have a Logitech M505 mouse and I love using the vertical scroll button to minimize and maximize the active window. I have never used these buttons according to its original function, they were always configured to minimize and maximize Window.</p>"},{"location":"Tips_And_Tricks/#what-you-need-to-install","title":"What you need to install?","text":"<ul> <li>xbindkeys</li> <li>xvkbd</li> <li>xdotool</li> <li>wmctrl  </li> </ul> <p>Install them with one command: </p> <pre><code>sudo apt install xbindkeys xvkbd xdotool wmctrl\n</code></pre>"},{"location":"Tips_And_Tricks/#create-sample-configuration-file","title":"Create sample configuration file","text":"<pre><code>xbindkeys -d &gt; ~/.xbindkeysrc\n</code></pre> <p>This will create a sample configuration file in your home directory.</p> <p>Please remove the \"Examples of commands:\" section from this file to avoid furtherer conflicts.</p> <pre><code>\"xbindkeys_show\" \n  control+shift + q\n</code></pre>"},{"location":"Tips_And_Tricks/#determine-the-id-of-the-buttons-you-want-to-use","title":"Determine the ID of the buttons you want to use","text":"<p>It is very simple. Just run <code>xev</code> command.</p> <p>For example my left button code (button 1):</p> <pre><code>ButtonPress event, serial 37, synthetic NO, window 0x4800001,\n    root 0xc5, subw 0x0, time 7138218, (103,85), root:(965,1564),\n    state 0x10, button 1, same_screen YES\n\nButtonRelease event, serial 37, synthetic NO, window 0x4800001,\n    root 0xc5, subw 0x0, time 7138264, (103,85), root:(965,1564),\n    state 0x110, button 1, same_screen YES\n</code></pre> <p>Example with <code>grep</code> to easier determine button ID:</p> <pre><code>xev | egrep -o 'button [0-9]{1,2}'\n</code></pre>"},{"location":"Tips_And_Tricks/#configure-xbindkeysrc","title":"Configure <code>.xbindkeysrc</code>","text":"<p>So in my case I want to configure only two buttons:</p> <ul> <li>Left Scrolling for minimize window</li> <li>Right Scrolling for maximize window</li> </ul> <p>I had to add only these two section to .xbindkeysrc:</p> <ul> <li>Minimize (Button 11)</li> </ul> <p><pre><code>\"xdotool getactivewindow windowminimize\"\nb:11\n</code></pre> * Maximize (button 12)</p> <pre><code>\"wmctrl -r :ACTIVE: -b toggle,maximized_vert,maximized_horz\"\nb:12\n</code></pre> <p>As you can see we had to use two different command: <code>xdotool</code> and <code>wmctrl</code>.</p> <p>If you want to use your buttons for any other activity I'm pretty sure that after some googleing you will find your solution.</p>"},{"location":"Tips_And_Tricks/#case-insensitive-search-in-oracle-db","title":"Case Insensitive Search In Oracle DB","text":"<pre><code>alter session set NLS_COMP=ANSI;\nalter session set NLS_SORT=BINARY_CI;\n</code></pre> <p>REFERENCE: </p> <p>http://stackoverflow.com/questions/1031844/oracle-db-how-can-i-write-query-ignoring-case</p>"},{"location":"Tips_And_Tricks/#change-date-timestamp-format-in-oracle-db","title":"Change Date &amp; Timestamp Format In Oracle DB","text":"<pre><code>alter session set NLS_DATE_FORMAT='yyyy-mm-dd HH24:mi:ss';\nALTER SESSION SET NLS_TIMESTAMP_FORMAT='yyyy-mm-dd HH24:mi:ss';\n</code></pre>"},{"location":"Tips_And_Tricks/#redirect-all-output-stderr-stdout-to-a-file","title":"Redirect all output (stderr, stdout) to a file","text":"<pre><code>#!/bin/bash\n\nLOG=\"[LOG file location]\"\nexec &gt;&gt; $LOG 2&gt;&amp;1\n...\n...\n...\n</code></pre>"},{"location":"Tips_And_Tricks/#redirect-nohup-output","title":"Redirect nohup output","text":"<p>You can redirect stdout and stderr to differrent files or into the same file.</p> <p>Examples:</p> <ol> <li><code>nohup ./program &gt;stdout.log 2&gt;stderr.log</code></li> <li><code>nohup ./progrem &gt;stoutAndStderr.log 2&gt;&amp;1</code></li> <li><code>abbreviated syntax</code></li> </ol> <p>nohup command &gt; output-$(date +%Y%m%d_%H%M%S).log &amp;</p> <p>Example startup script for OpenHAB:</p> <p><code>cat start-daemon.sh</code></p> <pre><code>#!/bin/sh\n...\n...\n...\n\necho Launching the openHAB runtime...\nnohup java \\\n    -Dosgi.clean=true \\\n    -Declipse.ignoreApp=true \\\n    -Dosgi.noShutdown=true  \\\n    -Djetty.port=$HTTP_PORT  \\\n    -Djetty.port.ssl=$HTTPS_PORT \\\n    -Djetty.home=.  \\\n    -Dlogback.configurationFile=configurations/logback.xml \\\n    -Dfelix.fileinstall.dir=addons -Dfelix.fileinstall.filter=.*\\\\.jar \\\n    -Djava.library.path=lib \\\n    -Djava.security.auth.login.config=/opt/openhab/runtime/distribution-1.8.3-runtime/etc/login.conf \\\n    -Dorg.quartz.properties=./etc/quartz.properties \\\n    -Dequinox.ds.block_timeout=240000 \\\n    -Dequinox.scr.waitTimeOnBlock=60000 \\\n    -Dfelix.fileinstall.active.level=4 \\\n    -Djava.awt.headless=true \\\n    -jar $cp $* \\\n    -console 9898 &gt;nohup-$(date +%Y%m%d_%H%M%S).out &amp;\n</code></pre>"},{"location":"new-file/","title":"Ez egy uj test file","text":""},{"location":"new-file/#proba3","title":"Proba3","text":"<pre><code>ls -al\nfind . -type f -name 'lol'\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/","title":"Install A Single Node Kubernetes \"Cluster\"","text":"<p>In this article we will install a single node kubernetes cluster on Debian 11 (bullseye). I will walk through step by step all the commands and configurations.</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#prerequisite","title":"Prerequisite","text":"<p>I'm using a really old system for this demonstration:</p> <ul> <li>CPU: Intel(R) Core(TM)2 CPU 6400  @ 2.13GHz (Dell Optiplex 745)</li> <li>Mem: 4 GB</li> <li>Disk: 500 GB HDD</li> <li>OS: Debian GNU/Linux 11 (bullseye)</li> </ul> <p>After successfully installation of my Debian system, install some necessary tools:</p> <pre><code>apt-get install vim mc net-tools sudo jq\n</code></pre> <ul> <li>(Optional) Change sudoers file:</li> </ul> <pre><code>--- /etc/sudoers-orig 2021-10-11 10:14:00.276397052 +0200\n+++ /etc/sudoers  2021-10-11 10:14:20.832894911 +0200\n@@ -20,7 +20,7 @@\n root ALL=(ALL:ALL) ALL\n\n # Allow members of group sudo to execute any command\n-%sudo  ALL=(ALL:ALL) ALL\n+%sudo  ALL=(ALL:ALL) NOPASSWD:ALL\n\n # See sudoers(5) for more information on \"@include\" directives:\n</code></pre> <ul> <li>Add user to <code>sudo</code> group (<code>/usr/sbin/visudo</code>)</li> </ul> <pre><code>/usr/sbin/usermod -aG sudo kube\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#install-container-runtime","title":"Install Container Runtime","text":"<p>I always recommend to follow the official installation guide: https://kubernetes.io/docs/setup/production-environment/container-runtimes Everything is well docomented and easy to follow. </p> <p>Kubernetes will leave Docker support so we will use Containerd as container runtime. </p> <p>Info</p> <p>Read more about Docker vs Kubernetes: https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</p> <p>Before you start you may want to check the official Docker installation page: https://docs.docker.com/engine/install/debian/</p> <p>Info</p> <p>I know that I post a lot of link as references, but it is really important to understand how important is it to always read the official documentation. Almost all these installation steps are copy-pasted from the official sites. </p> <ul> <li>Install requirements</li> </ul> <pre><code>sudo bash\n\napt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n</code></pre> <ul> <li>Add GPG key</li> </ul> <pre><code>curl -fsSL https://download.docker.com/linux/debian/gpg |  gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\" |  tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <ul> <li>Install Docker &amp; containerd</li> </ul> <pre><code>apt-get update\napt-get install docker-ce docker-ce-cli containerd.io\n</code></pre> <ul> <li>Check the installtion</li> </ul> <pre><code>docker info\ndocker run hello-world\n</code></pre> <ul> <li>Prepare Containerd for Kubernetes</li> </ul> <pre><code>cat &lt;&lt;EOF |  tee /etc/modules-load.d/containerd.conf\noverlay\nbr_netfilter\nEOF\n</code></pre> <pre><code>modprobe overlay\nmodprobe br_netfilter\n</code></pre> <pre><code>cat &lt;&lt;EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\nsysctl --system\n\nmkdir -p /etc/containerd\ncontainerd config default | tee /etc/containerd/config.toml\n\n\nsystemctl restart containerd\nsystemctl status containerd\n</code></pre> <ul> <li>Using the systemd cgroup drive</li> </ul> <pre><code>cp /etc/containerd/config.toml /etc/containerd/config.toml-orig\n</code></pre> <p>Edit the <code>/etc/containerd/config.toml</code> file</p> <pre><code>--- /etc/containerd/config.toml-orig  2021-10-11 10:33:39.603577510 +0200\n+++ /etc/containerd/config.toml 2021-10-11 10:35:01.753393462 +0200\n@@ -94,6 +94,7 @@\n           privileged_without_host_devices = false\n           base_runtime_spec = \"\"\n           [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n+            SystemdCgroup = true\n     [plugins.\"io.containerd.grpc.v1.cri\".cni]\n       bin_dir = \"/opt/cni/bin\"\n       conf_dir = \"/etc/cni/net.d\"\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#fix-crictl-error","title":"Fix <code>crictl</code> error","text":"<pre><code>crictl ps\nFATA[0010] failed to connect: failed to connect: context deadline exceeded\n</code></pre> <p>Fix:</p> <pre><code>cat &lt;&lt;EOF&gt;/etc/crictl.yaml\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: false\nEOF\n</code></pre> <p>Reference: https://github.com/cri-o/cri-o/issues/1922#issuecomment-828275332</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#install-kubernetes","title":"Install Kubernetes","text":"<p>Follow the steps described here: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#installing-kubeadm","title":"Installing kubeadm","text":"<pre><code>cat &lt;&lt;EOF |  tee /etc/modules-load.d/k8s.conf\nbr_netfilter\nEOF\n\ncat &lt;&lt;EOF |  tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\n\nsysctl --system\n</code></pre> <pre><code>apt-get update\napt-get install -y apt-transport-https ca-certificates curl\n\n# The URL has been changed! curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\ncurl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key |  gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg\n\n# Deprecated : echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" |  tee /etc/apt/sources.list.d/kubernetes.list\necho 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' |  tee /etc/apt/sources.list.d/kubernetes.list\n\napt-get update\n</code></pre> <p>Warning</p> <p>Location of the keyring and the source has been changed. If you need older version please modify the url accordingly. Example: <code>deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.27/deb/ /'</code></p> <p>Now we do some extra steps before installing the kubeadm. In the world of Kubernetes it is important to install the same version of kubeadm kubelet and kubectl. So fist we check the avaiable versions:</p> <p>Command<pre><code># apt-cache madison kubeadm | egrep '(1.22|1.21)'\napt-cache madison kubeadm\n</code></pre> Output<pre><code>   kubeadm | 1.28.2-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.1-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n   kubeadm | 1.28.0-1.1 | https://pkgs.k8s.io/core:/stable:/v1.28/deb  Packages\n</code></pre></p> <p>We won't install the latest version in order to be able to show you an update process as well.</p> <pre><code>apt-get install -y kubelet=1.28.1-1.1  kubeadm=1.28.1-1.1  kubectl=1.28.1-1.1 \napt-mark hold kubelet kubeadm kubectl\n</code></pre> <p>Info</p> <p>We don't want to update kubeadm, kubeclt and kubelet with system (os) updates (apt-get update &amp; upgrade). Kubernetes update has its own process. Always be careful when update you base system, and never update these packages alongside with the underlying os.</p> <p>Check the installed version Command<pre><code>kubeadm version -o yaml\n</code></pre> Output<pre><code>clientVersion:\n  buildDate: \"2023-08-24T11:21:51Z\"\n  compiler: gc\n  gitCommit: 8dc49c4b984b897d423aab4971090e1879eb4f23\n  gitTreeState: clean\n  gitVersion: v1.28.1\n  goVersion: go1.20.7\n  major: \"1\"\n  minor: \"28\"\n  platform: linux/amd64\n</code></pre></p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#init-the-cluster","title":"Init the cluster","text":"<p>Check the breif help of the <code>kubeadm init</code> command:</p> <pre><code>kubeadm init --help\n</code></pre> <p>Caution</p> <p>Disable SWAP before you start, otherwise you will get this error: <code>ERROR Swap]: running with swap on is not supported. Please disable swap</code> To do this remove the corresponding line from <code>/etc/fstab</code> and run <code>swapoff --all</code> command.</p> <p>Options</p> <ul> <li><code>--cri-socket /var/run/containerd/containerd.sock</code> --&gt; We want to use Containerd as container runtime insted of the default docker.</li> <li><code>--service-cidr 10.22.0.0/16</code> and  <code>--pod-network-cidr 10.23.0.0/16</code> --&gt; Really important to size well your internal Kubernets network. Be sure that none of these IP address ranges don't overlap your phisical network, VPN connection or each other. Since this is only a demo system it will be enough about 250 IP address for PODS and Services. </li> </ul> <p>Warning</p> <p>W0914 09:42:57.948845    4568 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme \"unix\" to the \"criSocket\" with value \"/var/run/containerd/containerd.sock\". Please update your configuration!</p> <p>Command<pre><code>kubeadm init \\\n--cri-socket unix:///var/run/containerd/containerd.sock \\\n--service-cidr 10.22.0.0/16 \\\n--pod-network-cidr 10.23.0.0/16\n</code></pre> Output<pre><code>[init] Using Kubernetes version: v1.28.2\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\nW0914 09:45:09.492639    6161 checks.go:835] detected that the sandbox image \"registry.k8s.io/pause:3.6\" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \"registry.k8s.io/pause:3.9\" as the CRI sandbox image.\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local singlek8s] and IPs [10.22.0.1 172.16.1.70]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [localhost singlek8s] and IPs [172.16.1.70 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [localhost singlek8s] and IPs [172.16.1.70 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 4.002225 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node singlek8s as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node singlek8s as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: m8tywn.9f3xegmdoa30d8v4\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 172.16.1.70:6443 --token m8tywn.9f3xegmdoa30d8v4 \\\n        --discovery-token-ca-cert-hash sha256:4ee5a244df12c803c78ba4bf55518d6c0f7ef84e655d4bce1cb40f8c967d60c2\n</code></pre></p> <p>Beleve or not our Single node Kubernetes cluster is almost ready. :)</p> <p>Check it:</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\nkubectl get nodes -o wide\n</code></pre> <p>Output looks like this: <pre><code>NAME        STATUS   ROLES                  AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME\nsinglek8s   Ready    control-plane,worker   21m   v1.28.1   172.16.1.70   &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.1.0-12-amd64   containerd://1.6.22\n</code></pre></p> <p>It is beutiful, isn't it?</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#why-my-node-is-in-notready-state","title":"Why my node is in <code>NotReady</code> state?","text":"<p>I can say that this behavior is normal in case of newly installed Kubernetes cluster. Check the reaseon:</p> <p>Command<pre><code>kubectl get pods --all-namespaces -o wide\n</code></pre> Output<pre><code>NAMESPACE     NAME                                READY   STATUS    RESTARTS        AGE    IP            NODE        NOMINATED NODE   READINESS GATES\nkube-system   coredns-5dd5756b68-756n6            0/1     Pending   0               18s    &lt;none&gt;        &lt;none&gt;      &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-5dd5756b68-l82qm            0/1     Pending   0               18s    &lt;none&gt;        &lt;none&gt;      &lt;none&gt;           &lt;none&gt;\nkube-system   etcd-singlek8s                      1/1     Running   3 (2m20s ago)   106s   172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-apiserver-singlek8s            1/1     Running   4 (59s ago)     106s   172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-controller-manager-singlek8s   1/1     Running   4 (78s ago)     106s   172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-rkqnk                    1/1     Running   1 (25s ago)     89s    172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-scheduler-singlek8s            1/1     Running   4 (78s ago)     106s   172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>You can see that the coredns pods are in pending state. These pods are responsible for internal DNS queries inside the Cluster Network. What should be the problem? We don't have any network plugin installed in the cluster....</p> <p>Links: </p> <ul> <li>https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/</li> <li>https://kubevious.io/blog/post/comparing-kubernetes-container-network-interface-cni-providers</li> </ul> <p>As you can see that there are a lot of varions of network plugins. For this little home environment I chose weave: https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#install</p> <p>It is really simple to install, can be achieved with only one command:</p> <p>Command<pre><code># Depreaceted: kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\nkubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre> Output<pre><code>serviceaccount/weave-net created\nclusterrole.rbac.authorization.k8s.io/weave-net created\nclusterrolebinding.rbac.authorization.k8s.io/weave-net created\nrole.rbac.authorization.k8s.io/weave-net created\nrolebinding.rbac.authorization.k8s.io/weave-net created\ndaemonset.apps/weave-net created\n</code></pre></p> <p>Check again the cluster:</p> <p>Command<pre><code>kubectl get pods --all-namespaces -o wide\n</code></pre> Output<pre><code>NAMESPACE     NAME                                READY   STATUS    RESTARTS        AGE     IP            NODE        NOMINATED NODE   READINESS GATES\nkube-system   coredns-5dd5756b68-756n6            1/1     Running   2 (6m11s ago)   17m     10.32.0.1     singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-5dd5756b68-l82qm            1/1     Running   1 (9m29s ago)   17m     10.32.0.3     singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   etcd-singlek8s                      1/1     Running   6 (3m55s ago)   19m     172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-apiserver-singlek8s            1/1     Running   8 (3m13s ago)   19m     172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-controller-manager-singlek8s   1/1     Running   9 (5m52s ago)   19m     172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-rkqnk                    1/1     Running   7 (4m7s ago)    19m     172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-scheduler-singlek8s            1/1     Running   9 (7m30s ago)   19m     172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\nkube-system   weave-net-68qbb                     2/2     Running   0               8m36s   172.16.1.70   singlek8s   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Command<pre><code>kubectl get nodes -o wide\n</code></pre> Output<pre><code>NAME        STATUS   ROLES                  AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION   CONTAINER-RUNTIME\nkube-test   Ready    control-plane,master   18m   v1.21.5   172.16.1.214   &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.10.0-9-amd64   containerd://1.4.11\n</code></pre></p> <p>Now we really have a working single node Kubernetes cluster. </p> <p>Before jump to the next section take a look at the node role: <code>control-plane,master</code> This means the only node we hava acts as control-plane and master, since we won't have any other worker nodes this nodes must have worker role as well:</p> <pre><code>kubectl label node singlek8s node-role.kubernetes.io/worker=\n</code></pre> <p>But this is not enoguh because the master nodes have taint by default: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</p> <p>You can check the taint with <code>kubectl get node kube-test -o yaml</code> command:</p> <pre><code>...\n  taints:\n  - effect: NoSchedule\n    key: node-role.kubernetes.io/control-plane\n</code></pre> <p>Remove this taint: <pre><code>kubectl taint nodes kube-test node-role.kubernetes.io/master=:NoSchedule-\nkubectl taint nodes singlek8s node-role.kubernetes.io/control-plane-\n</code></pre></p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#post-installation-steps","title":"Post Installation Steps","text":""},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#scale-down-the-coredns-deployment","title":"Scale Down The CoreDNS Deployment","text":"<p>Remember we have only one node in this demo cluster. It's not neccessary to have multiple instance of our applications, because all of them will run on this single node, and this behaviour doesn't give us any extras. </p> <pre><code>kubectl edit -n kube-system deployment coredns\n</code></pre> <p>Change the replicas to 1: <pre><code>spec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n</code></pre></p> <p>Check Command<pre><code>kubectl -n kube-system get pods\n</code></pre> Output<pre><code>NAME                                READY   STATUS    RESTARTS   AGE\ncoredns-558bd4d5db-l6gtq            1/1     Running   0          32m\netcd-kube-test                      1/1     Running   0          33m\nkube-apiserver-kube-test            1/1     Running   0          33m\nkube-controller-manager-kube-test   1/1     Running   0          33m\nkube-proxy-nhm2h                    1/1     Running   0          32m\nkube-scheduler-kube-test            1/1     Running   0          33m\nweave-net-l8xkh                     2/2     Running   1          16m\n</code></pre></p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#install-kuberntes-metrics-server","title":"Install Kuberntes Metrics Server","text":"<p>In order to get basic performance information about our cluster or pods we have to install Kubernetes Metrics Server:</p> <ul> <li>https://github.com/kubernetes-sigs/metrics-server</li> </ul> <pre><code>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n</code></pre> <p>You probalby will get the following error:</p> <pre><code>kubectl -n kube-system logs metrics-server-6775f684f6-gxpcz\n</code></pre> <p>Output: </p> <pre><code>E1011 09:43:22.529064       1 scraper.go:139] \"Failed to scrape node\" err=\"Get \\\"https://172.16.1.214:10250/stats/summary?only_cpu_and_memory=true\\\": x509: cannot validate certificate for 172.16.1.214 because it doesn't contain any IP SANs\" node=\"kube-test\"\n</code></pre> <p>Edit the deployment <code>kubectl -n kube-system edit  deployment metrics-server</code> and add this lint to <code>args</code>:</p> <pre><code>      containers:\n      - args:\n        - --kubelet-insecure-tls\n</code></pre> <p>Check Command<pre><code>kubectl -n kube-system top pods\n</code></pre> Output<pre><code>NAME                                CPU(cores)   MEMORY(bytes)   \ncoredns-558bd4d5db-l6gtq            3m           18Mi            \netcd-kube-test                      16m          41Mi            \nkube-apiserver-kube-test            76m          333Mi           \nkube-controller-manager-kube-test   12m          60Mi            \nkube-proxy-nhm2h                    1m           20Mi            \nkube-scheduler-kube-test            4m           25Mi            \nmetrics-server-6775f684f6-gxpcz     6m           18Mi            \nweave-net-l8xkh                     2m           61Mi\n</code></pre></p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#install-nginx-ingress-controller","title":"Install (Nginx) Ingress Controller","text":"<p>Maybe this it the most interesting part of this article as for now. You can chose from various ingress controller to install. Check the official documentation for more details:</p> <ul> <li>https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</li> </ul> <p>For this demo I choose the NGinx Ingress Controller. I think it is easy to install and maybe some of your already have experience with NGinx on bare metal. I can't tell you any other news than checking the official docs: https://kubernetes.github.io/ingress-nginx/deploy/ </p> <p>We are going to follow the Bare Metal installation: https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal</p> <pre><code># Old version: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.49.3/deploy/static/provider/baremetal/deploy.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml\n\n\n# Check The Install Process: \n\nkubectl get pods -n ingress-nginx \\\n  -l app.kubernetes.io/name=ingress-nginx --watch\n</code></pre> <p>Warning</p> <p>Do NOT deploy the latest version (v1.0.3)! It won't work with hostNetwork on Bare Metal installation. The address field will be blank even if you specify <code>--report-node-internal-ip-address</code> command line arguments. I've tried a lot of settings but none of them worked.</p> <p>Wait for this line: <pre><code>ingress-nginx-controller-6c68f5b657-hvfn9   1/1     Running             0          60s\n</code></pre></p> <p>This is the default installation process. But at this point we have only NodePort service for incomming traffic:</p> <p>Command<pre><code>kubectl -n ingress-nginx get services\n</code></pre> Ouptut<pre><code>NAME                                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx-controller             NodePort    10.22.0.78    &lt;none&gt;        80:30016/TCP,443:32242/TCP   104s\ningress-nginx-controller-admission   ClusterIP   10.22.0.242   &lt;none&gt;        443/TCP                      104s\n</code></pre></p> <p>Warning</p> <p>Please read very carefully this documentation: https://kubernetes.github.io/ingress-nginx/deploy/baremetal/</p> <p>This NodePort means that our NGinx ingress controller is accessible on every node in the cluster on <code>30016/TCP</code> and <code>32242/TCP</code>. It is OK if you planning to use these ports to access the applications inside the cluster. Or you can install a reverse proxy somewehere in you physical network. This can be the Kubernetes host itself. Configuring a reverse proxy could be a pain, and it is not the subject of this article.</p> <p>Check</p> <p>To check the NGinx Ingress Controller you should use another machine in your network. Most of the time I use curl to check if web server is running or not, but you can use your browser instead.</p> <p>Command<pre><code>curl -i http://172.16.1.214:30016 \n</code></pre> Output<pre><code>HTTP/1.1 404 Not Found\nDate: Mon, 11 Oct 2021 10:11:40 GMT\nContent-Type: text/html\nContent-Length: 146\nConnection: keep-alive\n\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>Command<pre><code>curl -ik https://172.16.1.214:32242\n</code></pre> Ouptut<pre><code>HTTP/2 404 \ndate: Mon, 11 Oct 2021 10:12:13 GMT\ncontent-type: text/html\ncontent-length: 146\nstrict-transport-security: max-age=15724800; includeSubDomains\n\n&lt;html&gt;\n&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;\n&lt;body&gt;\n&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;\n&lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre></p> <p>This is not the desired stat I want, I want to access my Ingresss Controller over the standard 80(http) and 443(https) ports, so choose the hostPort: https://kubernetes.github.io/ingress-nginx/deploy/baremetal/#via-the-host-network</p> <p>Caution</p> <p>This setup absolutely not suitable for Production environment, but just enough for this demo.</p> <pre><code>kubectl patch deployment/ingress-nginx-controller -n ingress-nginx \\\n-p '{ \"spec\": { \"template\": { \"spec\": { \"hostNetwork\": true } } } }'\n</code></pre> <p>Info</p> <p>This command does't not effects the <code>Service</code>. If you want to check the config describe your pod with <code>kubectl -n ingress-nginx describe  $(kubectl -n ingress-nginx get pods -o name | grep  \"ingress-nginx-controller\")</code> command and look for this line <code>Host Ports:    80/TCP, 443/TCP, 8443/TCP</code></p> <p>Check</p> <pre><code>curl -ikI https://172.16.1.70\nHTTP/2 404 \ndate: Mon, 11 Oct 2021 10:29:13 GMT\ncontent-type: text/html\ncontent-length: 146\nstrict-transport-security: max-age=15724800; includeSubDomains\n\ncurl -ikI http://172.16.1.70\nHTTP/1.1 404 Not Found\nDate: Mon, 11 Oct 2021 10:29:16 GMT\nContent-Type: text/html\nContent-Length: 146\nConnection: keep-alive\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#deploy-an-application-and-create-service-and-ingress-for-it","title":"Deploy An Application And Create Service And Ingress For It","text":"<p>For demonstration we deploy a Ghost (blog) instance.</p> <pre><code># kubectl create deployment ghost-test --image=ghost:latest\nkubectl create deployment nginx-test --image=nginx:latest\n</code></pre> <p>Note</p> <p>Without explicitly specifying the namespace the pod will be created in the <code>default</code> namespace.</p> <p>At this point we have a POD running, but we could not access it. First we need to create a <code>Service</code> object.</p> <pre><code>#kubectl create service clusterip ghost-test --tcp=2368:2368\nkubectl create service clusterip nginx-test --tcp=80:80\n</code></pre> <p>Important</p> <p>Use the same name as the Deployment (ghost-test)! This will create the appropiate selector.</p> <p>Check the Service:</p> <p>Command<pre><code>kubectl describe svc nginx-test\n</code></pre> Output<pre><code>Name:              nginx-test\nNamespace:         default\nLabels:            app=nginx-test\nAnnotations:       &lt;none&gt;\nSelector:          app=nginx-test\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.22.103.28\nIPs:               10.22.103.28\nPort:              80-80  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.32.0.5:80\nSession Affinity:  None\nEvents:            &lt;none&gt;\n</code></pre></p> <p>The most important thing here is the Endpoints:</p> <pre><code>Endpoints:          10.32.0.5:80\n</code></pre> <p>The IP address should point to the IP address of the Ghost POD:</p> <p>Command<pre><code>kubectl get pods -o wide\n</code></pre> Output<pre><code>NAME                          READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES\nnginx-test-5f4c58bccc-l5p9s   1/1     Running            0              82s     10.32.0.5   singlek8s   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>The last step is to create the <code>Ingress</code> object.</p> <p>The <code>yaml</code> file: <pre><code>cat &lt;&lt;EOF&gt;nginx-ingress.yaml\nkind: Ingress\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: nginx-web\n  namespace: default\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: nginx-test.example.local\n    http:\n      paths:\n      - pathType: ImplementationSpecific\n        backend:\n          service:\n            name: nginx-test\n            port:\n              number: 80\nEOF\n</code></pre></p> <p>Apply the yaml:</p> <pre><code>kubectl apply -f nginx-ingress.yaml\n</code></pre> <p>Check the ingress</p> <p>Command<pre><code>kubectl describe ingress nginx-web \n</code></pre> Ouptut<pre><code>Name:             nginx-web\nLabels:           &lt;none&gt;\nNamespace:        default\nAddress:\nIngress Class:    nginx\nDefault backend:  &lt;default&gt;\nRules:\n  Host                      Path  Backends\n  ----                      ----  --------\n  nginx-test.example.local\n                               nginx-test:80 (10.32.0.5:80)\nAnnotations:                &lt;none&gt;\nEvents:\n  Type    Reason  Age                    From                      Message\n  ----    ------  ----                   ----                      -------\n  Normal  Sync    2m26s (x3 over 4m19s)  nginx-ingress-controller  Scheduled for sync\n</code></pre></p> <p>Check With <code>curl</code></p> <pre><code>curl  -H  'Host: nginx-test.example.local' http://172.16.1.70\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#dns-entries","title":"DNS Entries","text":"<p>The above example works only if you overwrite the HTTP Host header with <code>'Host: ghost-test.example.local'</code>. This is because the <code>ghost-test.example.local</code> domain name does not exist. At this point we have to decide how to manage the DNS. The Kubernetes Igress works the best if we have a wildcard DNS entry. I don't want to get deep insude the DNS problem, but give you some tips:</p> <ul> <li>If your home router support static DNS entries you can define one, for example: <code>*.k8s.test.local</code>. My Mikrotik router has this function, so I can easily create a local wildcard DNS. Example <code>/ip dns static add address=172.16.1.214 regexp=.k8s-test.loc</code></li> <li>You can use your existing domain, or buy a new one. For example I have vinczejanos.info domain registered at Godaddy. I can add a wildcard DNS record which points to the IP address of the Kubernetes machine. (<code>*.local.k8s.vinczejanos.info</code>) Be aware that this can be a security risk, everybody on the public Internet can resolve this host name, and it points to a private IP address.</li> <li>You can set up your own DNS server. Maybe the easiest way is the DNSMasq, but Bind9 can be also a good alternative. (Bind maybe a bit robust for this purpose.) In this case you have to configure all of your clients to use the new DNS server. </li> <li>Since this is only a demo environment, a suitable solution can be to use your <code>hosts</code> file, but keep in mind that hosts file doesn't support wildcards, so you have to specify all host names one-by-one. The hosts file must be updated on all the host from where you want to access your Kubernetes Cluster.</li> </ul> Example <p>Hosts file location:</p> <ul> <li>Linux: <code>/etc/hosts</code></li> <li>Windows: <code>Windows\\System32\\drivers\\etc</code> </li> </ul> <p>Now, we have a fully functional Single Node Kubernetes Cluster. </p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#install-kubernetes-dashboard","title":"Install Kubernetes Dashboard","text":"<p>Link: https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</p> <pre><code>#kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v3.0.0-alpha0/charts/kubernetes-dashboard.yaml\n</code></pre> <p>Check if the pods are fine.</p> <p>Command<pre><code>kubectl -n kubernetes-dashboard get pods\n</code></pre> Output<pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\nkubernetes-dashboard-api-8586787f7-hmhmn                1/1     Running   0          35s\nkubernetes-dashboard-metrics-scraper-6959b784dc-669wh   1/1     Running   0          35s\nkubernetes-dashboard-web-6b6d549b4-8bwhm                1/1     Running   0          35s\n</code></pre></p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#create-an-ingress","title":"Create An Ingress","text":"<p>Check The Service</p> <p>Command<pre><code>kubectl -n kubernetes-dashboard describe  svc kubernetes-dashboard\n</code></pre> Output<pre><code>Name:              kubernetes-dashboard-web\nNamespace:         kubernetes-dashboard\nLabels:            app.kubernetes.io/component=web\n                   app.kubernetes.io/name=kubernetes-dashboard-web\n                   app.kubernetes.io/part-of=kubernetes-dashboard\n                   app.kubernetes.io/version=v1.0.0\nAnnotations:       &lt;none&gt;\nSelector:          app.kubernetes.io/name=kubernetes-dashboard-web,app.kubernetes.io/part-of=kubernetes-dashboard\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.22.216.140\nIPs:               10.22.216.140\nPort:              web  8000/TCP\nTargetPort:        8000/TCP\nEndpoints:         10.32.0.7:8000\nSession Affinity:  None\nEvents:            &lt;none&gt;\n</code></pre></p> <p>The Ingress <code>yaml</code></p> <pre><code>cat &lt;&lt;EOF&gt;kubernetes-dashboard-ingress.yaml\nkind: Ingress\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: kubernetes-dashboard-v3\n  namespace: kubernetes-dashboard\n  labels:\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/part-of: kubernetes-dashboard\n  annotations:\nspec:\n  ingressClassName: nginx\n  tls:\n    - hosts:\n        - k8s-dashboard.vincze.work\n      secretName: k8s-dashboardv3-certs\n  rules:\n    - host: dashboard.k8s-test.loc\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-web\n                port:\n                  name: web\n          - path: /api\n            pathType: Prefix\n            backend:\n              service:\n                name: kubernetes-dashboard-api\n                port:\n                  name: api\nEOF\n</code></pre> <p>Apply the ingress:</p> <pre><code>kubectl apply -f kubernetes-dashboard-ingress.yaml\n</code></pre> <p>Now I can access the Kubernetes Dashboard at https://dashboard.k8s-test.loc</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#the-token","title":"The Token","text":"<p>When you see the login screen, you are asked for the token or kubeconfig file. I choose the simpier way and use token.</p> <p>Create the token:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubernetes-dashboard-token\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: kubernetes-dashboard\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre> <p>Get The Token <pre><code>kubectl -n kubernetes-dashboard get secret/kubernetes-dashboard-token -o json | jq -r '.data.token' | base64 -d ;echo\n</code></pre></p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#fix-permission","title":"Fix Permission","text":"<p>You will see a lot of error message like this:</p> <pre><code>namespaces is forbidden: User \"system:serviceaccount:kubernetes-dashboard:kubernetes-dashboard\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope\n</code></pre> <p>Screenshot:</p> <p></p> <p>I don't want to bother with roles and rolebindigns at this article. I want quick win, so edit the <code>kubernetes-dashboard</code> ClusterRole.</p> <pre><code>kubectl  edit  clusterrole kubernetes-dashboard\n</code></pre> <p>And modify: <pre><code> rules:\n - apiGroups:\n-  - metrics.k8s.io\n+  - '*'\n   resources:\n-  - pods\n-  - nodes\n+  - '*'\n   verbs:\n-  - get\n-  - list\n-  - watch\n+  - '*'\n+- nonResourceURLs:\n+  - '*'\n+  verbs:\n+  - '*'\n</code></pre></p> <p>It should look like this (<code>kubectl  get clusterrole kubernetes-dashboard -o yaml</code>): <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"labels\":{\"k8s-app\":\"kubernetes-dashboard\"},\"name\":\"kubernetes-dashboard\"},\"rules\":[{\"apiGroups\":[\"metrics.k8s.io\"],\"resources\":[\"pods\",\"nodes\"],\"verbs\":[\"get\",\"list\",\"watch\"]}]}\n  creationTimestamp: \"2021-10-11T14:33:17Z\"\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  resourceVersion: \"15040\"\n  uid: 26495412-c50f-48fb-b3ce-448d42dff15b\nrules:\n- apiGroups:\n  - '*'\n  resources:\n  - '*'\n  verbs:\n  - '*'\n- nonResourceURLs:\n  - '*'\n  verbs:\n  - '*'\n</code></pre></p> <p>Caution</p> <p>Do NOT do this in Production system! This way you give cluster-admin role to kubernetes-dashboard ServiceAccount. Everybody who knows the token can do anything with your Kubernetes cluster!</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#cheatsheet","title":"CheatSheet","text":"<p>Finally I write here some command I'm using in daily basis.</p>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#list-and-delete-completed-pods","title":"List And Delete <code>Completed</code> PODS","text":"<p>When a Job finishes it's work, the container is left as <code>Completed</code>. A lot of PODS in <code>Completed</code> statw can be disturbing, and they can be safely deleted.</p> <p>List: </p> <pre><code>kubectl get pods --all-namespaces --field-selector=status.phase=Succeeded\n</code></pre> <p>Delete:</p> <pre><code>kubectl delete pods --all-namespaces --field-selector=status.phase=Succeeded\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#check-container-logs","title":"Check Container Logs","text":"<pre><code># List PODS\nkubectl -n default get pods\n\n# Show the logs:\nkubectl -n default logs ghost-test-66846549b5-qgcl8\n\n# Or follow the logs:\nkubectl -n default logs ghost-test-66846549b5-qgcl8 -f\n\n# Or follow without showing only the logs written from now.\nkubectl -n default logs ghost-test-66846549b5-qgcl8 -f --tail=0\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#get-into-the-container","title":"Get Into The Container","text":"<p>Sometimes you need to see what happens inside a container. In this case you can get a shell inside the container.</p> <pre><code>kubectl -n default exec -it ghost-test-66846549b5-qgcl8 -- /bin/bash\n\n# Or if no bash installed, you can try sh\nkubectl -n default exec -it ghost-test-66846549b5-qgcl8 -- /bin/sh\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#pending-pods-nginx-ingress","title":"Pending PODS (nginx ingress)","text":"<p>Example: <pre><code>NAME                                        READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-5f7bb7476d-2nhqc   0/1     Pending   0          4s\ningress-nginx-controller-745c7c9f6c-m9q5q   1/1     Running   2          128m\n</code></pre></p> <p>Events: Command<pre><code>kubectl -n ingress-nginx get events\n</code></pre> Output<pre><code>LAST SEEN   TYPE      REASON              OBJECT                                           MESSAGE\n34s         Warning   FailedScheduling    pod/ingress-nginx-controller-5f7bb7476d-2nhqc    0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports.\n35s         Normal    SuccessfulCreate    replicaset/ingress-nginx-controller-5f7bb7476d   Created pod: ingress-nginx-controller-5f7bb7476d-2nhqc\n36s         Normal    ScalingReplicaSet   deployment/ingress-nginx-controller              Scaled up replica set ingress-nginx-controller-5f7bb7476d to 1\n</code></pre></p> <p>The problem: <code>0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports.</code></p> <p>NGinx Ingress Controller (the POD) uses hostPort, and RollingUpdate strategy. This means that Kubernetes tries to start a new instance, and after the new instance is Running and healthy stop the \"old\" one. But in this case it is not possible because two container can not bind the same ports (80,443). </p> <p>The easiest way to solve this is to delete the old pod manually. ( <code>ingress-nginx-controller-745c7c9f6c-m9q5q</code> )</p> <pre><code>kubectl -n ingress-nginx delete pod ingress-nginx-controller-745c7c9f6c-m9q5q\n</code></pre> <p>Permanent solution could be changing the strategy to ReCreate.</p> <pre><code>kubectl patch deployment/ingress-nginx-controller -n ingress-nginx  -p '{ \"spec\": { \"strategy\":{ \"$retainKeys\": [\"type\"],\"type\": \"Recreate\"}}}'\n</code></pre> <p>Show pretty print Json <pre><code>{\n  \"spec\": {\n    \"strategy\": {\n      \"$retainKeys\": [\n        \"type\"\n      ],\n      \"type\": \"Recreate\"\n    }\n  }\n}\n</code></pre></p> <p>Reference: </p> <ul> <li>Use strategic merge patch to update a Deployment using the retainKeys strategy</li> <li>https://blog.container-solutions.com/kubernetes-deployment-strategies</li> </ul>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#get-all-all-namespaces","title":"Get All / All Namespaces","text":"<pre><code>kubectl -n ingress-nginx get all\n\n# Get all ingress in the cluster\n\nkubectl get ingress --all-namespaces -o wide\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#print-join-command-add-worker-node","title":"Print Join Command (Add Worker Node)","text":"<pre><code>kubeadm token create --print-join-command\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#get-all-resources-get-help","title":"Get All Resources / Get Help","text":"<pre><code>kubectl api-resources\n\n# Explain\n\nkubectl explain pod\nkubectl explain pod.spec\nkubectl explain pod.spec.tolerations\n</code></pre>"},{"location":"Blog/2021/09/25/Install_Single_Node_Kubernetes_Cluster/#copy-dir-file-from-container","title":"Copy Dir / File From Container","text":"<pre><code># File \n\nkubectl -n default cp ghost-test-66846549b5-qgcl8:/var/lib/ghost/config.production.json  config.production.json\n\n# Directory\nkubectl -n default cp ghost-test-66846549b5-qgcl8:/var/lib/ghost .\n</code></pre> Warning <p><code>kubectl -n default cp ghost-test-66846549b5-qgcl8:/var/lib/ghost .</code> &lt;-- This will copy the conents of the directory. If you want to create the 'ghost' directory on the destination use: </p> <p><code>kubectl -n default cp ghost-test-66846549b5-qgcl8:/var/lib/ghost ghost</code></p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/","title":"How To Use MKDocs with Docker?","text":""},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#tldr","title":"TL;DR","text":"<p>As wrote on the index page, I changed from Ghost Blog to MKDocs. This was necessary because my Ghost version was really outdated and I could not get used to its new editor. I have some essential requirement against the platform I use:</p> <ul> <li>Should be easy to use with native MarkDown support</li> <li>Should not have billions of features I won't use ever (to avoid unnecessary system load)</li> <li>Must have integrated very good search engine</li> <li>Look-And-Feel must be easily customized without plugins or addons</li> <li>Since I post a lot of code bloks, code highlighting is must. It should be achieved natively or using prism.js</li> <li>Source code of the contents (*.md) should be stored in a Git repository.</li> </ul>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#what-was-the-alternatives","title":"What was the alternatives?","text":"<ul> <li>CMS Systems</li> <li>Drupal, WordPress, Joomla : All of these are CMS systems, and too robust for my purpose. I needed a light-weight system.</li> <li>Static Site Generators and </li> <li>Jekyll, Gatsby.js, Scully, MKDocs, etc. : They are way more closer to my expectations. </li> </ul> <p>And how I chose up MKDocs over the rest? My selection process was really simple. I gave a try all of them, this means I spent 2 hours to try each of them. The winner was with which I could get closer to my expectations in 2 hours. And of course which was closer to my taste in coding, manageability and flexibility.</p> <p>Now It seems MKDocs does exactly what I need. Probably most of the other static website generator would have been perfect for me, but after 2 hours of using I found it really comfortable for me. And I did not regret my choice. All of my old contents are migrated to this site, and meantime I did some customization on the the theme and the site. And this is the main topic of this post: How I migrated the content and how I use MKDocs?</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#prepare-mkdocs-docker-image","title":"Prepare MKDocs Docker Image","text":"<p>If you read the official MKDocs installation page you can see that the installation should be done by run <code>python pip install</code> command. I don't like to install various python packages on my computer, because sooner or later I'm going to stuck in failed requirements. So at the first step I had to make decision what to use: python virtualenv or Docker. Of course I chose Docker.</p> <p>I do know that \"Material for MkDocs\" have official Docker image, but I like to use Docker images was built by my own. Every time I build a new Docker image from scratch I learn something or make my knowledge deeper about Dockerfiles, so it's absolutely worth it.</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#dockerfile-build","title":"Dockerfile &amp; Build","text":"Click Here For Raw Source<pre><code>FROM python:3-alpine\n\nARG USER=1001\n\nRUN adduser -h /usr/src/mkdocs -D -u $USER mkdocs \\\n&amp;&amp; apk add bash \\\n&amp;&amp; apk add git \n\nENV PATH=\"${PATH}:/usr/src/mkdocs/.local/bin\"\n\nUSER mkdocs\nRUN mkdir -p /usr/src/mkdocs/build\nWORKDIR /usr/src/mkdocs/build\n\nRUN pip install --upgrade pip\n\nRUN pip install pymdown-extensions \\\n&amp;&amp; pip install mkdocs \\\n&amp;&amp; pip install mkdocs-material \\\n&amp;&amp; pip install mkdocs-rtd-dropdown \\\n&amp;&amp; pip install mkdocs-git-revision-date-plugin \\\n&amp;&amp; pip install mkdocs-git-revision-date-localized-plugin\n\n\nENTRYPOINT [\"/usr/src/mkdocs/.local/bin/mkdocs\"]\n</code></pre> <ul> <li><code>FROM python:3-alpine</code> --&gt; Using the official python image.</li> <li><code>ARG USER=1001</code> --&gt; Default user id. If you don't specify another when building the container (see below)</li> <li><code>ENTRYPOINT</code> --&gt; The default command to run when the container starts.</li> </ul> <p>Build command <pre><code>docker build -t exmaple-mkdocs:v1 --build-arg=USER=$(id -u) .\n</code></pre></p> <p>Important</p> <p>Docker container will be built using your local user id. This will help you to avoid permission deined when mounting local directory inside the container. </p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#usage-examples","title":"Usage Examples","text":"<ul> <li> <p>Get help <pre><code>docker run -it exmaple-mkdocs:v1 --help\n</code></pre></p> </li> <li> <p>Create New Site <pre><code>mkdir /tmp/example/\ndocker run -it -v /tmp/example:/build exmaple-mkdocs:v1 new /build\n</code></pre></p> </li> </ul> <p>This command will create the following initial files (inside the <code>/tmp/example</code> directory on you local system: <pre><code>.\n./docs\n./docs/index.md\n./mkdocs.yml\n</code></pre></p> <p>Info</p> <p>With  <code>-v</code> parameter you can mount one of your local directory inside the container (bind mount). So the running process iside the container will see your local direcotry <code>/tmp/example</code> as <code>bild</code>. (<code>-v /HOST-DIR:/CONTAINER-DIR</code>). If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host (default location: <code>/var/lib/docker/volumes</code>).</p> <ul> <li> <p>Get into the container <pre><code>docker run -it --entrypoint=/bin/bash exmaple-mkdocs:v1\n</code></pre></p> </li> <li> <p>Run the builtin development server <pre><code>docker run -it -p 8789:8000 \\\n-v /tmp/example:/build exmaple-mkdocs:v1 \\\nserve --dev-addr 0.0.0.0:8000 --config-file /build/mkdocs.yml\n</code></pre></p> </li> </ul> <p>This command may need some explanation:</p> <ul> <li><code>docker run -it</code> --&gt; Run the container in interactive mode and allocate a pseudo-tty.</li> <li><code>-p 8789:8000</code> --&gt; Publish container port 8000 on the host port 8789. This means process binding the port 8000 inside the container will be published on the local port 8789.</li> <li><code>serve --dev-addr 0.0.0.0:8000 --config-file /build/mkdocs.yml</code> --&gt; Arguments of the <code>mkdocs</code> command. (ENTRYPOINT)</li> <li>The bind port (8000) must be the same as specified at <code>-p</code> parameter.</li> <li>Since we bind mounted the <code>/tmp/example</code> local directory into the container's <code>/build</code> we can acces mkdocs.yml inside the container as <code>/build/mkdocs.yml</code></li> </ul> <p>Now you can access your newly created site at http://localhost:8789 or http://[your machine ip address]:8789. Every modification inside the <code>/tmp/example</code> directory immediately take effects, so you don't need to restart the container, your browser will refresh the page automatically. But be aware that if you make systax error in the <code>mkdocs.yml</code> the container will exit and you need to manually retart it.</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#configure","title":"Configure","text":""},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#mkdocsyml","title":"mkdocs.yml","text":"<p>Configuring your MKDocs intstance basically means editing <code>mkdocs.yml</code>.</p> <p>You can see my current configuration below: Click Here For Raw Source<pre><code>site_name: Vincze Janos Blog\nsite_url: https://readthedocs.vinczejanos.info\ntheme:\n    name: material\n    locale: en\n    custom_dir: overrides\n    features:\n      - navigation.tracking\n      - navigation.top\n      - search.suggest\n      - search.highlight\n      - search.suggest\n      - search.highlight\n    palette:\n      - scheme: default\n        toggle:\n          icon: material/toggle-switch-off-outline\n          name: Switch to dark mode\n      - scheme: slate\n        toggle:\n          icon: material/toggle-switch\n          name: Switch to light mode\n          primary: green\n          accent: ambe\nmarkdown_extensions:\n  - admonition\n  - toc:\n      permalink: true\n      toc_depth: 4\n  - pymdownx.betterem:\n      smart_enable: all\n  - pymdownx.mark\n  - pymdownx.tilde\n  - pymdownx.details\n  - pymdownx.snippets  \n  - attr_list\nnav:\n  - \"How I Built This Site?\": 'How_I_Built_This_Site.md'\n  - \"Tips And Tricks\": 'Tips_And_Tricks.md'\n  - \"Old Blog Conetent\":\n    - \"Collect Network Statistic With Telegraf &amp; VNSTAT\": 'old/Collect_Network_Statistic_With_Telegraf_Vnstat.md'\n    - \"Iptables Examples\": 'old/Iptables_Examples.md'\n    - \"Nokia 6120c (BB5) Forgotten Security Code\": 'old/Nokia_6120c_(bb5)_Forgotten_Security_Code.md'\n    - \"Sonoff Relays With OpenHab And Tasmota Firmware\": 'old/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware.md'    \n    - \"Raspberry &amp; Orange PI\":\n      - \"Move root file system to USB storage (RPI2 &amp; RPI3)\": 'old/raspberry/Move_Root_File_System_To_Usb_Storage_(rpi2_&amp;_Rpi3).md'\n      - \"Raspberry PI 3 As Wifi Range Extender\": 'old/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender.md'\n      - \"Install OpenALPR on Raspberry PI 3 (Part 2)\": 'old/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2.md'\n    - \"NodeMCU - LUA / ESP8266\":\n      - \"How To Unbrick ESP8266 (Blinking Blue Led)\": 'old/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led.md'\n      - \"Logging MQTT data (subscription) to MySQL with Shell Script\": 'old/nodemcu/Logging_Mqtt_Data_(subscription)_To_Mysql_With_Shell_Script.md'\n    - \"Linux Tutorials\":\n      - \"Install mps-youtube console based youtube player\": 'old/linux/Install_Mps-youtube_Console_Based_Youtube_Player.md'\n      - \"Create Your Own DynDns Service with Bind (Named)\": 'old/linux/create_Your_Own_Dyndns_Service_With_Bind_named.md'\nextra_css:\n  - css/extra.css\n  - css/prism.css\nextra_javascript:\n  - js/prism.js\nextra:\n  disqus: shortname-jvincze-test\n  analytics:\n    provider: google\n    property: G-3LZJ7L57GQ\n</code></pre></p> <p>I think there is nothing special in this configuration, but could be a good example. Every part of this file is very well documented on the officail Material and MKDocs website:</p> <ul> <li>https://www.mkdocs.org/user-guide/</li> <li>https://squidfunk.github.io/mkdocs-material/</li> </ul> <p>Info</p> <p>All the items in the <code>nav</code> section is relative to the <code>docs</code> directory. Example: <code>old/Iptables_Examples.md</code> is located at <code>/tmp/example/docs/old/Iptables_Examples.md</code></p> <p>I think the only thing to metion is my <code>extra.css</code> file.</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#extracss","title":"<code>extra.css</code>","text":"<pre><code>.md-grid {\nmax-width: initial;\n}\n\npre[class*=\"language-\"] {\n       max-height: 32em !important; \n}\n\n.md-clipboard {\n  display: none !important;\n}\n\n.md-typeset pre&gt;code {\n   overflow: unset !important;\n   padding: unset !important;\n}\n</code></pre> <p>I know using <code>!important</code> is not the best things to do, but I'm not a web developer and I needed a 'quick and dirty solution'. Maybe later, if I have more time I will customise the mkdocs theme and leave <code>!important</code>.</p> Info <p>More about <code>!important</code>: https://stackoverflow.com/questions/9245353/what-does-important-mean-in-css. \"Using !important has its purposes (though I struggle to think of them), but it's much like using a nuclear explosion to stop the foxes killing your chickens; yes, the foxes will be killed, but so will the chickens. And the neighborhood.\"</p> <ul> <li>md-grid (Material Theme)</li> </ul> <p>Reference: Content area width</p> <p>The width of the content area is set so the length of each line doesn't exceed 80-100 characters, depending on the width of the characters. While this is a reasonable default, as longer lines tend to be harder to read, it may be desirable to increase the overall width of the content area, or even make it stretch to the entire available space. This can easily be achieved with an additional stylesheet and a few lines of CSS: .md-grid {  max-width: initial;  }</p> <ul> <li>pre[class*=\"language-\"] (prismjs)</li> </ul> <p>Add vertical scroll bar when code block contains more than 32 lines.</p> <ul> <li>md-clipboard (Material Theme)</li> </ul> <p>This section disables the theme built in \"copy to clipoad\" funcion, it's neccessary if you wish to use the prismjs \"copy to clipboard\" method.</p> <ul> <li>md-typeset pre&gt;code (Material Theme)</li> </ul> <p>Some functions of prismj won't work properly without this modification, for example line numbering.</p> <p>Example:</p> <p><pre><code>&lt;pre class=\"line-numbers language-docker\" data-src=\"/files/Dockerfile\"&gt;&lt;/pre&gt;\n</code></pre> Screenshot:</p> <p></p> <p>You can see that the line number from the left hand side of the lines are missing.</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#build-the-site","title":"Build the site","text":"<p>If you are done writing your docs, and <code>nav</code> section is properly configured in <code>mkdocs.yml</code> it's time to build your site. I will show three methods to publis the site.</p> <p>Important</p> <p><code>mkdocs serve</code> is absolutely not suitable for production. It's only purpose to help you developing the site, and watch real-time your modification.</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#build-site-own-web-server","title":"Build Site &amp; Own Web Server","text":"<p>If you already have a web servers you can simply copy your content to the DocmentRoot. You can simply build your site:</p> <pre><code>docker run -it \\\n-v /tmp/example:/build exmaple-mkdocs:v1 \\\nbuild\n</code></pre> <p>This command will put your static html site into the <code>/tmp/example/site</code> directory on your host machine. Or you can specify where to store the generated content:</p> <pre><code>docker run -it \\\n-v /tmp/example:/build exmaple-mkdocs:v1 \\\n-v /var/www/html/mysite:/site \\\nbuild --site-dir /site\n</code></pre>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#build-site-nginx-with-docer","title":"Build Site &amp; Nginx with Docer","text":"<p>This method almost the same as the previous one, except that here we are using another Docker container to server our page. First build your site with buld command. </p> <p>Server your page with Nginx</p> <p>First start your container in the foreground to check if everything is fine: <pre><code>docker run -it --rm \\\n--name mkdocs \\\n-v [PATH TO YOUR SITE DIR]:/usr/share/nginx/html:ro \\\n-p 8087:80 \\\nnginx:latest\n</code></pre></p> <p>If you can access your site on the host port 8087, you should stop the container (ctrl+c) and start again in detached mode:</p> <pre><code>docker run -d \\\n--restart alaway \\\n--name mkdocs \\\n-v [PATH TO YOUR SITE DIR]:/usr/share/nginx/html:ro \\\n-p 8087:80 \\\nnginx:latest\n</code></pre> <p>More about Nginx container image: https://hub.docker.com/_/nginx</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#github-pages","title":"GitHub Pages","text":"<p>This is the method I use. I'm not borering with own web server instead I use github pages: https://pages.github.com For this you need a free Github registration. </p> <ul> <li>1. Create a repository</li> </ul> <p></p> <p>The name of the repository must be [your username].github.io and must be public. In my case:</p> <p></p> <ul> <li>2. Push</li> </ul> <p>Github does not support user/pass auth anymore, so you need to create an auth token.</p> <p>Go to settings: </p> <p></p> <p>Developer settings:</p> <p></p> <p>And Personal access tokens:</p> <p>Finally click on the Generate new token, select the permissions you need and generate the token.</p> <p>Push your mkdocs root dir:</p> <p>Command<pre><code>ls -al\n</code></pre> Output<pre><code>total 40\ndrwxr-xr-x  7 root root 4096 Oct  9 14:34 .\ndrwx------ 22 root root 4096 Oct  9 14:33 ..\ndrwxr-xr-x  6 root root 4096 Oct  9 14:33 cinder\n-rw-r--r--  1 root root  162 Oct  9 14:33 docker.cmd\ndrwxr-xr-x  8 root root 4096 Oct  9 14:33 docs\ndrwxr-xr-x  8 root root 4096 Oct  9 14:35 .git\n-rw-r--r--  1 root root 4471 Oct  9 14:33 mkdocs.yml\ndrwxr-xr-x  2 root root 4096 Oct  9 14:33 overrides\ndrwxr-xr-x 11 root root 4096 Oct  9 14:33 site\n</code></pre> <pre><code>git init\ngit add --all\ngit commit -m 'Initial release'\ngit remote add origin https://github.com/jvincze84/test-delete.git\ngit push -u origin master\n</code></pre></p> <ul> <li>3. Push gh-pages</li> </ul> <p>Command<pre><code>docker run -it -v /root/test-delete/:/usr/src/mkdocs/build example-mkdocs:v2 gh-deploy\n</code></pre> Output<pre><code>INFO     -  Cleaning site directory\nINFO     -  Building documentation to directory: /usr/src/mkdocs/build/site\nINFO     -  Documentation built in 1.04 seconds\nWARNING  -  Version check skipped: No version specified in previous deployment.\nINFO     -  Copying '/usr/src/mkdocs/build/site' to 'gh-pages' branch and pushing to GitHub.\nUsername for 'https://github.com': jvincze84\nPassword for 'https://jvincze84@github.com': \nEnumerating objects: 219, done.\nCounting objects: 100% (219/219), done.\nDelta compression using up to 8 threads\nCompressing objects: 100% (109/109), done.\nWriting objects: 100% (219/219), 14.91 MiB | 9.50 MiB/s, done.\nTotal 219 (delta 72), reused 219 (delta 72), pack-reused 0\nremote: Resolving deltas: 100% (72/72), done.\nremote: \nremote: Create a pull request for 'gh-pages' on GitHub by visiting:\nremote:      https://github.com/jvincze84/test-delete/pull/new/gh-pages\nremote: \nTo https://github.com/jvincze84/test-delete.git\n * [new branch]      gh-pages -&gt; gh-pages\nINFO     -  Based on your CNAME file, your documentation should be available shortly at: http://readthedocs.vinczejanos.info\nINFO     -  NOTE: Your DNS records must be configured appropriately for your CNAME URL to work.\n</code></pre></p> <p>We are almost done. Go back to Github, and set up the newly created \"gh-pages\" branch for pages:</p> <p></p> <ul> <li>4. Custom Domain</li> </ul> <p>Github publishes your content to \"https://[username].github.io\" (example: jvincze84.github.io). If you want to use your custom domain, put a file into the docs folder with name <code>CNAME</code>:</p> <pre><code>cat docs/CNAME \nreadthedocs.vinczejanos.info\n</code></pre> <p>But first you need to create a CNAME DNS record which points to \"[username].github.io\". Godaddy example:</p> <p></p> <p>Warning</p> <p>Do not modify your custom domain directly on github.com; <code>mkdocs gh-deploy</code> will overwrite your config.</p>"},{"location":"Blog/2021/10/01/How_to_use_MKdocs/#update-the-site","title":"Update The Site","text":"<p>If you want to update a page or add new pages you can follow these steps:</p> <p>Clone your repository</p> <p>Command<pre><code>git clone https://github.com/jvincze84/jvincze84.github.io\n</code></pre> Output<pre><code>Cloning into 'jvincze84.github.io'...\nremote: Enumerating objects: 850, done.\nremote: Counting objects: 100% (850/850), done.\nremote: Compressing objects: 100% (292/292), done.\nremote: Total 850 (delta 351), reused 820 (delta 327), pack-reused 0\nReceiving objects: 100% (850/850), 16.09 MiB | 10.59 MiB/s, done.\nResolving deltas: 100% (351/351), done.\n</code></pre></p> <p>Info</p> <p>If you want to save your git credentials run this command: <code>git config --global credential.helper store</code></p> <p>After you have done the neccessary modification (add/change page) push your changes:</p> <p>Warning</p> <p>If you add a new page (.md) don't forget to add it to the <code>nav</code> section in mkdocs.yml</p> <p>push to git</p> <p>Command<pre><code>git add docs/How_to_use_MKdocs.md mkdocs.yml\ngit commit -m 'Add new page : docs/How_to_use_MKdocs.md'\n</code></pre> Output<pre><code>master 0949efa] Add new page : docs/How_to_use_MKdocs.md\n 1 file changed, 25 insertions(+), 9 deletions(-)\ngit push\nEnumerating objects: 7, done.\nCounting objects: 100% (7/7), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (4/4), 993 bytes | 993.00 KiB/s, done.\nTotal 4 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 3 local objects.\nTo https://github.com/jvincze84/jvincze84.github.io\n   a9585e6..0949efa  master -&gt; master\n</code></pre></p> <p>gh-deploy</p> <p>Finally apply the modification on Github Pages</p> <pre><code>docker run -it --rm \\\n-v /home/mkdocs/Documents/mkdocs/jvincze84.github.io/:/usr/src/mkdocs/build \\\nmkdocs:1 gh-deploy\n</code></pre> <p>If you previously saved your git credentials you may want to use it inside the container:</p> <pre><code>docker run -it --rm \\\n-v /home/mkdocs/.git-credentials:/usr/src/mkdocs/.git-credentials \\\n-v /home/mkdocs/Documents/mkdocs/jvincze84.github.io/:/usr/src/mkdocs/build \\\nmkdocs:1 gh-deploy\n</code></pre>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/","title":"Get Started With Portainer","text":""},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#tldr","title":"TL;DR","text":"<p>Nowadays everybody talking and writing about containers, Docker, Kubernetes, OpenShift, etc. I don't want to explain here what are these meaning, instead give some practical use cases. I'm always testing my solutions at home with low cost HWs. I have an article about installing single node Kubernetes cluster, but now I step back to Docker containers. This is because deploying a container orchestrator not always the goal. This article could be useful for home users, or developers who want learn about containers. </p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#use-containers-instead-of-virtual-machines","title":"Use Containers Instead Of Virtual Machines","text":"<p>What is wrong with installing software to your Laptop or Desktop? Sooner or later you will end up in having a lot of unnecessary software on your machine. Or you have to install various software with various dependencies which could not be resolved. There are so many possibilities to separate a test environment from your host machine:</p> <ul> <li>WirtualBox</li> <li>LXC</li> <li>KVM</li> <li>Proxmox</li> <li>PythonVirtualEnv</li> </ul> <p>All of them has the advantages and disadvantages. Proxmox is really good if you have a separate hardware to install. It can handle LXC Containers and HW virtualization as well. VirtualBox is an easy-to-use Virtualization platform. It can be installed on Linux, Windows and even on MAC. But all of these have bigger footprint on you host hardware than Docker.</p> <p>Let's see how easy to try a software with Docker, for example Ghost blog:</p> <p>Link: https://hub.docker.com/_/ghost</p> <pre><code>docker run -d --name some-ghost \\\n-p 3001:2368 \\\n-v /path/to/ghost/blog:/var/lib/ghost/content \\\nghost:alpine\n</code></pre> <p>In some seconds you can access Ghost on <code>http://localhost:3001</code>. I can't imagine easier or faster way to try out something.</p> <p>Note</p> <p>The situation is more complicated, of coures, if you want to use external database, redis, etc in conjungtion with Ghost. But if you only want to test/try out Ghost it is the fastest way I think.</p> <p>In most cases you can find a really good documentation about how to use a software with Docker. --&gt; A lot of application have Dockerized version, but you can create own image, as well.</p> <p>Docker can be useful even in a situation where you don't have root access to the system, but Docker engine is installed and you can access to it. </p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#install-portainer","title":"Install Portainer","text":"<p>What is Portainer?</p> <p>Quote</p> <p>Portainer is a really nice web UI for Docker. Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use.</p> <p>Official Install Documentation: https://docs.portainer.io/v/ce-2.9/start/install/server/docker/linux</p> <p>The doc says: \"First, create the volume that Portainer Server will use to store its database\" But I won't do that. I don't like using Docker volumes. By default the volumes are stored in <code>/var/lib/docker/volumes</code> directory, and extra steps are needed to find which volume belongs to which container. So I usually use Bind Mounts. </p> <p>Start Portainer:</p> <pre><code>docker run -d \\\n-p 8000:8000 -p 9443:9443 \\\n--name portainer \\\n--restart=always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /opt/docker/portainer/data:/data \\\nportainer/portainer-ce:2.9.1-alpine\n</code></pre> <p>The official documentation says to use the <code>latest</code> (<code>portainer/portainer-ce:latest</code>) image, but:</p> <p>Read about <code>latest</code> tag: https://vsupalov.com/docker-latest-tag/</p> <p>What is my biggest problem with the <code>latest</code> tag? The updates! For example: an application is installed to the Kubernetes cluster, with database dependency. A new version of the app come out and the <code>latest</code> tag will points to this newer version. Next time when the pod restarts (for some reason) the newer version will be deployed. If the new version has breaking changes (eg.: database scheme changes, or manual config needs), your app won't start (properly). Or simply you don't want always use the latest version...</p> <p>Info</p> <p>The situation above partially can be avoided with properly configured imagePullPolicy: https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy</p> <p>Just partially because if the pod restarts in a node that don't have the image, alway  the <code>latest</code> image will be pulled...</p> <p>So I recommend not to use the <code>latest</code> tag.</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#portainer-on-vps-public-internet","title":"Portainer On VPS (Public Internet)","text":"<p>It is not an elegant to publish your Portainer to the public Internet. Of Course this not an issue when you install portainer inside a private network. But if you planning to use portainer on a VPS which has public IP address it is recommended not to bind the public IP address, or set up a firewall (iptables).</p> <p>I think the best solution is setting up a VPN connection between the server and your PC. I can recommend two almost zero config VPN solutions:</p> <ul> <li>https://tailscale.com</li> <li>https://www.wireguard.com</li> <li>Or you can use OpenVPN/IpSecVPN, if you familiar with them.</li> </ul> <p>So if you want to bind Portainer on a specific IP address use the following command:</p> <pre><code>docker run -d \\\n-p 10.5.0.2:8000:8000 -p 10.5.0.2:9443:9443 \\\n--name portainer \\\n--restart=always \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /opt/docker/portainer/data:/data \\\nportainer/portainer-ce:2.9.1-alpine\n</code></pre> <p>Where the <code>10.5.0.2</code>  is the IP Address of your VPN interface (tun0,wg0,tailscale0, etc).</p> <p>How does it look like:</p> <p>Command<pre><code>docker ps --filter name=portainer\n</code></pre> Output<pre><code>CONTAINER ID   IMAGE                           COMMAND        CREATED        STATUS        PORTS                                                          NAMES\n90ca6ef446e3   portainer/portainer-ce:latest   \"/portainer\"   43 hours ago   Up 43 hours   10.5.0.2:8000-&gt;8000/tcp, 10.5.0.2:9443-&gt;9443/tcp, 9000/tcp   portainer\n</code></pre></p> <p>Now you can access Portainer on https://10.5.0.2:9443</p> <p>We are now done with the installation.</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#using-portainer","title":"Using Portainer","text":""},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#first-steps","title":"First Steps","text":"<p>Create an admin user</p> <p></p> <p>Select \"GetStarted\"</p> <p></p> <p>Click on the \"local\" machine</p> <p></p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#create-new-docker-network","title":"Create New Docker Network","text":"<p>By default there are three networks:</p> <p></p> <p>What you see is equialent to <code>docker network ls</code> command:</p> <p>Command<pre><code>docker network ls\n</code></pre> Output<pre><code>NETWORK ID     NAME      DRIVER    SCOPE\ne15c0c8f8d67   bridge    bridge    local\nf728316f0193   host      host      local\n54ac360308df   none      null      local&lt;/code&gt;&lt;/pre&gt;\n</code></pre></p> <p>If you need more details about a network use the <code>inspect</code> command:</p> <p>Command<pre><code>docker inspect  e15c0c8f8d67\n</code></pre> Output<pre><code>[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"e15c0c8f8d67adc6b03876861c15c97bf581a97c75b9bedef7d48ecf6c906b0a\",\n        \"Created\": \"2021-10-01T11:25:52.320103343+02:00\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.18.0.0/16\",\n                    \"Gateway\": \"172.18.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"6c543ab5794bd752c29709a622f561a5d6647a071e802729ab8386759d3bcae6\": {\n                \"Name\": \"portainer\",\n                \"EndpointID\": \"b22b41e7e501de25a5b25d4320297224b5a87deb7c898c56ed0376af02c54917\",\n                \"MacAddress\": \"02:42:ac:12:00:02\",\n                \"IPv4Address\": \"172.18.0.2/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]\n</code></pre></p> <p>Why should we create new network?</p> <p>Link: https://docs.docker.com/network/bridge/#differences-between-user-defined-bridges-and-the-default-bridge</p> <p>I want to emphasize two adventages:</p> <ul> <li>User-defined bridges provide automatic DNS resolution between containers</li> <li>You can enable manual container attachment</li> </ul> <p>Hint</p> <p>Inside a docker network it is absolutely not recommended to use static IP addresses for containers. It is antipatern. Use the container's name instead of the IP address when you have to connect containers to each other and put them into the same network.</p> <p>Click on the \"Add network\" button.</p> <ul> <li>Name: Give a name to the new network. (custom_bridge)</li> <li>Driver: Bridge</li> <li>IPV4 Network configuration</li> </ul> <p></p> <p>Info</p> <p>IP range: Can differ from the Subnet. The containers will automatically get IP address from this range. (DHCP) Must be smaller than the entire Subnet.</p> <ul> <li>Enable manual container attachment: Turn On</li> </ul> <p>This can achieve with cli command as well:</p> <pre><code>docker network create custom-cli \\\n--attachable \\\n--driver bridge \\\n--gateway 10.10.10.1 \\\n--ip-range 10.10.10.0/24 \\\n--subnet 10.10.10.0/24\n</code></pre> <p>You can decide which method (portainer or cli) is best for you.</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#deploy-app-container-from-template","title":"Deploy App (Container) From Template","text":"<p>This is the easies way in Portainer to deploy an application. Choose \"App Templates\" from the menu, and click on Docker \"Registry\":</p> <p></p> <p>And Fill Name and Network, then click on \"Deploy the container\":</p> <p></p> <p>Chheck the container:</p> <p></p> <p>You can click on it and see all the details. </p> <p>Try out our new docker Registry:</p> <p>Commands<pre><code>docker tag registry:latest 127.0.0.1:49153/registry/registry:v1\ndocker push 127.0.0.1:49153/registry/registry:v1\n</code></pre> Outptu<pre><code>The push refers to repository [127.0.0.1:49153/registry/registry]\n6da1e15d5d7f: Pushed\nd385a2515a0f: Pushed\nd661c8a70d1e: Pushed\n02ada6f7a843: Pushed\n39982b2a789a: Pushed\nv1: digest: sha256:b0b8dd398630cbb819d9a9c2fbd50561370856874b5d5d935be2e0af07c0ff4c size: 1363\n</code></pre></p> <p>We successfully pushed the locally stored portainer image to our new registry. Unfortunately there is no easy way the list the images form the custom registry. If you need a gui please check this: https://github.com/Joxit/docker-registry-ui</p> <p>At the moment we don't have any other choice than use the API:</p> <pre><code>curl -s http://127.0.0.1:49153/v2/_catalog | jq\n{\n  \"repositories\": [\n    \"registry/registry\"\n  ]\n}\n\ncurl -s http://127.0.0.1:49153/v2/registry/registry/tags/list | jq\n{\n  \"name\": \"registry/registry\",\n  \"tags\": [\n    \"v1\"\n  ]\n}\n</code></pre>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#deploy-single-container","title":"Deploy Single Container","text":"<p>We are going to deploy the registy ui, metioned in the previous chapter.</p> <p>Docker HUB link: https://hub.docker.com/r/joxit/docker-registry-ui</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#name-registry-an-port-mapping","title":"Name, Registry An Port Mapping","text":"<ul> <li>Name: registriy-ui</li> <li>Registry: DockerHub</li> <li>Image: joxit/docker-registry-ui</li> <li>Always pull the image: OFF</li> <li>Publish all exposed network ports to random host ports: ON</li> <li>Publish Port 18081 --&gt; 80</li> </ul> <p>Important</p> <p>You have to pull the image first, if you use always pull image: off <code>docker pull joxit/docker-registry-ui</code> Or you can use the Portainer Image menu for pulling the image.</p> <p>Do not use \"Publish all exposed network ports to random host ports\"! Every time the container restart new host port will be assigned!</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#network","title":"Network","text":"<p>Use the previusly defined network:</p> <p></p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#environments","title":"Environments","text":"<p>Check the official documentation on DockerHUB: https://hub.docker.com/r/joxit/docker-registry-ui</p> <pre><code>REGISTRY_URL=http://localhost:18081\nREGISTRY_TITLE=Test Registry @ localhost\nSHOW_CONTENT_DIGEST=true\nSINGLE_REGISTRY=true\nNGINX_PROXY_PASS_URL=http://registry:5000\n</code></pre> <p>Screenshot</p> <p></p> <p>Warning</p> <p>In the above screenshot the port number is wrong at <code>REGISTRY_URL</code>. The correct one is: <code>REGISTRY_URL=http://localhost:18081</code></p> <ul> <li>REGISTRY_URL</li> </ul> <p>Use the host on which you want to access your registry-ui. It should be the same as used in Docker port publishing (-p), or at portainer gui. Another example:</p> <p></p> <p>In this case the REGISTRY_URL should be <code>http://10.5.0.2:18081</code>.</p> <ul> <li>NGINX_PROXY_PASS_URL</li> </ul> <p>The most important is attaching both the registry and registry-ui to the same network (custom_bridge)! In this way containers can access each other inside this network with there name.</p> <p>For better understanding please see the following examples:</p> <pre><code>docker ps\nCONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS          PORTS                                                      NAMES\ndcbf77aa6492   joxit/docker-registry-ui:latest       \"/docker-entrypoint.\u2026\"   11 minutes ago   Up 11 minutes   0.0.0.0:18081-&gt;80/tcp                                      registriy-ui\n031ffbed2392   registry:latest                       \"/entrypoint.sh /etc\u2026\"   4 hours ago      Up 4 hours      0.0.0.0:49153-&gt;5000/tcp                                    registry\n6c543ab5794b   portainer/portainer-ce:2.9.1-alpine   \"/portainer\"             5 hours ago      Up 5 hours      0.0.0.0:8000-&gt;8000/tcp, 0.0.0.0:9443-&gt;9443/tcp, 9000/tcp   portainer\n\ndocker exec -it registriy-ui /bin/sh\ncurl -i http://registry:5000\nHTTP/1.1 200 OK\nCache-Control: no-cache\nDate: Thu, 14 Oct 2021 16:43:45 GMT\nContent-Length: 0\n\ndocker exec -it registry /bin/sh\nping registriy-ui\nPING registriy-ui (10.1.1.3): 56 data bytes\n64 bytes from 10.1.1.3: seq=0 ttl=64 time=0.068 ms\n64 bytes from 10.1.1.3: seq=1 ttl=64 time=0.098 ms\n^C\n--- registriy-ui ping statistics ---\n2 packets transmitted, 2 packets received, 0% packet loss\nround-trip min/avg/max = 0.068/0.083/0.098 ms\n</code></pre> <p>Regarding the port, inside a Docker network always use the port on which the process itself is listening on, NOT the published port. (<code>0.0.0.0:49153-&gt;5000/tcp</code>)</p> <p>If you get error like this: <pre><code>An error occured: Check your connection and your registry must have `Access-Control-Allow-Origin` header set to `http://127.0.0.1:18081`\n</code></pre></p> <p>This is likely becase the <code>REGISTRY_URL</code> is miss-configured.</p> <p>If everything went good, the Registry-UI web should look like this:</p> <p></p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#stack-compose","title":"Stack (compose)","text":"<p>What Is The Problem With Single containers?</p> <p>Deploying singe containers is suitable only for a quick test. Main problems:</p> <ul> <li>If you accidentally delete the container you loose all it's configuration (port mapping, environments, volume and network settings).</li> <li>You can not easily reproduce the container on another host</li> <li>If you need to connect two or more container to each other you must manually assign them to the same network, and even the network has to be created manually.</li> <li>Containers won't be backed up when you create Portainer backup. </li> </ul> <p>The solution is docker-compose, or the the Portainer way = Stack. Create a Stack means writing a docker compose (yaml) file .</p> <p>All we have done before can be defined in a single file:</p> <pre><code>version: \"3\"\nservices:\n  registry:\n    deploy:\n      replicas: 1\n    image: registry:latest\n    container_name: registry\n    restart: always\n    volumes:\n      - /var/lib/registry:/var/lib/registry\n    environment:\n      - TZ=Europe/Budapest\n    ports:\n      - 5000:5000\n    networks:\n      - registy\n  registry-ui:\n    deploy:\n      replicas: 1\n    image: joxit/docker-registry-ui:latest\n    container_name: registry-ui\n    ports:\n      - 18081:80\n    environment:\n      - REGISTRY_URL=http://localhost:18081\n      - REGISTRY_TITLE=Test Registry @ localhost\n      - SHOW_CONTENT_DIGEST=true\n      - SINGLE_REGISTRY=true\n      - NGINX_PROXY_PASS_URL=http://registry:5000\n    networks:\n      - registy\nnetworks:\n  registy:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n       - subnet: \"10.101.0.1/29\"\n</code></pre> <p>Copy and paste this yaml to your Portainer:</p> <p></p> <p>And Click \"Deploy the stack\" button.</p> <p>This Stack will create a network:</p> <p></p> <p>And the Stack will look like this:</p> <p></p> <p>This <code>yaml</code> file can be used on any Portainer instance or without Portainer using the <code>docker-compose</code> cli tool.</p> <p>Save the above <code>yaml</code> as <code>docker-compose.yaml</code> and start the containers: <pre><code>docker-compose up -d\nCreating network \"registry_registy\" with driver \"bridge\"\nCreating registry-ui ... done\nCreating registry    ... done\n</code></pre></p> <p>You can stop the Stack with <code>docker-compose down</code> command. </p> <p>Warning</p> <p>You have to be in the directory which contains <code>docker-compose.yaml</code> file when running  <code>docker-compose</code> command</p> Tip <p>Do not mix using docker-compose and Portainer Stack, unless you are brave enough. It won't cause any tehcnical problem, but very confusing, I think.</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#stack-with-git","title":"Stack With Git","text":"<p>When you are working on something for hours or days it become really important to have a backup of your work. In this case also important that your work could be reproducible on any other system (portability). Stack are very good start points. But how to track the modifications? How to make backups? Where to store our yaml files? You can create backup from your portainer instance. (Setting / Backup configuration) But unfortunately scheduled backup available only for business users. Workaround could be saving the portainer data directory (<code>/opt/docker/portainer/data</code>). Despite backup we still need track the modifications. </p> <p>Portainer has a great feature: We can use a git repository for creating stacks. I absolutely recommend everybody to use Git repository to store the Stack (comopse) files. </p> <p>Example:</p> <p></p> <p>Tip</p> <p>You can use any Git based repository not just Github (eg.: Self hosted Gitea) If your repository protected with username and password, don't forget to enable \"Authentication\" and provide you credentials.</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#install-portainer-on-kubernetes","title":"Install Portainer On Kubernetes","text":"<p>Installation documantation: https://docs.portainer.io/v/ce-2.9/start/install/server/kubernetes/baremetal</p> <p>Before you install Portainer on the top of your Kubernetes cluster you have to deploy one persistent storage provider.</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#persistent-storage","title":"Persistent Storage","text":"<p>In this article I don't want to bother too much with this topic, therefore I chose a minimal installation of OpenEBS. </p> <p>Link: https://docs.openebs.io/docs/next/uglocalpv-hostpath.html</p> <p>Deploy the operator:</p> <pre><code>kubectl apply -f https://openebs.github.io/charts/openebs-operator-lite.yaml\n</code></pre> <p>Download the StorageClass manifest:</p> <pre><code>wget https://openebs.github.io/charts/openebs-lite-sc.yaml\n</code></pre> <p>And remove the second StorageClass (openebs-device). Or you can use this snippet:</p> <pre><code>cat &lt;&lt;EOF&gt;openebs-lite-sc.yaml\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: openebs-hostpath\n  annotations:\n    openebs.io/cas-type: local\n    cas.openebs.io/config: |\n      #hostpath type will create a PV by\n      # creating a sub-directory under the\n      # BASEPATH provided below.\n      - name: StorageType\n        value: \"hostpath\"\n      #Specify the location (directory) where\n      # where PV(volume) data will be saved.\n      # A sub-directory with pv-name will be\n      # created. When the volume is deleted,\n      # the PV sub-directory will be deleted.\n      #Default value is /var/openebs/local\n      - name: BasePath\n        value: \"/openebs/local/\"\nprovisioner: openebs.io/local\nvolumeBindingMode: WaitForFirstConsumer\nreclaimPolicy: Delete\nEOF\n</code></pre> <p>Apply:</p> <pre><code>kubectl apply -f openebs-lite-sc.yaml\n</code></pre> <p>You can check if it is working:</p> <pre><code>kubectl apply -f https://openebs.github.io/charts/examples/local-hostpath/local-hostpath-pvc.yaml\nkubectl apply -f https://openebs.github.io/charts/examples/local-hostpath/local-hostpath-pod.yaml\n</code></pre> <p>Command<pre><code>kubectl get pvc\n</code></pre> Output<pre><code>NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE\nlocal-hostpath-pvc   Bound    pvc-dda89e6e-43e6-4552-8985-9564b95f5e9a   5G         RWO            openebs-hostpath   27m\n</code></pre></p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#default-storageclass","title":"Default StorageClass","text":"<p>Portainer needs default Storage Class. But the <code>openebs-hostpath</code> is not annotated as default:</p> <p>Command<pre><code>kubectl get sc\n</code></pre> Output<pre><code>NAME               PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nopenebs-hostpath   openebs.io/local   Delete          WaitForFirstConsumer   false                  25m\n</code></pre></p> <p>Fix this:</p> <pre><code>kubectl patch storageclass openebs-hostpath -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n</code></pre> <p>And check again:</p> <p>Command<pre><code>kubectl get sc\n</code></pre> Output<pre><code>NAME                         PROVISIONER        RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nopenebs-hostpath (default)   openebs.io/local   Delete          WaitForFirstConsumer   false                  27m\n</code></pre></p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#deploy-portainer","title":"Deploy Portainer","text":"<pre><code>kubectl apply -n portainer -f https://raw.githubusercontent.com/portainer/k8s/master/deploy/manifests/portainer/portainer.yaml\n</code></pre> <p>Now we have to figure out on which port the Portainer is accessible (nodePort):</p> <p>Command<pre><code>kubectl -n portainer  get svc\n</code></pre> Outout<pre><code>NAME        TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)                                         AGE\nportainer   NodePort   10.22.142.7   &lt;none&gt;        9000:30777/TCP,9443:30779/TCP,30776:30776/TCP   2m31s \n</code></pre></p> <p>The winner is: 9443:30779/TCP</p> <p>Now you can access Portainer on every Kubernetes node using the port number 30779. (<code>http://172.16.1.214:30779</code>)</p>"},{"location":"Blog/2021/10/10/Get_Started_With_Portainer/#final-thoughts","title":"Final Thoughts","text":"<p>I think Portainer can be a very helpful tool for everyone who whats to work with containers, but for managing Kubernetes cluster I think there are bertter software. I always like to see the Kubernetes objects (\"Kinds\") as they are. I like Kubernetes Dashboard much more than Portainer for managing my cluster, but everybody has different taste. I advise to try Kubernetes Dashboard, Rancher, Portainer, K8Sdash and so on, and choose what you really liked.</p> <p>But, if you just now getting familiar to Containers I think Portainer can help you to understand how Networks/Volumes/Stack/Swarms/Images/etc work. Most people like to see things on a graphical interface not just the CLI, this is where the Portainer is great. But I think CLIs (kubectl, oc, docker-compose, docker) are the most powerfull tools, if you are willing to learn them. All the outputs can be piped to grep, sed, jq, etc, and you can write scripts to automate your daily work or process the outputs.</p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/","title":"Install And Configure Wireguard VPN","text":""},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#preface","title":"Preface","text":"<p>I think it is not necessary to emphasize how important you privacy on the public Internet. In the world of high speed internet more and more applications are running at your home, eg.: OpenHab, Plex, Private NAS, etc. People usually want to access these services outside the home network too. It is not recommended to publish these service directly on your public ip address:</p> <ul> <li>It is becoming more common to use CGN Nat by many internet providers. This situation make it impossible to publish your service on Public IP address, because you won't have any.</li> <li>Everybody on the public Internet will have access to your private servers. Hacker can easily find your service and may steal your sensitive data.</li> <li>You have to make effort to configure NAT in your router.</li> <li>Usually consumer internet providers assign IP address dynamically. In this case you have to choose a dynamic DNS service. One of the best I think is DudkDNS, it is completely free for use. You need to update you ip address somehow (router, shell script, etc.). DuckDNS supports a lot of method to do this.</li> </ul> <p>Not all of above could be eliminated with Wireguard VPN:</p> <ul> <li>If you don't have static public ip address you still need NAT configuration and DynDNS service.</li> <li>With CGN Nat it is still impossible to get into your home network.</li> </ul> <p>If you are looking for a really zero-configuration VPN solution your best option may be Tailscale.  It can be easily installed on any popular platforms (Linux, Mac, Windows, Android or IOS). If you are interested in how Tailscale solve the NAT and dynamic IP problems you should read this article: https://tailscale.com/blog/how-nat-traversal-works/. And the best: it's completely free up to 20 devices. </p> <p>But this article is not about Tailscale, but Wireguard.</p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#install-wireguard","title":"Install Wireguard","text":"<p>Quote</p> <p>WireGuard\u00ae is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. Initially released for the Linux kernel, it is now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry. </p> <p>Official Web Page: https://www.wireguard.com/install/</p> <p>The install process is the same as Server or Client, but WireGuard is a decentralized VPN solution so there is no classic Server-Client terminology as in case of example OpenVPN. </p> <p>I have VPS server with static and public IP address. This machine will be the \"Server\". Unfortunately lack of at least one static ip address makes the situation complicated, and there is no overall (magic) solution. So later in this article it is assumed that you have at least one public, static ip address. I'm going to highlight how can you partially solve the lack of the public ip, but everything is only Workaround and solve the problems only partially.</p> <p>Install Wireguard:</p> <pre><code>apt-get install wireguard wireguard-tools qrencode\n</code></pre> <p>Warning</p> <p>Users with Debian releases older than Bullseye should enable backports.</p> <p>You may want to check the official install doc if you have another system than Debian based OS: https://www.wireguard.com/install/</p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#configure-the-server","title":"Configure The Server","text":"<ul> <li> <p>Private Key <pre><code>export PRIVATE_KEY=$( wg genkey )\n</code></pre></p> </li> <li> <p>Public Key <pre><code>export PUBLIC_KEY=$( echo $PRIVATE_KEY | wg pubkey )\n</code></pre></p> </li> </ul> <p>Now you have to choose a public IP address range which never will overlap any of your existing network. Example: <code>10.9.0.0/24</code></p> <ul> <li> <p>Create The Interface Configuration file <code>/etc/wireguard/wg0.conf</code> <pre><code>cat &lt;&lt;EOF&gt;/etc/wireguard/wg0.conf\n# PubKey: $PUBLIC_KEY\n[Interface]\nAddress = 10.9.0.1/32\nListenPort = 51820\nPrivateKey = $PRIVATE_KEY\nEOF\n</code></pre></p> </li> <li> <p>Start the VPN Server <pre><code>systemctl enable wg-quick@wg0\nsystemctl start wg-quick@wg0\n</code></pre></p> </li> <li> <p>Check The Interface Command<pre><code>ifconfig wg0\n</code></pre> Ouptut<pre><code>wg0: flags=209&lt;UP,POINTOPOINT,RUNNING,NOARP&gt;  mtu 1420\n        inet 10.9.0.1  netmask 255.255.255.255  destination 10.9.0.1\n        unspec 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00  txqueuelen 1000  (UNSPEC)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0 &lt;/UP,POINTOPOINT,RUNNING,NOARP&gt;\n</code></pre></p> </li> </ul>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#create-peer-client-config","title":"Create Peer (Client) Config","text":""},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#collect-things-we-need","title":"Collect Things We Need","text":"<pre><code># Private Key (Client)\nwg genkey\niEypOx3Xt5HE++e5I5udO8oJ+bArSoEXqK3XvuvFeXo=\n\n# Public Key (Client)\necho \"iEypOx3Xt5HE++e5I5udO8oJ+bArSoEXqK3XvuvFeXo=\" | wg pubkey\nk6k5GnW3+JJSmzCqEQzkZyFyg7OFO3RhiXhSXur5tFI=\n\n# The Public Key Of the Server\ncat /etc/wireguard/wg0.conf | tr -d ' ' | grep -oP '(?&lt;=PrivateKey=).*[^$]' | wg pubkey\nTSXemmthLlXp8gsLSTfcmqgjolvYWmppNhIUeppg/CU=\n\n# Preshared Key\nwg genpsk\nSamqdyf9gVcfEUPCS52I1hJCLMlXAmHoitk1l5y9UO0=\n</code></pre> <p>Peer Config File:</p> <pre><code>[Interface]\nAddress = 10.9.0.2/24\nListenPort = 51820\nPrivateKey = iEypOx3Xt5HE++e5I5udO8oJ+bArSoEXqK3XvuvFeXo=\n\n[Peer]\nPublicKey = TSXemmthLlXp8gsLSTfcmqgjolvYWmppNhIUeppg/CU=\nPresharedKey = Samqdyf9gVcfEUPCS52I1hJCLMlXAmHoitk1l5y9UO0=\nEndPoint=172.16.1.214:51820\nPersistentKeepalive = 25\nAllowedIPs = 10.9.0.0/24\n</code></pre> <p>This file should be saved on your client (<code>/etc/wireguard/wg0.conf</code>). Or save it to a file (<code>01.conf</code>) and generate QR code for you Android or IOS client:</p> <pre><code>qrencode -t ansiutf8 &lt;01.conf\n</code></pre> <p>Before you start using your client you need to update your Wireguard Server. Add these lines:</p> <p><pre><code>cat &lt;&lt;EOF&gt;&gt;/etc/wireguard/wg0.conf\n\n[Peer]\nPublicKey = k6k5GnW3+JJSmzCqEQzkZyFyg7OFO3RhiXhSXur5tFI=\nPresharedKey = Samqdyf9gVcfEUPCS52I1hJCLMlXAmHoitk1l5y9UO0=\nPersistentKeepalive = 25\nAllowedIPs = 10.9.0.2/32\n\nEOF\n</code></pre> * Reload The Server Config (Without Interrupting Connections)</p> <pre><code>wg syncconf wg0 &lt;(wg-quick strip wg0)\n</code></pre>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#peer-configs","title":"Peer Configs","text":"<p>At fist sight it could be confusing, but if you look closer I hope will understand the configs.</p> <ul> <li>PrivateKey: Should never shared. Unique across all peers and appear only in <code>[interface]</code> section.</li> <li>PublicKey: Generated from <code>PrivateKey</code>. <ul> <li>Should shared across peers. </li> <li>Peer1 gets the PublicKey of Peer2, and vice versa, </li> <li>Peer2 gets the PublicKey of Peer1.</li> </ul> </li> <li>PresharedKey: Should be the same between two peers.<ul> <li>Peer1 and Peer2 use the same PresharedKey1, </li> <li>Peer2 and Peer3 use the same PresharedKey2, </li> <li>Peer1 and Peer3 use the same PresharedKey3.</li> <li>This assumes that all of these three peers connected each other (decentralized connection), and there will be three different PresharedKey.</li> </ul> </li> </ul> <p>I try to explain through the following schema of <code>[peer]</code> configs. Only The keys are mentioned. Interface configs are always unique, does not have common parts.</p> <p>Peer1 Config</p> <pre><code>[Peer]\nPublicKey = Public Key Of Peer 2 (echo [PRIVATE_KEY OF PEER2] | wg pubkey)\nPresharedKey = Shared Key12 \n\n[Peer]\nPublicKey = Public Key Of peer 3 (echo [PRIVATE_KEY OF PEER3] | wg pubkey)\nPresharedKey = Shared Key13\n</code></pre> <p>Peer2 Config</p> <pre><code>[Peer]\nPublicKey = Public Key Of Peer 1 (echo [PRIVATE_KEY OF PEER1] | wg pubkey)\nPresharedKey = Shared Key12 \n\n[Peer]\nPublicKey = Public Key Of peer 3 (echo [PRIVATE_KEY OF PEER3] | wg pubkey)\nPresharedKey = Shared Key23\n</code></pre> <p>Peer3 Config</p> <pre><code>[Peer]\nPublicKey = Public Key Of Peer 1 (echo [PRIVATE_KEY OF PEER1] | wg pubkey)\nPresharedKey = Shared Key13\n\n[Peer]\nPublicKey = Public Key Of peer 2 (echo [PRIVATE_KEY OF PEER2] | wg pubkey)\nPresharedKey = Shared Key23\n</code></pre>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#endpoint","title":"EndPoint","text":"<p>One out of two peers have to know where to find the other.</p> ID Peer1 Know Peer2 IP adddress Peer2 Know Peer1 IP adddress Comment 1 yes yes Best Situation. PersistentKeepalive is not needed. 2 yes no Good Situation. PersistentKeepalive should be set on Peer1. 3 no yes Good Situation. PersistentKeepalive should be set on Peer2. 4 no no Worst Situation. No Connection could be made. <ul> <li>First Situation</li> </ul> <p>This the best scenario. Both clients have its own static public ip address. You can configure the Endpoint each side to point each other.</p> <ul> <li>Second And Third Situation</li> </ul> <p>You have at least one peer - the server(s) - with static public IP address, and one or more clients. The Endpoint configuration on all clients the same and points to the server' IP address and port. On the server side you should not use any Endpoint declaration for the Clients.</p> <ul> <li>Worst Situation</li> </ul> <p>In this scenario all of your peers are behind NAT or CNG Nat. Example of 2 peers:</p> <p>The first peer is behind NAT in your home (private) network. You have port forwarding, firewall and dynamic DNS set up. The second peer is your mobile phone with mobile internet.</p> <p>In your configuration of the first peer you can not specify anything as Endpoint, since you don't know the always actual ip address of you mobile phone. In the configuration of your mobile client you can specify your DynDNS host name (eg.: test-wg.duckdns.org).  </p> <p>The problem here: If your public IP address (to which the DynDns provider points to) changes you have to restart the Wireguard client on your mobile phone. If you can live together with this limitation Wireguard will work fine in this kind of setup.</p> <p>BUT! If the first peer behind CGN Nat, you don't have any option to get Wireguard works. If you can not specify at least one Enpoint, Wireguard just won't work.</p> <p>If you want to read about how Tailscale solve this kind of issue click on this link: https://tailscale.com/blog/how-nat-traversal-works/</p> <p>Direct Connection Matrix:</p> Connect FROM \\ TO Server Peer (91.12.21.142) Peer2 (inside priv net) Peer3 (inside priv net) Server Peer (91.12.21.142) X Doesn't know peer2 address Doesn't know peer3 address Peer2 (inside priv net) <code>EndPoint: 91.12.21.142:51820</code> <code>PersistentKeepalive = 25</code> X No Ip addresses is known Peer3 (inside priv net) <code>EndPoint: 91.12.21.142:51820</code> <code>PersistentKeepalive = 25</code> No Ip addresses is known X <ul> <li>Server Peer (91.12.21.142) Has Static Public IP address (91.12.21.142)<ul> <li>When you configure peers on the server side usually no <code>EndPoint</code> and nor <code>PersistentKeepalive</code> is set up.</li> </ul> </li> <li>Peer2 (inside priv net) AND Peer3 (inside priv net) are behind NAT. (Home network or your phone/tablet with mobile internet)<ul> <li>In the clients configuration we specify where to find the Server (<code>EndPoint</code>) and   <code>PersistentKeepalive</code> to update its ip address periodically.</li> </ul> </li> </ul> <p>Info</p> <p>The mentioned <code>EndPoint</code> and <code>PersistentKeepalive</code> settings go to the clients config not to the server's one.</p> <p>As we discussed earlier general peers may be behind CGN NAT, or inside your home network, or even could be your mobile phone with mobile internet connection. These peers don't have static, public IP addresses. Of course you can open port on your firewall and set up NAT, use dynamic DNS, etc., but Wireguard check DNS record only when it starts. Every time your router gets a new public ip address you should trigger Wireguard restart. This situation could not be  solved easily, and this is where Tailscale comes to the picture. If all of your peers are behind NAT you should use Tailscale instead of Wireguard. You probably won't find any other good and stable solution. </p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#summarize","title":"Summarize","text":"<p>Now we have a server and a client configuration:</p> <ul> <li>Server</li> </ul> <pre><code>[Interface]\nAddress = 10.9.0.1/32\nListenPort = 51820\nPrivateKey = oMQf0fLAvnIoiLr+z4zmOlETxA0eR/Z7oPHhXRpQoEY=\n\n\n[Peer]\nPublicKey = k6k5GnW3+JJSmzCqEQzkZyFyg7OFO3RhiXhSXur5tFI=\nPresharedKey = Samqdyf9gVcfEUPCS52I1hJCLMlXAmHoitk1l5y9UO0=\nPersistentKeepalive = 25\nAllowedIPs = 10.9.0.2/32\n</code></pre> <ul> <li>Client</li> </ul> <pre><code>[Interface]\nAddress = 10.9.0.2/24\nListenPort = 51820\nPrivateKey = iEypOx3Xt5HE++e5I5udO8oJ+bArSoEXqK3XvuvFeXo=\n\n[Peer]\nPublicKey = TSXemmthLlXp8gsLSTfcmqgjolvYWmppNhIUeppg/CU=\nPresharedKey = Samqdyf9gVcfEUPCS52I1hJCLMlXAmHoitk1l5y9UO0=\nEndPoint=172.16.1.214:51820\nPersistentKeepalive = 25\nAllowedIPs = 10.9.0.0/24\n</code></pre> <p>We haven't talk about two important things:</p> <ul> <li><code>Interface</code> / <code>Address</code></li> </ul> <p>Quote</p> <p>The Address setting is the virtual address of the local WireGuard peer. It\u2019s the IP address of the virtual network interface that WireGuard sets up for the peer;     and as such you can set it to whatever you want (whatever makes sense for the virtual WireGuard network you\u2019re building).    </p> <p>Like with other network interfaces, the IP address for a WireGuard interface is defined with a network prefix, which tells the local host what other IP addresses     are available on the same virtual subnet as the interface. In the above example, this prefix is /32 (which generally is a safe default for a WireGuard interface).         If we set it to /24, that would indicate to the local host that other addresses in the same /24 block as the address itself (10.0.0.0 to 10.0.0.255) are routable     through the interface.</p> <ul> <li><code>Peer</code> / <code>AllowedIPs</code></li> </ul> <p>Quote</p> <p>Is the set of IP addresses the local host should route to the remote peer through the WireGuard tunnel. This setting tells the local host what goes in tunnel.</p> <p>Example: <code>AllowedIPs = 10.8.0.0/24,192.168.100.0/24</code></p> <p>Basically this means that traffic to <code>10.8.0.0/24</code> and <code>192.168.100.0/24</code> is routed to the <code>wg0</code> interface (route command):</p> <pre><code>Kernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         _gateway        0.0.0.0         UG    100    0        0 eth0\n10.8.0.0        0.0.0.0         255.255.255.0   U     0      0        0 wg0\n....\n....\n192.168.100.0   0.0.0.0         255.255.255.0   U     0      0        0 wg0\n</code></pre> <p>Read more:</p> <ul> <li>https://stackoverflow.com/questions/65444747/what-is-the-difference-between-endpoint-and-allowedips-fields-in-wireguard-confi</li> <li>https://www.procustodibus.com/blog/2021/01/wireguard-endpoints-and-ip-addresses/</li> </ul>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#topology-examples","title":"Topology Examples","text":""},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#wireguard-server","title":"WireGuard Server","text":"<ul> <li>Match the colors: Config and the element have the same color. Example green peer config text goes to the green client.</li> <li>The \"Cloud\" pictogram means the public Internet.</li> <li>VPN and physical connections are separately shown in the pictures. VPN connections are indicated by dashed lines.</li> <li>The WireGuard server sitting on a VPS. This VPS has static, public IP address.</li> <li>All (4) peers connect to this WireGuard Server.<ul> <li>WireGuard Server don't have EndPoint configurations to the peers, since the IP addresses are not known of the peers. Therefore <code>PersistentKeepAlive</code> is not necessary.</li> </ul> </li> <li>Peers have <code>PersistentKeepAlive</code> set to 25 seconds. This means all peer update its EndPoint on the ServerSide.</li> <li>Peer configurations between Client1 and Client3 have no <code>PersistentKeepAlive</code>  set up, because they know each other IP addresses. </li> <li>Client1 and Client3 have direct VPN connection to each other.</li> <li>Client1 has 2 peer configured:<ul> <li>Connection to the server</li> <li>Connection to Client3</li> </ul> </li> <li>Client3 has 2 peer configured<ul> <li>Connection to the server</li> <li>Connection to Client1</li> </ul> </li> <li>Client2 has one peer configured: to the Wireguard Server.</li> <li>\"Your mobile phone\" has only one peer configured, to the WireGuard server. <ul> <li>The mobile cient access other peers over the WireGuard server.</li> <li>Mobile clients can access the others on their VPN IP address (10.10.0.0/24)</li> </ul> </li> <li>If the WireGuard server fails your mobule cient lose connection to all peers. Only Client1 and Client3 can access each other over the VPN without the server.</li> <li>The connection between Client2 and Client3 or Client2 and Client1 goes through the server.</li> <li>This setup works even when your private home network is behind CGN NAT.</li> </ul> <p>Disatvantages:</p> <ul> <li>You need a server on the public internet (VPS). There are a lot of cheap VPS provider, so if you willing to pay for a VPS you can easily find one even under $5. (Be carefully, most VPS provider limit the bandwidth.)</li> </ul> <p>Tip</p> <p>Connection could work without <code>PersistentKeepAlive</code> as well. BUT, the tunnel will only be establish after the client send at least one package to the other peer.</p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#dynamic-dns","title":"Dynamic DNS","text":"<ul> <li>Network Address Translation is set on your home router<ul> <li>Incomming connection to 55870 goes to Client1</li> <li>Incomming connection to 55871 goes to Client3</li> </ul> </li> <li>You must configure a Duckdns client to update the ip address of test-wg.duckdns.org.</li> <li>The mobile clint has (peer) configuration to Client1 and Client3.</li> <li>Client1 and Client3 have direct peer configuration. </li> <li><code>Endpoint</code> must set on the mobile client. Mobile client knows where to find the other peers, but the other clients don't know the IP address of the mobile client. <code>PersistentKeepalive is optional but recommended.</code></li> </ul> <p>Disatvantages:</p> <ul> <li>If your public IP address changes you have to manually restart your mobile WG Client.</li> <li>You have to configure NAT and DynDNS provider.</li> <li>Won't work if you are behind CGN NAT.</li> </ul>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#wireguard-gateway","title":"Wireguard Gateway","text":"<ul> <li>Moblie client can access the entire private home network through  the VPN gateway.<ul> <li>This means that traffic from the mobile client to the <code>172.16.0.0/22</code> first goes to the WG Gateway over the VPN tunnel. After the <code>iptables</code> rule (masquerade) applied the package reaches the target machine via the physical home network. </li> <li>Flow: <code>Mobile Client (wg:10.10.0.13) --&gt; (wg0:10.10.0.254) Wireguard Gateway (eth0:172.16.0.254) --&gt; (eth0:172.16.0.22) Machine</code></li> </ul> </li> <li>Similar to the previous scenario, except you don't need to install WG on all your internal device which you want to access. </li> </ul> <p>Disatvantages:</p> <ul> <li>All of them which is included in the previous scenario.</li> <li>You should carefully set up iptables. In this example the entire home network is open to the mobile client.</li> <li>Machines inside your home network without WireGuard don't access the VPN network without extra configuration.<ul> <li>This means you have to configure the routing table in your router to send traffic to the VPN gateway. </li> <li>Or add route to the routing table of all machines from where you want to access VPN ip addresses.</li> <li>Extra iptalbes rules also have to be added to the VPN gateway.</li> </ul> </li> </ul>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#generate-peer-config-using-bash-script","title":"Generate Peer Config Using Bash Script","text":"<p>First let see the script:</p> Click Here For Raw Source<pre><code>#!/bin/bash\n\nVPN_IP_ADDRESS=\"10.8.0\"\nPRIVATE_KEY=$( wg genkey )\nPUBLIC_KEY=$( echo $PRIVATE_KEY | wg pubkey )\nNEXT_IP_ADDR=$( cat /etc/wireguard/clients/ip.txt )\n\nPUBLIC=$( cat ../wg0.conf | tr -d ' ' | grep -oP '(?&lt;=PrivateKey=).*[^$]' | wg pubkey )\nPRESHARED=$( wg genpsk )\n\nread -p 'Config Name (peername): ' peername\npeer=$( echo $peername | tr '[:upper:]' '[:lower:]' | tr -d ' ' )\n\nread -p 'Endpoint (leave blank for 23.88.60.51:51820): ' ENDPOINT\nENDPOINT=\"${ENDPOINT:-23.88.60.51:51820}\"\n\nPUBLIC=$( cat ../wg0.conf | tr -d ' ' | grep -oP '(?&lt;=PrivateKey=).*[^$]' | wg pubkey )\nPRESHARED=$( wg genpsk )\n\ncat &lt;&lt;EOF &gt; $peer.conf\n#:${PUBLIC_KEY},${VPN_IP_ADDRESS}.${NEXT_IP_ADDR},${peer}\n[Interface]\nAddress = ${VPN_IP_ADDRESS}.${NEXT_IP_ADDR}/32\nListenPort = 51820\nPrivateKey = ${PRIVATE_KEY}\n#DNS = 1.1.1.1\n\n[Peer]\nPublicKey = ${PUBLIC}\nPresharedKey = ${PRESHARED}\nEndPoint=${ENDPOINT}\nPersistentKeepalive = 25\nAllowedIPs = ${VPN_IP_ADDRESS}.0/24\nEOF\n\nqrencode -t ansiutf8 &lt; $peer.conf\n\necho \"==================== wg0 config change: ====================\"\ncat &lt;&lt;EOF \n\n[Peer] # Config File: $peer.conf\nPublicKey = ${PUBLIC_KEY}\nPresharedKey = ${PRESHARED}\nPersistentKeepalive = 25\nAllowedIPs = ${VPN_IP_ADDRESS}.${NEXT_IP_ADDR}/32\n\nEOF\n\n((NEXT_IP_ADDR=NEXT_IP_ADDR+1))\necho $NEXT_IP_ADDR &gt;/etc/wireguard/clients/ip.txt\n\necho \"==================== Client Config ====================\"\ncat $peer.conf\n\necho\necho -e  \"Run command to reload Wireguard: \\nwg syncconf wg0 &lt;(wg-quick strip wg0)\"\n</code></pre>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#configuration","title":"Configuration","text":"<ul> <li>This script generate only the clients configurations, the server config must be manually created.</li> <li>Put this script to <code>/etc/wireguard/clients</code> on the VPN server and run it from this directory (<code>./script-name.sh</code>). </li> <li>Modify line 3 to change your VPN IP address range. Only <code>/24</code> is supported! </li> <li>Initialize the first client IP address by running <code>echo -n 2 &gt;/etc/wireguard/clients/ip.txt</code> command.</li> <li>Change Line 14 and 15 to set up the default IP address of the VPN server.</li> <li>Optional settings:<ul> <li>Line 23 (Interface/Address): You can change the subnet bits (default: <code>/32</code>).</li> <li>Line 25 : Change the listen port of the client, if you don't like the <code>51820</code>.</li> </ul> </li> </ul> <p>Caution</p> <p>Do not forget to enable Linux IP forwarding! Check: <code>cat /proc/sys/net/ipv4/ip_forward</code> Enable: <code>sysctl -w net.ipv4.ip_forward=1</code> Apply the change: <code>sysctl -p</code></p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#generate-the-server-config","title":"Generate The Server Config","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/wg0.conf\n[Interface]\nAddress = 10.8.0.2/32\nListenPort = 51820\nPrivateKey = $( wg genkey )\nDNS = 1.1.1.1\nEOF\n</code></pre> <p>Copy <code>wg0.conf</code> to the default config directory.</p> <pre><code>cp /tmp/wg0.conf /etc/wireguard\n</code></pre>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#generate-your-first-peer","title":"Generate Your First Peer","text":"<pre><code>cd /etc/wireguard/clients\n./generate.sh \nConfig Name (peername): test-peer01\nEndpoint (leave blank for 232.188.60.51:51820): \n\n# Here comes the QR Code\n\n==================== wg0 config change: ====================\n\n[Peer] # Config File: test-peer01.conf\nPublicKey = ob1**********QqyiwM8YxQv/1JHqP+pN7lK/QSegxc=\nPresharedKey = ICU35s/KULp3j8iP**********Nn6TLvrvFaefKez68=\nPersistentKeepalive = 25\nAllowedIPs = 10.8.0.28/32\n\n==================== Client Config ====================\n#:ob1**********QqyiwM8YxQv/1JHqP+pN7lK/QSegxc=,10.8.0.28,test-peer01\n[Interface]\nAddress = 10.8.0.28/32\nListenPort = 51820\nPrivateKey = iDtl*********t8pKVxWeVGlcG3h27/w/stjHvUg1o=\n#DNS = 1.1.1.1\n\n[Peer]\nPublicKey = qanM0i7p**********E/cvw0LDvN0UdW9VaxuBfRIn4=\nPresharedKey = ICU35s/KULp3j8iP**********Nn6TLvrvFaefKez68=\nEndPoint=23.88.60.51:51820\nPersistentKeepalive = 25\nAllowedIPs = 10.8.0.0/24\n\nRun command to reload Wireguard: \nwg syncconf wg0 &lt;(wg-quick strip wg0)\n</code></pre> <ul> <li>The script will generate QR code for mobile client, and the config file: <code>test-peer01.conf</code><ul> <li>You should use the generated config file or copy-paste from the script output.</li> <li>If you want to use mobile device (android or ios), use the generated QR code. You can regenerate this code with <code>qrencode -t ansiutf8 &lt;test-peer01.conf</code> command.</li> </ul> </li> <li>The script doesn't automatically update the server <code>wg0.conf</code> file.<ul> <li>You have to manually add the \"wg0 config change:\" section to the server config (<code>wg0.conf</code>).</li> <li>It you want to apply the changes without interrupt the existing connection run the <code>wg syncconf wg0 &lt;(wg-quick strip wg0)</code> command.</li> </ul> </li> </ul> <p>This little script is suitable only for managing few nodes, and may help to understand how Wireguard works. Writing an overall configuration manager was not my goal. If you don't like my script never mind, there are a lot of alternatives out there:</p> <ul> <li>Wireguard-manager</li> <li>wg-gui</li> <li>subspace</li> <li>wg-manager</li> <li>mistborn</li> <li>Read more: https://medium.com/swlh/web-uis-for-wireguard-that-make-configuration-easier-e104710fa7bd</li> </ul>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#summary","title":"Summary","text":"<p>Maybe this article a bit long, and you get discouraged, because Wireguard looks so complicated, but it doesn't.  That's why I copy here a complete example with 3 peers. All peers are in the same private network, but if you get familiar with Wireguard and understand the configuration you can adopt these examples in your network.</p>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#peer-interface-configs","title":"Peer Interface Configs","text":"<p>Before you run these snippets don't forget to modify the followings:</p> <ul> <li><code>Interface / Address</code> : The desired VPN IP address of the peers. You can change the subnet bit to <code>/32</code>.</li> <li><code>Interface / ListenPort</code> : (Optional) On which port should the Wireguard service listen</li> <li><code>Peer / EndPoint</code> : The actual IP address of the peer in the existing network. (If you have modified the port don't forget to match to it.)</li> <li><code>Peer /AllowedIPs</code> : Since all peer connect to each other you have to set AllowedIPS to the VPN address of the peers. If you set to other than <code>/32</code> all traffic to this subnet tries to go through the peer. </li> <li><code>Interface / DNS</code> : You can delete these lines. </li> </ul> <p>Peer1</p> <pre><code>cat &lt;&lt;EOF&gt;/tmp/peer1-interface.conf\n[Interface]\nAddress = 10.22.0.2/24\nListenPort = 55321\nPrivateKey = $( wg genkey )\nDNS = 1.1.1.1\n\nEOF\n</code></pre> <p>Peer2</p> <p><pre><code>cat &lt;&lt;EOF&gt;/tmp/peer2-interface.conf\n[Interface]\nAddress = 10.22.0.3/24\nListenPort = 55321\nPrivateKey = $( wg genkey )\nDNS = 1.1.1.1\n\nEOF\n</code></pre> Peer3</p> <pre><code>cat &lt;&lt;EOF&gt;/tmp/peer3-interface.conf\n[Interface]\nAddress = 10.22.0.4/24\nListenPort = 55321\nPrivateKey = $( wg genkey )\nDNS = 1.1.1.1\n\nEOF\n</code></pre>"},{"location":"Blog/2021/10/20/Install_And_Configure_Wireguard_VPN/#peer-configs_1","title":"Peer Configs","text":"<p>Preshared Keys</p> <pre><code>export PRESHARED_12=$( wg genpsk )\nexport PRESHARED_13=$( wg genpsk )\nexport PRESHARED_23=$( wg genpsk )\n</code></pre> <ul> <li><code>PRESHARED_12</code> --&gt; Connection between peer 1 and 2</li> <li><code>PRESHARED_13</code> --&gt; Connection between peer 1 and 3</li> <li><code>PRESHARED_23</code> --&gt; Connection between peer 2 and 3</li> </ul> FROM   \\  TO Peer1 Peer2 Peer3 Peer1 X PRESHARED_12 PRESHARED_13 Peer2 PRESHARED_12 X PRESHARED_23 Peer3 PRESHARED_13 PRESHARED_23 X <p>Endpoints</p> FROM   \\  TO Peer1 Peer2 Peer3 Peer1 X 172.17.0.1:55321 172.16.0.27:55321 Peer2 172.16.1.213:55321 X 172.16.0.27:55321 Peer3 172.16.1.213:55321 172.17.0.1:55321 X <p>AllowedIPs</p> FROM   \\  TO Peer1 Peer2 Peer3 Peer1 X 10.22.0.3/32 10.22.0.4/32 Peer2 10.22.0.2/32 X 10.22.0.4/32 Peer3 10.22.0.2/32 10.22.0.2/32 X <p>Info</p> <p>You can skip the usage of the pre-shared key. It adds extra security, but not mandatory.</p> <p>Peer1</p> <pre><code>export PUBLIC_KEY=$( cat /tmp/peer1-interface.conf | tr -d ' ' | grep -oP '(?&lt;=PrivateKey=).*[^$]' | wg pubkey )\ncat &lt;&lt;EOF | tee -a /tmp/peer2-interface.conf\n# Connection to Peer1\n[Peer]\nPublicKey = $PUBLIC_KEY\nPresharedKey = $PRESHARED_12\nEndPoint=172.16.1.213:55321\n#PersistentKeepalive = 25\nAllowedIPs = 10.22.0.2/32\n\nEOF\n\ncat &lt;&lt;EOF | tee -a /tmp/peer3-interface.conf\n# Connection to Peer1\n[Peer]\nPublicKey = $PUBLIC_KEY\nPresharedKey = $PRESHARED_13\nEndPoint=172.16.1.213:55321\n#PersistentKeepalive = 25\nAllowedIPs = 10.22.0.2/32\n\n\nEOF\n</code></pre> <p>Peer2</p> <pre><code>export PUBLIC_KEY=$( cat /tmp/peer2-interface.conf | tr -d ' ' | grep -oP '(?&lt;=PrivateKey=).*[^$]' | wg pubkey )\ncat &lt;&lt;EOF | tee -a /tmp/peer1-interface.conf \n# Connection to Peer2\n[Peer]\nPublicKey = $PUBLIC_KEY\nPresharedKey = $PRESHARED_12\nEndPoint=172.17.0.1:55321\n#PersistentKeepalive = 25\nAllowedIPs = 10.22.0.3/32\n\nEOF\n\ncat &lt;&lt;EOF | tee -a /tmp/peer3-interface.conf\n# Connection to Peer2\n[Peer]\nPublicKey = $PUBLIC_KEY\nPresharedKey = $PRESHARED_23\nEndPoint=172.17.0.1:55321\n#PersistentKeepalive = 25\nAllowedIPs = 10.22.0.3/32\n\nEOF\n</code></pre> <p>Peer3</p> <pre><code>export PUBLIC_KEY=$( cat /tmp/peer3-interface.conf | tr -d ' ' | grep -oP '(?&lt;=PrivateKey=).*[^$]' | wg pubkey )\ncat &lt;&lt;EOF | tee -a /tmp/peer1-interface.conf \n# Connection to Peer3\n[Peer]\nPublicKey = $PUBLIC_KEY\nPresharedKey = $PRESHARED_13\nEndPoint=172.16.0.27:55321\n#PersistentKeepalive = 25\nAllowedIPs = 10.22.0.4/32\n\nEOF\n\ncat &lt;&lt;EOF | tee -a /tmp/peer2-interface.conf\n# Connection to Peer3\n[Peer]\nPublicKey = $PUBLIC_KEY\nPresharedKey = $PRESHARED_23\nEndPoint=172.16.0.27:55321\n#PersistentKeepalive = 25\nAllowedIPs = 10.22.0.4/32\n\nEOF\n</code></pre> <p>We have 3 files created:</p> <pre><code>-rw-r--r--  1 root root  578 Oct 19 20:01 peer1-interface.conf\n-rw-r--r--  1 root root  580 Oct 19 20:01 peer2-interface.conf\n-rw-r--r--  1 root root  580 Oct 19 20:01 peer3-interface.conf\n</code></pre> <ul> <li><code>peer1-interface.conf</code> goes to <code>/etc/wireguard/wg1.conf</code> on peer1</li> <li><code>peer2-interface.conf</code> goes to <code>/etc/wireguard/wg1.conf</code> on peer2</li> <li><code>peer3-interface.conf</code> goes to <code>/etc/wireguard/wg1.conf</code> on peer3</li> </ul> <p>Run on all peers</p> <pre><code>sysctl -w net.ipv4.ip_forward=1\nsysctl -p\nsystemctl enable wg-quick@wg1\nsystemctl start wg-quick@wg1\n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/","title":"Install Matrix Home Server On Kubernetes","text":""},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#preface","title":"Preface","text":"<p>What is Matrix?</p> <p>Quote</p> <p>Matrix is an open standard for interoperable, decentralised, real-time communication over IP. It can be used to power Instant Messaging, VoIP/WebRTC signalling, Internet of Things communication - or anywhere you need a standard HTTP API for publishing and subscribing to data whilst tracking the conversation history.</p> <p>Link: https://matrix.org/faq/#what-is-matrix%3F</p> <p>So we are about to install a private real time messaging (Chat) server. It can be useful for you if you want to replace Whatsapp, Telegram, FB messenger, Viber, etc, or just want your own messaging server. Or if you don't trust in these services and want a service which focuses on your privacy. Another question is how your partners with whom you want to chat trust your server.</p> <p>I'm wondering if you have ever thought  about having your own messaging server. If the answer is yes, it's time to build one. I hope you will easily achieve  this with the help of this article.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#requirements","title":"Requirements","text":"<ul> <li>First and most important to have a valid domain name. If you don't have any you can pick up one free from DuckDNS</li> <li>Installed Kubernets cluster</li> <li>Public Internet access. </li> <li>At least 2 GB of free RAM. </li> </ul> <p>I assume you build this server for your family and friends, and don't want to share with the whole World. For some tens of people you don't need to purchase an expensive server, but according to the number of attachments (file, pictures, videos,etc) you may need some hundreds of GB disk space.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#docker-compose","title":"Docker Compose","text":"<p>Maybe the easiest way to install everything all together is writing a Docker compose file. The compose file below can be used with <code>docker-compose</code> command or as Stack in Portainer.  Later in this article we will use this compose file as reference for writing the Kubernetes manifest files (cm, deployment, sevice, pvc, etc).</p> Click Here For Raw Source<pre><code>version: \"3\"\nservices:\n  matrix:\n    deploy:\n      replicas: 1\n    image: matrixdotorg/synapse:latest\n    container_name: synapse-matrix\n    restart: always\n    volumes:\n      - /opt/docker/matrix/config:/config\n      - /opt/docker/matrix/data:/data\n    environment:\n      - TZ=Europe/Budapest\n      - SYNAPSE_CONFIG_DIR=/data\n      - SYNAPSE_CONFIG_PATH=/config/homeserver.yaml\n    ports:\n      - 8008:8008\n    networks:\n      - matrix\n  caddy:\n    deploy:\n      replicas: 1\n    image: caddy:latest\n    container_name: matrix-web-caddy\n    restart: always\n    volumes:\n      - /opt/docker/matrix/caddy/Caddyfile:/etc/caddy/Caddyfile\n      - /opt/docker/matrix/caddy/srv:/srv\n      - /opt/docker/matrix/caddy/data:/data\n      - /opt/docker/matrix/caddy/config:/config\n    ports:\n      - 80:80\n      - 443:443\n    networks:\n      - matrix\n  postgres:\n    deploy:\n      replicas: 1\n    image: postgres:14.0-alpine\n    container_name: matrix-postgres\n    restart: always\n    environment:\n      - POSTGRES_PASSWORD=MatrixPass\n      - POSTGRES_USER=matrix\n      - PGDATA=/data\n      - TZ=Europe/Budapest\n    volumes:\n      - /opt/docker/matrix/postgres:/data\n    networks:\n      - matrix\nnetworks:\n  matrix:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n       - subnet: \"10.101.0.8/29\"\n</code></pre> <p>You can see that we have 3 services:</p> <ul> <li>matrix : The Matrix server</li> <li>caddy : Web server for use as reverse proxy. <ul> <li>You can use any other web server you like (eg.: Apache httpd, Nginx)</li> <li>I chose Caddy because it is super easy to configure as a reverse proxy and supports automatic SSL certificate generation and maintenance.</li> </ul> </li> <li>postgres : Database engine.<ul> <li>You can skip this if you want to use the default sqlite engine, but it is not recommended for daily (production) use.</li> </ul> </li> </ul> <p>Before you <code>up</code> this compose file create the necessary directories:</p> <pre><code>mkdir -p /opt/docker/matrix/config\nmkdir /opt/docker/matrix/data\nmkdir /opt/docker/matrix/caddy\nmkdir /opt/docker/matrix/caddy/srv\nmkdir /opt/docker/matrix/caddy/data\nmkdir /opt/docker/matrix/caddy/config\nmkdir /opt/docker/matrix/postgres\n</code></pre> <p>Matrix process run as <code>991</code> userID and groupID so we need to run <code>chown</code> command:</p> <pre><code>chown -R 991:991 /opt/docker/matrix\n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#generate-the-matrix-config-file","title":"Generate The Matrix Config File","text":"<p>For generating the initial config files please follow these steps:</p> <p>Command<pre><code>docker run -it --rm \\\n    --mount type=bind,src=/opt/docker/matrix/config,dst=/data \\\n    -e SYNAPSE_SERVER_NAME=matrix.vincze.work \\\n    -e SYNAPSE_REPORT_STATS=yes \\\n    matrixdotorg/synapse:latest generate\n</code></pre> Output<pre><code>Unable to find image 'matrixdotorg/synapse:latest' locally\nlatest: Pulling from matrixdotorg/synapse\n7d63c13d9b9b: Pull complete\n7c9d54bd144b: Pull complete\n6c659176d5c8: Pull complete\n31bfadeaf52b: Pull complete\nb0be2954cd61: Pull complete\n24d50aa74e2c: Pull complete\n1816510873a0: Pull complete\n227c613c4a00: Pull complete\n097ac90fbed0: Pull complete\nDigest: sha256:2c74baa38d3241aaf4a059a7e7c01786ba51ac5fe6fcf473ede3eb148f9358ba\nStatus: Downloaded newer image for matrixdotorg/synapse:latest\nCreating log config /data/matrix.vincze.work.log.config\nGenerating config file /data/homeserver.yaml\nGenerating signing key file /data/matrix.vincze.work.signing.key\nA config file has been generated in '/data/homeserver.yaml' for server name 'matrix.vincze.work'. Please review this file and customise it to your needs.\n</code></pre> Command<pre><code>cd /opt/docker/matrix\nmv ./config/matrix.vincze.work.signing.key ./config/matrix.vincze.work.log.config ./data\nfind data/ config/\n</code></pre> Output<pre><code>data/\ndata/matrix.vincze.work.signing.key\ndata/matrix.vincze.work.log.config\nconfig/\nconfig/homeserver.yaml\n</code></pre></p> <p>Important</p> <p>You have to change <code>SYNAPSE_SERVER_NAME</code> to point to your own domain.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#inititalize-the-database","title":"Inititalize The Database","text":"<ul> <li>Start the a postgres instance</li> </ul> Command<pre><code>docker run -d \\\n--name postgres-init \\\n--env POSTGRES_PASSWORD=rootpass \\\n--env POSTGRES_USER=root \\\n--env PGDATA=/data \\\n--env TZ=Europe/Budapest \\\n-v /opt/docker/matrix/postgres:/data \\\npostgres:14.0-alpine\n</code></pre> <p>Important</p> <p>Use the same environment values as in the compose file!</p> <p>You may want to check the logs:</p> <pre><code>docker logs postgres-init -f\n</code></pre> <p>You should see the following lines: <pre><code>PostgreSQL init process complete; ready for start up.\n\n2021-10-22 13:20:19.900 CEST [1] LOG:  starting PostgreSQL 14.0 on x86_64-pc-linux-musl, compiled by gcc (Alpine 10.3.1_git20210424) 10.3.1 20210424, 64-bit\n2021-10-22 13:20:19.900 CEST [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2021-10-22 13:20:19.900 CEST [1] LOG:  listening on IPv6 address \"::\", port 5432\n2021-10-22 13:20:19.974 CEST [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2021-10-22 13:20:20.042 CEST [51] LOG:  database system was shut down at 2021-10-22 13:20:19 CEST\n2021-10-22 13:20:20.077 CEST [1] LOG:  database system is ready to accept connections\n</code></pre></p> <ul> <li>Get into the container and create the user and database</li> </ul> <pre><code>docker exec -it postgres-init /bin/bash\ncreateuser --pwprompt synapse_user\nEnter password for new role:\nEnter it again:\ncreatedb --encoding=UTF8 --locale=C --template=template0 --owner=synapse_user synapse\nexit\n</code></pre> <p>Info</p> <p>If you use another user than <code>root</code> (<code>POSTGRES_USER=root</code>) add <code>-U [USERNAME]</code> paramter at the and of the commands. <code>createuser --pwprompt synapse_user -U [USERNAME]</code></p> <p>Warning</p> <p>Note your provided password, you will need it when configuring Matrix!</p> <ul> <li>Remove The Container</li> </ul> <pre><code>docker stop postgres-init\ndocker rm postgres-init\n</code></pre> <p>Info</p> <p>If something went wrong you can simply stop and remove the container and delete the content of the database directory. </p> <p><code>rm -rf /opt/docker/matrix/postgres/*</code></p> <p>And you can start over the init process of the database.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#edit-homeserveryaml","title":"Edit <code>homeserver.yaml</code>","text":"<pre><code>--- homeserver.yaml-org 2021-10-22 13:43:26.753645597 +0200\n+++ homeserver.yaml     2021-10-22 13:45:27.948188284 +0200\n@@ -742,25 +742,25 @@\n #\n # Example Postgres configuration:\n #\n-#database:\n-#  name: psycopg2\n-#  txn_limit: 10000\n-#  args:\n-#    user: synapse_user\n-#    password: synapse\n-#    database: synapse\n-#    host: localhost\n-#    port: 5432\n-#    cp_min: 5\n-#    cp_max: 10\n+database:\n+  name: psycopg2\n+  txn_limit: 10000\n+  args:\n+    user: synapse_user\n+    password: 12345678\n+    database: synapse\n+    host: matrix-postgres\n+    port: 5432\n+    cp_min: 5\n+    cp_max: 10\n #\n # For more information on using Synapse with Postgres,\n # see https://matrix-org.github.io/synapse/latest/postgres.html.\n #\n-database:\n-  name: sqlite3\n-  args:\n-    database: /data/homeserver.db\n+#database:\n+#  name: sqlite3\n+#  args:\n+#    database: /data/homeserver.db\n\n\n ## Logging ##\n</code></pre> <p>Caution</p> <p>Don't forget to remove sqlite3 related lines as the example shows.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#inititalize-the-caddyfile","title":"Inititalize The Caddyfile","text":"<pre><code>cd /opt/docker/matrix/caddy\ndocker  run -it  --entrypoint=/bin/sh  caddy:latest -c \"cat /etc/caddy/Caddyfile\"  &gt;Caddyfile\n</code></pre> <p>This will create a minimal <code>Caddyfile</code> example. Actually this command does nothing than copy the <code>Caddyfile</code> from the container to the directory where you are.</p> <p>Edit this file</p> <p>You Caddyfile should look like this:</p> <pre><code>matrix.vincze.work {\n  # Set this path to your site's directory.\n  root * /usr/share/caddy\n\n  # Enable the static file server.\n  file_server\n\n  # Another common task is to set up a reverse proxy:\n  reverse_proxy synapse-matrix:8008\n\n  # Or serve a PHP site through php-fpm:\n  # php_fastcgi localhost:9000\n}\n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#start-everything","title":"Start Everything","text":"<p>We are ready to start the Matrix HomeServer. Save the <code>docker-compose.yaml</code> file if you haven't already do that, and run:</p> <pre><code>docker-compose up --detach\n</code></pre> <p>And wait for <code>up</code> condition:</p> <p>Command<pre><code>docker-compose ps\n</code></pre> Output<pre><code>      Name                    Command                  State                                            Ports                                      \n---------------------------------------------------------------------------------------------------------------------------------------------------\nmatrix-postgres    docker-entrypoint.sh postgres    Up             5432/tcp                                                                        \nmatrix-web-caddy   caddy run --config /etc/ca ...   Up             2019/tcp, 0.0.0.0:443-&gt;443/tcp,:::443-&gt;443/tcp, 0.0.0.0:80-&gt;80/tcp,:::80-&gt;80/tcp\nsynapse-matrix     /start.py                        Up (healthy)   0.0.0.0:8008-&gt;8008/tcp,:::8008-&gt;8008/tcp, 8009/tcp, 8448/tcp\n</code></pre></p> <p>Check your matrix server:</p> <p>Command<pre><code>curl https://matrix.vincze.work/health\n</code></pre> Output<pre><code>OK\n</code></pre></p> <p>Browser Screenshot:</p> <p></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#federation","title":"Federation","text":"<p>What does federated mean?</p> <p>Quote</p> <p>Federation allows separate deployments of a communication service to communicate with each other - for instance a mail server run by Google federates with a mail server run by Microsoft when you send email from @gmail.com to @hotmail.com.</p> <p>interoperable clients may simply be running on the same deployment - whereas in federation the deployments themselves are exchanging data in a compatible manner.</p> <p>Matrix provides open federation - meaning that anyone on the internet can join into the Matrix ecosystem by deploying their own server.</p> <p>In order to the <code>federation</code> work you need to modify the <code>Caddyfile</code> and <code>docker-compose.yaml</code>.</p> <p><code>docker-compose.yaml</code></p> <pre><code>--- docker-compose.yaml 2021-10-23 16:31:16.567890416 +0200\n+++ docker-compose.yaml-orig  2021-10-23 17:04:08.640359385 +0200\n@@ -31,6 +31,7 @@\n     ports:\n       - 80:80\n       - 443:443\n+      - 8448:8448\n     networks:\n       - matrix\n   postgres:\n</code></pre> <p><code>Caddyfile</code></p> <pre><code>@@ -8,7 +8,7 @@\n # this machine's public IP, then replace \":80\" below with your\n # domain name.\n\n-matrix.vincze.work {\n+matrix.vincze.work:443 matrix.vincze.work:8448 {\n  # Set this path to your site's directory.\n  root * /usr/share/caddy\n\n@@ -24,4 +24,3 @@\n\n # Refer to the Caddy docs for more information:\n # https://caddyserver.com/docs/caddyfile\n</code></pre> <p>You can check if fedearation work or not: https://federationtester.matrix.org</p> <p>ScreenShot:</p> <p></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#login","title":"Login","text":"<p>We don't have any user, yet. We have three option for registering new users:</p> <ol> <li>Enable registration in the homeserver.yaml (<code>enable_registration: true</code>)</li> <li>Use the <code>registration_shared_secret</code>. <ul> <li>https://matrix-org.github.io/synapse/latest/admin_api/register_api.html#shared-secret-registration</li> </ul> </li> <li>Or use command line interface inside the container.</li> </ol> <p>I will show the third option:</p> <ul> <li>Get Into the container</li> </ul> <pre><code>docker exec -it synapse-matrix /bin/bash\n</code></pre> <ul> <li>Register new user</li> </ul> <pre><code>register_new_matrix_user -u jvincze -p Matrix1234 -a -c /config/homeserver.yaml http://localhost:8008\n</code></pre> <ul> <li>Open https://app.element.io/#/welcome in your browser</li> <li>Click on \"Sign In\"</li> <li>Change the \"Homeserver\" URL</li> </ul> <p></p> <ul> <li>Enter your credentials </li> </ul> <p>And we are done. We have a fully functional Matrix Homeserver. Of course there are a lot of configurations available in the <code>homesever.yaml</code>, and I recommend going through this file at least once to get to know the possibilities.</p> <p>We are going to deploy this minimal installation of Matrix to Kubernetes cluster in the next section.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#deploy-to-kubernetes","title":"Deploy To Kubernetes","text":""},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#create-the-namespace","title":"Create the namespace","text":"<pre><code>kubectl create ns matrix\n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#prepare-matrix-configmap-storage","title":"Prepare Matrix Configmap &amp; Storage","text":"<ul> <li>Generate the config files</li> </ul> <pre><code>mkdir -p /tmp/matrix/config /tmp/matrix/data\ndocker run -it --rm \\\n    --mount type=bind,src=/tmp/matrix/config,dst=/config \\\n    --mount type=bind,src=/tmp/matrix/data,dst=/data \\\n    -e SYNAPSE_SERVER_NAME=matrix-kub-test.duckdns.org \\\n    -e SYNAPSE_REPORT_STATS=yes \\\n    -e SYNAPSE_CONFIG_DIR=/config \\\n    -e SYNAPSE_CONFIG_PATH=/config/homeserver.yaml \\\n    matrixdotorg/synapse:latest generate\n</code></pre> <ul> <li>Create the Configmap &amp; Secret</li> </ul> <pre><code>cd /tmp/matrix\nkubectl -n matrix create cm matrix \\\n--from-file=homeserver.yaml=./config/homeserver.yaml \\\n--from-file=matrix-kub-test.duckdns.org.log.config=./config/matrix-kub-test.duckdns.org.log.config\n\nkubectl -n matrix create secret generic matrix-key \\\n--from-file config/matrix-kub-test.duckdns.org.signing.key\n</code></pre> <ul> <li>Create Persistent Volume</li> </ul> Click Here For Raw Source<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: matrix-storage\n  namespace: matrix\nspec:\n  storageClassName: openebs-hostpath\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n</code></pre> <p>Download &amp; Apply</p> <pre><code>curl  -L -o /tmp/PersistentVolumeClaim-matrix.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/PersistentVolumeClaim-matrix.yaml\nkubectl apply -f /tmp/matrix-pvc.yaml\n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#deploy-matrix-homeserver","title":"Deploy Matrix Homeserver","text":"<p>First we deploy the Matrix homeserver without any configuration changes. Later we can update the <code>homeserver.yaml</code> in the Configmap.</p> Click Here For Raw Source<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: matrix\n  name: matrix\n  namespace: matrix\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: matrix\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        k8s-app: matrix\n      name: matrix\n    spec:\n      containers:\n      - env:\n        - name: SYNAPSE_CONFIG_DIR\n          value: /config\n        - name: SYNAPSE_CONFIG_PATH\n          value: /config/homeserver.yaml\n        image: matrixdotorg/synapse:latest\n        imagePullPolicy: Always\n        name: matrix\n        securityContext:\n          privileged: true\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /config/homeserver.yaml\n          name: matrix-cm\n          subPath: homeserver.yaml\n        - mountPath: /config/matrix-kub-test.duckdns.org.log.config\n          name: matrix-cm\n          subPath: matrix-kub-test.duckdns.org.log.config\n        - mountPath: /data\n          name: matrix-pv\n          subPath: matrix\n        - mountPath: /config/matrix-kub-test.duckdns.org.signing.key\n          name: matrix-key\n          subPath: matrix-kub-test.duckdns.org.signing.key\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      securityContext:\n        fsGroup: 991\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 10\n      volumes:\n      - name: matrix-key\n        secret:\n          defaultMode: 420\n          items:\n          - key: matrix-kub-test.duckdns.org.signing.key\n            path: matrix-kub-test.duckdns.org.signing.key\n          secretName: matrix-key\n      - name: matrix-pv\n        persistentVolumeClaim:\n          claimName: matrix-storage\n      - name: matrix-cm\n        configMap:\n          name: matrix\n          defaultMode: 420\n          items:\n          - key: homeserver.yaml\n            path: homeserver.yaml\n          - key: matrix-kub-test.duckdns.org.log.config\n            path: matrix-kub-test.duckdns.org.log.config\n</code></pre> <p>2023-04-28 - Permission denied</p> <p>You may experince permission denied error on the mounted <code>/data</code> directory. This is likely because your persistent volume provider mount the path with wrong uid/guid. Matirx uses 991:991 by default. So I added the followings to the Deployment manifest: <pre><code>      securityContext:\n        fsGroup: 991\n</code></pre></p> <p>Download &amp; Apply</p> <pre><code>curl  -L -o /tmp/Deployment-matrix.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/Deployment-matrix.yaml\nkubectl apply -f /tmp/Deployment-matrix.yaml\n</code></pre> <p>Check The Deployment</p> <p>Command<pre><code>kubectl -n matrix get deployment\n</code></pre> Output<pre><code>NAME     READY   UP-TO-DATE   AVAILABLE   AGE\nmatrix   1/1     1            1           3d1h \n</code></pre></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#create-service","title":"Create Service","text":"Click Here For Raw Source<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: postgres\n  name: matrix\n  namespace: matrix\nspec:\n  ports:\n  - name: matrix\n    port: 8008\n    protocol: TCP\n    targetPort: 8008\n  selector:\n    k8s-app: matrix\n  sessionAffinity: None\n  type: ClusterIP\n</code></pre> <p>Download &amp; Apply</p> <pre><code>curl  -L -o /tmp/Service-matrix.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/Service-matrix.yaml\nkubectl apply -f /tmp/Service-matrix.yaml\n</code></pre> <p>Check The Service</p> <p>Command<pre><code>kubectl -n matrix describe svc matrix\n</code></pre> Output<pre><code>Name:              matrix\nNamespace:         matrix\nLabels:            k8s-app=postgres\nAnnotations:       &lt;none&gt;\nSelector:          k8s-app=matrix\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.22.84.86\nIPs:               10.22.84.86\nPort:              matrix  8008/TCP\nTargetPort:        8008/TCP\nEndpoints:         10.32.0.3:8008\nSession Affinity:  None\nEvents:            &lt;none&gt; \n</code></pre></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#deploy-postgres-sql","title":"Deploy Postgres SQL","text":""},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#create-the-persistentvolumeclaim","title":"Create The <code>PersistentVolumeClaim</code>","text":"Click Here For Raw Source<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-storage\n  namespace: matrix\nspec:\n  storageClassName: openebs-hostpath\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 50Gi\n</code></pre> <p>Download &amp; Apply</p> <pre><code>curl -L -o /tmp/PersistentVolumeClaim-postgres.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/PersistentVolumeClaim-postgres.yaml\nkubectl apply -f /tmp/PersistentVolumeClaim-postgres.yaml\n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#create-secret","title":"Create Secret","text":"<pre><code>kubectl -n matrix create secret generic postgres-password --from-literal=pgpass=12345678\n</code></pre> <p>This password will be used in the <code>Deployment</code> as the password of the initial user (<code>POSTGRES_USER</code> = <code>matrix</code>).</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#deployment","title":"Deployment","text":"Click Here For Raw Source<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: postgres\n  name: postgres\n  namespace: matrix\nspec:\n  progressDeadlineSeconds: 600\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: postgres\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        k8s-app: postgres\n      name: postgres\n    spec:\n      containers:\n      - env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: pgpass\n              name: postgres-password\n        - name: POSTGRES_USER\n          value: matrix\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        image: postgres:13\n        imagePullPolicy: Always\n        name: postgres\n        securityContext:\n          privileged: false\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /var/lib/postgresql/data\n          name: postgresdata\n      dnsPolicy: ClusterFirst\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: postgresdata\n        persistentVolumeClaim:\n          claimName: postgres-storage\n</code></pre> <p>Download &amp; Apply</p> <pre><code>curl -L -o /tmp/PersistentVolumeClaim-postgres.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/Deployment-postgres.yaml\nkubectl apply -f /tmp/Deployment-postgres.yaml\n</code></pre> <p>Check The Pod &amp; Logs</p> <p>Command<pre><code>kubectl -n matrix get deployment\n</code></pre> Output<pre><code>NAME       READY   UP-TO-DATE   AVAILABLE   AGE\nmatrix     1/1     1            1           3d2h\npostgres   1/1     1            1           6m15s\n</code></pre> Command<pre><code>kubectl -n matrix get pods -o wide\n</code></pre> Output<pre><code>NAME                        READY   STATUS    RESTARTS   AGE     IP          NODE                           NOMINATED NODE   READINESS GATES\nmatrix-7658b9d5db-49kcc     1/1     Running   5          5d19h   10.32.0.3   kube-test.int.vinyosoft.info   &lt;none&gt;           &lt;none&gt;\npostgres-7698969f95-8c4jn   1/1     Running   1          2d18h   10.32.0.8   kube-test.int.vinyosoft.info   &lt;none&gt;           &lt;none&gt;\n</code></pre> Command<pre><code>kubectl -n matrix logs $(kubectl -n matrix get pods -o name | grep postgres ) | tail -n 3\n</code></pre> Output<pre><code>2021-10-26 18:41:34.085 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2021-10-26 18:41:34.170 UTC [64] LOG:  database system was shut down at 2021-10-26 18:41:33 UTC\n2021-10-26 18:41:34.216 UTC [1] LOG:  database system is ready to accept connections\n</code></pre></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#service","title":"Service","text":"Click Here For Raw Source<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: postgres\n  name: postgres\n  namespace: matrix\nspec:\n  ports:\n  - name: postgres\n    port: 5432\n    protocol: TCP\n    targetPort: 5432\n  selector:\n    k8s-app: postgres\n  sessionAffinity: None\n  type: ClusterIP\n</code></pre> <p>Download &amp; Apply</p> <pre><code>curl -L -o /tmp/Service-postgres.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/Service-postgres.yaml\nkubectl apply -f /tmp/Service-postgres.yaml\n</code></pre> <p>Check The Service</p> <p>Command<pre><code>kubectl -n matrix describe services postgres\n</code></pre> Output<pre><code>Name:              postgres\nNamespace:         matrix\nLabels:            k8s-app=postgres\nAnnotations:       &lt;none&gt;\nSelector:          k8s-app=postgres\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.22.24.132\nIPs:               10.22.24.132\nPort:              postgres  5432/TCP\nTargetPort:        5432/TCP\nEndpoints:         10.32.0.8:5432\nSession Affinity:  None\nEvents:            &lt;none&gt; \n</code></pre></p> <p>Check if the IP address and port of <code>Endpoints</code> are matching the Postgres POD IP address and port.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#connect-matix-homeserver-to-postgres","title":"Connect Matix Homeserver To Postgres","text":""},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#prepare-the-databse","title":"Prepare The Databse","text":"<p>First we need to create a user and database for the homeserver, just like we did before in the compose section.</p> <pre><code>kubectl -n matrix exec -it postgres-7698969f95-8c4jn -- /bin/bash\n\n# Inside the container:\ncreateuser --pwprompt synapse_user -U matrix\nEnter password for new role: 12345678\nEnter it again: 12345678\ncreatedb --encoding=UTF8 --locale=C --template=template0 --owner=synapse_user synapse -U matrix  \n</code></pre>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#modify-the-homeserver-configmap","title":"Modify The Homeserver Configmap","text":"<pre><code>kubectl -n matrix edit cm matrix\n</code></pre> <ul> <li>Lines To Remove</li> </ul> <pre><code>    database:\n      name: sqlite3\n      args:\n        database: /data/homeserver.db\n</code></pre> <ul> <li>Lines To add:</li> </ul> <pre><code>    database:\n      name: psycopg2\n      txn_limit: 10000\n      args:\n        user: synapse_user\n        password: 12345678\n        database: synapse\n        host: postgres.matrix.svc.cluster.local\n        port: 5432\n        cp_min: 5\n        cp_max: 10\n</code></pre> <p>Basically we are done with set up the Homeserver. There is only one thing to do, somehow publish the homeserver to the Internet.</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#ingress","title":"Ingress","text":"<p>There are several options for accessing the Matrix Homeserver from the Internet. I don't know your architecture, but maybe you already have an Ingress Controller and a reverse proxy, etc... I'll show some solutions:</p> <ul> <li>My setup at home looks something like this: <code>*.mydomain.hu --&gt; Apache WebServer (reverse proxy) --&gt; Kubernetes Ingress</code>. <ul> <li>Wildcard certificate installed on the Apache WebSever.</li> <li>In this setup I need to create only an <code>Ingress</code> in Kubernets and the Homserver immediately becomes accessible. </li> <li>I recommend using a separate reverse proxy for the incoming connection to Kubernetes. This reverse proxy could be Apache, Nginx, Caddy, etc. </li> </ul> </li> <li>If you don't have separate reverse proxy:<ul> <li>You can set up Homeserver to listen on a https port: <code>matrix.mydomain.hu --&gt; Ingress SSL Pass-through --&gt; Homeserver (https).</code> </li> <li>Another way is to setup your <code>Ingress</code> to listen on https: <code>matrix.mydomain.hu --&gt; Ingress (SSL) --&gt; Homeserver (plain http)</code> If you chose this option take a look at this page: https://kubernetes.github.io/ingress-nginx/user-guide/tls/#automated-certificate-management-with-cert-manager </li> <li>You can setup <code>Caddy</code> on the Kubernetes: <code>homeserver.mydomain.hu --&gt; Ingress To Caddy (http) --&gt; Caddy --&gt; Homserver (service)</code></li> </ul> </li> </ul> <p>I can't write example for the all available scenario, but I want to post here a working, overall solution, thus I show two examples:</p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#simple-ingress","title":"Simple Ingress","text":"<p>If you have already a working architecture you may need only an Ingress like this:</p> Click Here For Raw Source<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/proxy-body-size: 110m\n  name: matrix\n  namespace: matrix\nspec:\n  rules:\n  - host: matrix.k8s-test.loc\n    http:\n      paths:\n      - backend:\n          service:\n            name: matrix\n            port:\n              number: 8008\n        pathType: ImplementationSpecific\n</code></pre> <p>Download &amp; Apply</p> <pre><code>curl -L -o /tmp/Ingress-matrix.yaml \\\nhttps://raw.githubusercontent.com/jvincze84/jvincze84.github.io/master/docs/files/matrix/Ingress-matrix.yaml\nkubectl apply -f /tmp/Ingress-matrix.yaml\n</code></pre> <p>Check Ingress</p> <p>Command<pre><code>kubectl -n matrix describe ingress matrix\n</code></pre> Output<pre><code>Name:             matrix\nNamespace:        matrix\nAddress:          172.16.1.214\nDefault backend:  default-http-backend:80 (&lt;error: endpoints \"default-http-backend\" not found&gt;)\nRules:\n  Host                 Path  Backends\n  ----                 ----  --------\n  matrix.k8s-test.loc  \n                          matrix:8008 (10.32.0.3:8008)\nAnnotations:           nginx.ingress.kubernetes.io/proxy-body-size: 110m\nEvents:\n  Type    Reason  Age                From                      Message\n  ----    ------  ----               ----                      -------\n  Normal  Sync    24s (x3 over 17m)  nginx-ingress-controller  Scheduled for sync \n</code></pre></p> <p>How Ingress - Service And Deployment Are Related?</p> <p>If you want to know more about how these three things are related read the following article: https://dwdraju.medium.com/how-deployment-service-ingress-are-related-in-their-manifest-a2e553cf0ffb</p> <p></p> <p>In our case, this the flow:</p> <p></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#cert-manager","title":"Cert Manager","text":"<p>First I wanted to write about <code>Caddy</code> here, but changed my mind. I think a much more suitable solution is using the Cert Manager. My first thought was to deploy <code>Caddy</code>, setup as reverse proxy for the Matrix <code>Service</code>, and create an <code>Ingress</code> for <code>Caddy</code>. This would have been similar to what we did in the compose file before. I don't think anybody wants to use Ingress and Caddy in this way. </p> <p>Documentations:</p> <ul> <li>https://cert-manager.io</li> <li>https://kubernetes.github.io/ingress-nginx/user-guide/tls/#automated-certificate-management-with-cert-manager</li> </ul> <p>I'm using Nginx Ingrees Controller for this demo. I won't write here a step-by-step installation about the Ingress controller. If you need detailed documentation please consult the official site: https://kubernetes.github.io/ingress-nginx/deploy/, or see my single node Kubernetes installation article.</p> <p>Install Cert Manager</p> <p>Installing cert-manager is really simple, only one command:</p> <pre><code>kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.0/cert-manager.yaml\n</code></pre> <p>For more details visit the official documentation here</p> <p>ClusterIssuer</p> <p>We are going to create two <code>ClusterIssuer</code>: </p> <ol> <li>Letsencrypt Staging</li> </ol> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    email: jvincze84@gmail.com\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <ol> <li>Letsencrypt Production</li> </ol> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    email: jvincze84@gmail.com\n    server: https://acme-v02.api.letsencrypt.org/directory\n    preferredChain: \"ISRG Root X1\"\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n</code></pre> <p>Check:</p> <p>Command<pre><code>kubectl get ClusterIssuer -o wide\n</code></pre> Output<pre><code>NAME                  READY   STATUS                                                 AGE\nletsencrypt-prod      True    The ACME account was registered with the ACME server   49s\nletsencrypt-staging   True    The ACME account was registered with the ACME server   5h31m \n</code></pre></p> <p>Create <code>Ingress</code></p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    nginx.ingress.kubernetes.io/proxy-body-size: 110m\n  name: matrix\n  namespace: matrix\nspec:\n  rules:\n  - host: homeserver.matrix-kub-test.duckdns.org\n    http:\n      paths:\n      - backend:\n          service:\n            name: matrix\n            port:\n              number: 8008\n        pathType: ImplementationSpecific\n  tls:\n  - hosts:\n    - homeserver.matrix-kub-test.duckdns.org\n    secretName: matrix-prod-ingress\n</code></pre> <p>That's all! :) If your Kubernetes Ingress accessible from the Internet on port 80 and 443 the certificate issued in some seconds. </p> <p>Checks</p> <p>If something is not working as expected, there are some <code>resources</code> you should check.</p> <p>Command<pre><code>api-resources | grep cert-manager.io\n</code></pre> Output<pre><code>challenges                                     acme.cert-manager.io/v1                true         Challenge\norders                                         acme.cert-manager.io/v1                true         Order\ncertificaterequests               cr,crs       cert-manager.io/v1                     true         CertificateRequest\ncertificates                      cert,certs   cert-manager.io/v1                     true         Certificate\nclusterissuers                                 cert-manager.io/v1                     false        ClusterIssuer\nissuers                                        cert-manager.io/v1                     true         Issuer \n</code></pre></p> <p>First check <code>challenges</code>:</p> <p>Command<pre><code>kubectl -n matrix get challenges\n</code></pre> Output<pre><code>NAME                                              STATE     DOMAIN                                   AGE\nmatrix-prod-ingress-gptxf-1059591821-1281697531   pending   homeserver.matrix-kub-test.duckdns.org   2m42s \n</code></pre></p> <p>You can see that the <code>challenge</code> is in <code>pending</code> state. You can check what could be the problem with the following command:</p> <pre><code>kubectl -n matrix get challenges matrix-prod-ingress-gptxf-1059591821-1281697531 -o yaml\n</code></pre> <p>Example error message:</p> <pre><code>status:\n  presented: true\n  processing: true\n  reason: 'Waiting for HTTP-01 challenge propagation: failed to perform self check\n    GET request ''http://homeserver.matrix-kub-test.duckdns.org/.well-known/acme-challenge/lgUy6mCEhhbQ9n_v3w_C47cbVgc5Bfvoy6JW3oxrH1o'':\n    Get \"http://homeserver.matrix-kub-test.duckdns.org/.well-known/acme-challenge/lgUy6mCEhhbQ9n_v3w_C47cbVgc5Bfvoy6JW3oxrH1o\":\n    dial tcp 87.97.31.112:80: i/o timeout (Client.Timeout exceeded while awaiting\n    headers)'\n  state: pending\n</code></pre> <p>Info</p> <p>Just for reference, in my case the problem was that hairpin NAT wasn't set up properly in my router. </p> <p>Check <code>certificaterequests</code> and <code>certificates</code></p> <p>Command<pre><code>kubectl -n matrix get crs,certs\n</code></pre> Command<pre><code>NAME                                                           APPROVED   DENIED   READY   ISSUER             REQUESTOR                                         AGE\ncertificaterequest.cert-manager.io/matrix-prod-ingress-gptxf   True                True    letsencrypt-prod   system:serviceaccount:cert-manager:cert-manager   19m\n\nNAME                                              READY   SECRET                AGE\ncertificate.cert-manager.io/matrix-prod-ingress   True    matrix-prod-ingress   19m \n</code></pre></p> <p>Your certificate is stored in this <code>Secret</code>: <code>secretName: matrix-prod-ingress</code></p>"},{"location":"Blog/2021/10/27/Install_Matrix_Home_Server/#final-thoughts","title":"Final Thoughts","text":"<p>I hope this article contains a lot of useful examples, use cases and help you to build your own Matrix Homeserver. Of course there are any other options to deploy the Homeserver to Kubernetes, the way I showed here contains many useful examples on how to use docker-compose, service, ingress, cert-manager, configmap, secret, etc...</p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/","title":"Centralised Backup With Borg","text":""},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#tldr","title":"TL;DR","text":"<p>Why I Abandoned Duplicati?</p> <p>I use to use Duplicati as backup solution for ages. It's a really good backup solution with a handy user interface. Everything can be done on it's interface. </p> <p>My requirements toward a backup software:</p> <ul> <li>Differential backups on daily basis</li> <li>Support linux operation system</li> <li>Google Drive support for storing backup remotly</li> <li>Encryption, of course</li> <li>Be as lightweight as possible</li> </ul> <p>I know Google Drive support a bit unusall, but I have 2TB storage and I don't want to pay for an other service. Duplicati fulfills all my requirments. There are only two weakness because of why I looked for another solution: resource consumption and speed:</p> <ul> <li>Duplicati can be really slow on restore from Google Drive if you store a lot of files.</li> <li>Duplicati uses [Mono](https://www.mono-project.com. (Cross platform, open source .NET framework). Running .NET on linux is not my taste, and sometimes consumes too much resource, especially on a Raspberry PI3.</li> </ul> <p>After some hours of Googling and trying some softwares I found Borg Backup.</p> <p>The only missing feature is the Google Drive, but it can be achieved with <code>rclone</code>.</p> <p>I don't want to write pages about my choice, features, advantages, disadvantages, etc. If you are reading this article you probably want to try or use Borg.</p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#installing-borg","title":"Installing Borg","text":"<p>I will create three Virtual Machine for demonstration the installation and usage. </p> <p>Since the borg install procedure is always the same, I've done once and cloned the VM.</p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#install-on-debian-11","title":"Install On Debian 11","text":"<p>OS version: <pre><code>root@borg01:~# cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 11 (bullseye)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"11\"\nVERSION=\"11 (bullseye)\"\nVERSION_CODENAME=bullseye\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n</code></pre></p> <p>Update apt: <pre><code>apt-get update --allow-releaseinfo-change\napt-get upgrade\n</code></pre></p> <p>Install Borg Backup <pre><code>apt-get install borgbackup\n</code></pre></p> <p>Check installed version: <pre><code>root@borg01:~# borg --version\nborg 1.1.16\n</code></pre></p> <p>So now I have a Proxmox template with ID 102. I'm goning to create 3 clone:</p> <pre><code>qm clone 102 501 -full false  -name borg-01\nqm clone 102 502 -full false  -name borg-02\nqm clone 102 503 -full false  -name borg-03\n\n# Start \nqm start 501\nqm start 502\nqm start 503\n</code></pre> <p>IP Addresses:</p> <ul> <li>borg-01: <code>172.16.1.236/22</code></li> <li>borg-02: <code>172.16.1.218/22</code></li> <li>borg-03: <code>172.16.1.219/22</code></li> </ul>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#configure-ssh-and-users","title":"Configure SSH And Users","text":"<p>The borg-01 will be the server, it will store the backups.</p> <ul> <li> <p>Create User Command<pre><code>useradd --shell /bin/bash --create-home  borg\n</code></pre></p> </li> <li> <p>Create SSH keys <pre><code>su borg -s /bin/bash\nssh-keygen\ncat ~/.ssh/id_rsa.pub &gt;~/.ssh/authorized_keys\n</code></pre></p> </li> <li> <p>Distribute the private key accross the other servers. <pre><code>mkdir .ssh\ncat &lt;&lt;EOF&gt;~/.ssh/id_rsa\n-----BEGIN OPENSSH PRIVATE KEY-----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\nEmVgLE4QudDV4fAAAADGJvcmdAYm9yZy0wMQECAwQFBg==\n-----END OPENSSH PRIVATE KEY-----\nEOF\n\nchmod 700 ~/.ssh/\nchmod 600 ~/.ssh/id_rsa\n</code></pre></p> </li> </ul> <p>Test: Command<pre><code>ssh borg@172.16.1.215\n</code></pre></p> Output<pre><code>Linux borg-01 5.13.19-1-pve #1 SMP PVE 5.13.19-2 (Tue, 09 Nov 2021 12:59:38 +0100) x86_64\n\nThe programs included with the Debian GNU/Linux system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\npermitted by applicable law.\nLast login: Sat Dec 11 15:51:46 2021 from 172.16.1.216\nborg@borg-01:~$\n</code></pre> <p>Ok. It works well.</p> <p>Warning</p> <p>Please keep your private key safe. Since this is a demo environment I don't care much the security. I storngly recommend to use individual keys for each host. This post is not about ssh key management, but Borg backup.</p> <p>Info</p> <p>I'm using the <code>root</code> user on the clients, bacuse only the <code>root</code> account has access to the directories I want to backup.</p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#create-mange-backups","title":"Create &amp; Mange Backups","text":""},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#initialize-passphare","title":"Initialize &amp; Passphare","text":"<ul> <li> <p>Create very strong passphare <pre><code>head -c 32 /dev/urandom | base64 -w 0 &gt; ~/.borg-passphrase\nchmod 400 ~/.borg-passphrase\nexport BORG_PASSCOMMAND=\"cat $HOME/.borg-passphrase\"\n</code></pre></p> </li> <li> <p>Initialize Command<pre><code>borg init -e repokey-blake2 borg@172.16.1.236:/home/borg/borg-02\n</code></pre> Output<pre><code>By default repositories initialized with this version will produce security\nerrors if written to with an older version (up to and including Borg 1.0.8).\n\nIf you want to use these older versions, you can disable the check by running:\nborg upgrade --disable-tam ssh://borg@172.16.1.236/home/borg/borg-02\n\nSee https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability for details about the security implications.\n\nIMPORTANT: you will need both KEY AND PASSPHRASE to access this repo!\nUse \"borg key export\" to export the key, optionally in printable format.\nWrite down the passphrase. Store both at safe place(s). \n</code></pre></p> </li> </ul> <p>Caution</p> <p>If you lose the passphrase (<code>~/.borg-passphrase</code>) you lose all of the backups, as well. So I do really recommend to keep it in a safe place, not just in the home directory!</p> <p>Repeat this step on all nodes you want to create backup.</p> <p>Important</p> <p>There are two really important things to be kept in safe, the passphrase  and the key. I strongly recommend to save them to you password manager or keep them somewhere in really safe. </p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#manage-your-key-and-passphrase","title":"Manage Your Key And Passphrase","text":"<p>I assume you are using password manager in any way, and hope this manager is not a plain text file. :) </p> <p>Save your passphrase to your password manager is simple. Just save the conent of this file: <code>~/.borg-passphrase</code> <pre><code>cat ~/.borg-passphrase  ; echo\n</code></pre> The key is bit more complicated. Fisrt see how to export:</p> <p>Command<pre><code>borg key export borg@172.16.1.236:/home/borg/borg-02  /tmp/key\ncat /tmp/key\n</code></pre> Output<pre><code>BORG_KEY f968c50460d1ca7aaec9a8e2347a61fd286b26fb84adcaa6de7808966026b51e\nhqlhbGdvcml0aG2mc2hhMjU2pGRhdGHaAZ7vhs+Yzwxg9VgXxo95S5+ScE8RT3yY6elK5J\nKJKhfwz/YYJrGO6ZlDSpr9i+fnUI7qz6BfIxBLA6yILdcFVpOUuy99cDp79Uyc7wrIDnTV\nsk0oiQWBt3710yM3hJQC84Q69grriPrF0jdzgSCvDKn+FNfQQgLTgnYMavxxnXZESTStng\nzfxMtcJZMghEm1Mfd8ZwRTDXPgpF5z03bXy+7DrQ/btxgiW8G+h6DEccBDKvf0oAfDOPvH\nsGgC2aBq+lqpUcxdIhpd+CZ0BzFkWCMrQkr3QOhlMbGtkqi7a78/rIYeJWevyWwODM7RvZ\ni01qqbrDoflkRAg/LiY76p0wi46ls8Annygw9RY7YzOq7+xEvImGRYXX5joJ9Lb1GQ3Eh1\n7MSFFdVRfAXbcAUlyQXZ+k/TzxZIFw7ZsXvQL33AFD1MwuXVJdXCJZFtWNUD97Cd5cTwEq\nf6T5AofjK6WAIF5qD4RGVEoH0X8+7MJ6IHCu8aPbjnqVLvjR9Ubii7mS5gC9IRdaN5T61i\nPjNC3Lm8TjqL8WlSjqfqvu2BczikaGFzaNoAIL0XSOEvCdQ46MtJO5/Q98J1mEDsC9tLVv\nOBZZy+emXAqml0ZXJhdGlvbnPOAAGGoKRzYWx02gAg/tXk5wRp5YZlOHdzm+Gk+8f5Qi/f\ns2VHKZJPL8BfecWndmVyc2lvbgE=\n</code></pre></p> <p>If you can save this file as attachment  you are done. But not all password manager supports attachments, and the line brakes can be broken. In this case I recommend to use <code>base64</code>:</p> <p>Command<pre><code>cat /tmp/key | base64 -w0 ; echo\n</code></pre> Output<pre><code>Qk9SR19LRVkgZjk2OGM1MDQ2MGQxY2E3YWFlYzlhOGUyMzQ3YTYxZmQyODZiMjZmYjg0YWRjYWE2ZGU3ODA4OTY2MDI2YjUxZQpocWxoYkdkdmNtbDBhRzJtYzJoaE1qVTJwR1JoZEdIYUFaN3ZocytZend4ZzlWZ1h4bzk1UzUrU2NFOFJUM3lZNmVsSzVKCktKS2hmd3ovWVlKckdPNlpsRFNwcjlpK2ZuVUk3cXo2QmZJeEJMQTZ5SUxkY0ZWcE9VdXk5OWNEcDc5VXljN3dySURuVFYKc2swb2lRV0J0MzcxMHlNM2hKUUM4NFE2OWdycmlQckYwamR6Z1NDdkRLbitGTmZRUWdMVGduWU1hdnh4blhaRVNUU3RuZwp6ZnhNdGNKWk1naEVtMU1mZDhad1JURFhQZ3BGNXowM2JYeSs3RHJRL2J0eGdpVzhHK2g2REVjY0JES3ZmMG9BZkRPUHZICnNHZ0MyYUJxK2xxcFVjeGRJaHBkK0NaMEJ6RmtXQ01yUWtyM1FPaGxNYkd0a3FpN2E3OC9ySVllSldldnlXd09ETTdSdloKaTAxcXFickRvZmxrUkFnL0xpWTc2cDB3aTQ2bHM4QW5ueWd3OVJZN1l6T3E3K3hFdkltR1JZWFg1am9KOUxiMUdRM0VoMQo3TVNGRmRWUmZBWGJjQVVseVFYWitrL1R6eFpJRnc3WnNYdlFMMzNBRkQxTXd1WFZKZFhDSlpGdFdOVUQ5N0NkNWNUd0VxCmY2VDVBb2ZqSzZXQUlGNXFENFJHVkVvSDBYOCs3TUo2SUhDdThhUGJqbnFWTHZqUjlVYmlpN21TNWdDOUlSZGFONVQ2MWkKUGpOQzNMbThUanFMOFdsU2pxZnF2dTJCY3ppa2FHRnphTm9BSUwwWFNPRXZDZFE0Nk10Sk81L1E5OEoxbUVEc0M5dExWdgpPQlpaeStlbVhBcW1sMFpYSmhkR2x2Ym5QT0FBR0dvS1J6WVd4MDJnQWcvdFhrNXdScDVZWmxPSGR6bStHays4ZjVRaS9mCnMyVkhLWkpQTDhCZmVjV25kbVZ5YzJsdmJnRT0K\n</code></pre></p> <p>Restore command: <code>echo -n [BASE64_STRING] | base64 -d</code></p> <p>Tip</p> <p>If you want to make the <code>base64</code> string smaller you can use <code>gzip</code>. Encode: <code>cat /tmp/key | gzip -c | base64</code> Decode: <code>echo -n [BASE64_STRING] | base64 -d | gunzip -c</code></p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#create-backups","title":"Create Backups","text":"<p>Important</p> <p>Don't forget to export <code>BORG_PASSCOMMAND</code>  before you use <code>borg</code> command! <code>export BORG_PASSCOMMAND=\"cat $HOME/.borg-passphrase\"</code></p> <p>Command<pre><code>borg create --stats borg@172.16.1.236:/home/borg/borg-02::firstBackup /root /etc /home /opt\n</code></pre> Output<pre><code>------------------------------------------------------------------------------\nArchive name: firstBackup\nArchive fingerprint: 882ed726a7115928149aa438af4b78f09d322a34c17dd65f0bf7ce537092ee1b\nTime (start): Sat, 2021-12-11 17:43:09\nTime (end):   Sat, 2021-12-11 17:43:11\nDuration: 2.40 seconds\nNumber of files: 443\nUtilization of max. archive size: 0%\n------------------------------------------------------------------------------\n                       Original size      Compressed size    Deduplicated size\nThis archive:                1.61 MB            654.41 kB            649.83 kB\nAll archives:                1.61 MB            654.41 kB            649.83 kB\n\n                       Unique chunks         Total chunks\nChunk index:                     424                  434\n------------------------------------------------------------------------------\n</code></pre></p> <p>Create another backup:</p> <p>Command<pre><code>borg create --stats borg@172.16.1.236:/home/borg/borg-02::secondBackup /root /etc /home /opt /var/log/\n</code></pre> Output<pre><code>------------------------------------------------------------------------------\nArchive name: secondBackup\nArchive fingerprint: f61a6d2ce46fc6433a6cfa9cdb1f146933f897c6bce769d071644e7117684cd9\nTime (start): Sat, 2021-12-11 17:44:53\nTime (end):   Sat, 2021-12-11 17:44:55\nDuration: 1.56 seconds\nNumber of files: 470\nUtilization of max. archive size: 0%\n------------------------------------------------------------------------------\n                       Original size      Compressed size    Deduplicated size\nThis archive:               51.16 MB              8.95 MB              8.35 MB\nAll archives:               52.77 MB              9.60 MB              9.00 MB\n\n                       Unique chunks         Total chunks\nChunk index:                     458                  898\n------------------------------------------------------------------------------\n</code></pre></p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#check-list-backups","title":"Check &amp; List Backups","text":"<ul> <li> <p>List Backups  Command<pre><code>borg list borg@172.16.1.236:/home/borg/borg-02\n</code></pre> Output<pre><code>firstBackup                          Sat, 2021-12-11 17:43:09 [882ed726a7115928149aa438af4b78f09d322a34c17dd65f0bf7ce537092ee1b]\nsecondBackup                         Sat, 2021-12-11 17:44:53 [f61a6d2ce46fc6433a6cfa9cdb1f146933f897c6bce769d071644e7117684cd9]\n</code></pre></p> </li> <li> <p>Check The Conent Of A Backup Command<pre><code>borg list borg@172.16.1.236:/home/borg/borg-02::secondBackup\n</code></pre> Output<pre><code>drwx------ root   root          0 Sat, 2021-12-11 17:36:41 root\n-rw-r--r-- root   root        161 Tue, 2019-07-09 12:05:50 root/.profile\n-rw-r--r-- root   root        571 Sat, 2021-04-10 22:00:00 root/.bashrc\n-rw------- root   root        273 Sat, 2021-12-11 17:21:55 root/.bash_history\ndrwx------ root   root          0 Sat, 2021-12-11 17:26:32 root/.ssh\n-rw------- root   root       2602 Sat, 2021-12-11 17:25:59 root/.ssh/id_rsa\n-rw-r--r-- root   root        222 Sat, 2021-12-11 17:26:32 root/.ssh/known_hosts\n...\n...\n</code></pre></p> </li> </ul>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#extact-content","title":"Extact Content","text":"<p>For example we want to restore the home direcrory from the <code>secondBackup</code>. Command<pre><code>cd /tmp\nmkdir restore\ncd restore\nborg extract borg@172.16.1.236:/home/borg/borg-02::secondBackup home\nfind\n</code></pre> Output<pre><code>.\n./home\n./home/user\n./home/user/.bash_history\n./home/user/.bash_logout\n./home/user/.bashrc\n./home/user/.profile \n</code></pre></p> <p>Borg has a really lovely feature: you can mount any of your backup.</p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#mount-a-backup","title":"Mount A Backup","text":"<p>Command: <pre><code>borg mount borg@172.16.1.236:/home/borg/borg-02::secondBackup /mnt\n</code></pre></p> <p>Check: Command<pre><code>cd /mnt/\nls -la\n</code></pre> Output<pre><code>total 4\ndrwxr-xr-x  1 root root    0 Dec 11 18:01 .\ndrwxr-xr-x 18 root root 4096 Nov 21 13:38 ..\ndrwxr-xr-x  1 root root    0 Dec 11 17:43 etc\ndrwxr-xr-x  1 root root    0 Nov 21 13:42 home\ndrwxr-xr-x  1 root root    0 Nov 21 13:35 opt\ndrwx------  1 root root    0 Dec 11 17:36 root\ndrwxr-xr-x  1 root root    0 Dec 11 18:01 var \n</code></pre></p> <p>You can browse inside the backup and restore any file you want. </p> <p>If you don't need the mount anymore, don't forget to unmount: <pre><code>borg umount /mnt\n</code></pre></p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#prune","title":"Prune","text":"<p>Assume that you create backups every day. Probably you don't need every backup forever. You can prue your repository and keep only certain amount of backup. For example I use the following parameters to prune the repository:</p> <pre><code>borg prune  --stats --keep-daily 3 --keep-weekly 2  --keep-monthly 5 ${REPO_PATH}\n</code></pre> <p>This will keep 3 daily backup, 2 weekly and 5 monthly. What does it mean? Example:</p> <pre><code>Weekly 2:  2021-11-28T02:00:01                  Sun, 2021-11-28 02:00:03 [c1c349e361dc5f..... ]\nMonthly 1: 2021-11-30T02:00:01                  Tue, 2021-11-30 02:00:03 [a83f0b4d9d686f..... ]\nWeekly 1:  2021-12-05T02:00:01                  Sun, 2021-12-05 02:00:04 [39980ab7451c33..... ]\nDaily 3:   2021-12-09T02:00:01                  Thu, 2021-12-09 02:00:02 [daf1c1ea020b16..... ]\nDaily 2:   2021-12-10T02:00:01                  Fri, 2021-12-10 02:00:03 [dd6fee702f0593..... ]\nDaily 1:   2021-12-11T02:00:01                  Sat, 2021-12-11 02:00:03 [e0385e46a1e968..... ]\n</code></pre> <p>Info</p> <p>My script creates backup on every day at 02:00am.</p> <ul> <li><code>keep-daily</code> --&gt; keeps the last backup of each day. </li> <li><code>keep-weekly</code> --&gt; keeps the last backup from the last <code>n</code> Sunday</li> <li><code>keep-monthly</code> --&gt; keeps the last backup from the last day of the month</li> </ul> <p>Read more: https://borgbackup.readthedocs.io/en/stable/usage/prune.html</p>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#backup-with-crontab","title":"Backup With Crontab","text":"<p>I have a little shell script to automatize my daily backup:</p> <pre><code>#!/bin/bash\n\nexec &gt; &gt;(logger -i --tag borgbackup) 2&gt;&amp;1\n\nexport BORG_PASSCOMMAND=\"cat $HOME/.borg-passphrase\"\nREPO_PATH=\"borg@172.16.1.236:/home/borg/borg-02\"\nREPO_BCK_DATE=\"$(date +%FT%T)\"\nBACKUPS=\"/etc/ /opt/ /home/ /root\"\n\necho \"==================== Creating Backup ====================\"\nborg create --stats ${REPO_PATH}::${REPO_BCK_DATE} ${BACKUPS}\necho \"==================== Prune ====================\"\nborg prune  --stats --keep-daily 3 --keep-weekly 2  --keep-monthly 5 ${REPO_PATH}\necho \"==================== List ====================\"\nborg list ${REPO_PATH}\n</code></pre> <p>This script write logs to the <code>syslog</code>:</p> Command<pre><code>cat /var/log/syslog | grep 'borgbackup'\n</code></pre> Output<pre><code>Dec 11 18:23:56 borg-02 borgbackup[4557]: ==================== Creating Backup ====================\nDec 11 18:23:58 borg-02 borgbackup[4557]: ------------------------------------------------------------------------------\nDec 11 18:23:58 borg-02 borgbackup[4557]: Archive name: 2021-12-11T18:23:56\nDec 11 18:23:58 borg-02 borgbackup[4557]: Archive fingerprint: d8be120187e55f630cd4816b3879b2aae83f262a00336c3d20bd62da96dce7fa\nDec 11 18:23:58 borg-02 borgbackup[4557]: Time (start): Sat, 2021-12-11 18:23:56\nDec 11 18:23:58 borg-02 borgbackup[4557]: Time (end):   Sat, 2021-12-11 18:23:57\nDec 11 18:23:58 borg-02 borgbackup[4557]: Duration: 0.37 seconds\nDec 11 18:23:58 borg-02 borgbackup[4557]: Number of files: 444\nDec 11 18:23:58 borg-02 borgbackup[4557]: Utilization of max. archive size: 0%\nDec 11 18:23:58 borg-02 borgbackup[4557]: ------------------------------------------------------------------------------\nDec 11 18:23:58 borg-02 borgbackup[4557]:                        Original size      Compressed size    Deduplicated size\nDec 11 18:23:58 borg-02 borgbackup[4557]: This archive:                1.61 MB            654.31 kB             53.47 kB\nDec 11 18:23:58 borg-02 borgbackup[4557]: All archives:                3.22 MB              1.31 MB            703.21 kB\nDec 11 18:23:58 borg-02 borgbackup[4557]:\nDec 11 18:23:58 borg-02 borgbackup[4557]:                        Unique chunks         Total chunks\nDec 11 18:23:58 borg-02 borgbackup[4557]: Chunk index:                     430                  870\nDec 11 18:23:58 borg-02 borgbackup[4557]: ------------------------------------------------------------------------------\nDec 11 18:23:58 borg-02 borgbackup[4557]: ==================== Prune ====================\nDec 11 18:24:00 borg-02 borgbackup[4557]: ------------------------------------------------------------------------------\nDec 11 18:24:00 borg-02 borgbackup[4557]:                        Original size      Compressed size    Deduplicated size\nDec 11 18:24:00 borg-02 borgbackup[4557]: Deleted data:               -1.61 MB           -654.32 kB            -53.48 kB\nDec 11 18:24:00 borg-02 borgbackup[4557]: All archives:                1.61 MB            654.31 kB            649.73 kB\nDec 11 18:24:00 borg-02 borgbackup[4557]:\nDec 11 18:24:00 borg-02 borgbackup[4557]:                        Unique chunks         Total chunks\nDec 11 18:24:00 borg-02 borgbackup[4557]: Chunk index:                     425                  435\nDec 11 18:24:00 borg-02 borgbackup[4557]: ------------------------------------------------------------------------------\nDec 11 18:24:00 borg-02 borgbackup[4557]: ==================== List ====================\nDec 11 18:24:01 borg-02 borgbackup[4557]: 2021-12-11T18:23:56                  Sat, 2021-12-11 18:23:56 [d8be120187e55f630cd4816b3879b2aae83f262a00336c3d20bd62da96dce7fa]\n</code></pre> <p>You can schedule this script using <code>crontab</code>:</p> <pre><code>0       2       *       *       *       /root/do-backup.sh\n</code></pre>"},{"location":"Blog/2021/12/11/Centralised_Backup_With_Borg/#save-backup-to-google-drive","title":"Save Backup To Google Drive","text":"<p>As I mentioned early in this article, one of my important goal is to save my backups to Google Drive. I did not bother too much with this. There is an excellent tool for linux: Rclone</p> <p>After configuring <code>rclone</code> you can upload the backups to Google Dirve from your central Borg server. Example command: <pre><code>rclone sync -P /home/borg/ gdrive:/borgbackup\n</code></pre></p> <p>Quote</p> <p>Sync the source to the destination, changing the destination only.  Doesn't transfer unchanged files, testing by size and modification time or MD5SUM.  Destination is updated to match source, including deleting files if necessary.</p> <p>Info</p> <p>Since all of the backups are encrypted we don't need to bother with extra encryption or password protection. </p> <p>There are two disadvantages over Duplicati from my point of view: </p> <ul> <li>This way we store the backup twice. (On the Borg central server and Google Drive)</li> <li>We need a running Borg Server. <ul> <li>This can be avoided. You can create local backups and upload them individually from the hosts.</li> </ul> </li> </ul> <p>Thank you for reading!  I don't know if this post was useful for you or not, but I think giving more and more examples is always helpful.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/","title":"Home Assistant Switch Timer With Tasmota Pulsetime","text":""},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#motivation","title":"Motivation","text":"<p>I have several switches in my house and garden which can be controlled from Home Assistant. But sometimes I forget to turn them off. So there are some lights in the garden which should turn off after a certain time. For example I almost always forgot to turn off the light which directed to front gate of my garden after my arrives home. All of my devices are flahed with Tasmota firmware. Tasmota has a built in command to turn off the relay after certain period of time called \"Pulsetime\":</p> <p>Quote</p> <p>PulseTime  Display the amount of PulseTime remaining on the corresponding Relay <p> Set the duration to keep Relay ON when Power ON command is issued. After this amount of time, the power will be turned OFF. 0 / OFF = disable use of PulseTime for Relay 1..111 = set PulseTime for Relay in 0.1 second increments 112..64900 = set PulseTime for Relay, offset by 100, in 1 second increments. Add 100 to desired interval in seconds, e.g., PulseTime 113 = 13 seconds and PulseTime 460 = 6   minutes (i.e., 360 seconds)   Note if you have more than 8 relays: Defined PulseTime for relays &lt;1-8&gt; will also be active for correspondent Relay &lt;9-16&gt;. <p>My goal is to set this <code>PulseTime</code> value from my Home Assistant dashboard. There are several ways to achieve my goal, but I wanted to use the \"native\" Tasmota way. Maybe the only noticeable reason for using <code>PulsTime</code> is that this solution work even if the Home Assistant server or the network become unavailable, and you turn on your light with a button or switch. Many Sonoff device have button on them with which you can control the device without network connection.</p> <p>Most of you may want to use the Home Assistant Timer function, and there are a lot of article about this topic on the Internet, but not much which focus on the <code>PulseTime</code> feature.</p> <p>Before we begin I show you how it works, and you can decide to read more or leave. :D</p> <p>Here is a picture of my controlling Dashboard:</p> <p> </p> Fig 1 <p>I will refer to this picture a lot later in this post as <code>Fig 1</code>.</p> <ol> <li>Drop Down list: You can select which device do you want to set up.<ul> <li>Helper Name: <code>input_select.timer_set</code></li> <li>Related Automation: Changing The Device In The Dropdown List</li> </ul> </li> <li>Actual Value: This widget show you the actual value of the selected device. This box is automatically updated right after you select the device from the Drop Down list.<ul> <li>Helper Name: <code>input_number.pulsetimeactualvalue</code></li> <li>Related Automation: Catch MQTT Message</li> </ul> </li> <li>Input field: You can specify the new value. Right after you typed the new number the PulseTime value is sent to the device via MQTT. Meanwhile the Actual value also updated.<ul> <li>Helper Name: <code>input_number.number</code></li> <li>Related Automation: Set New PulseTime Value</li> </ul> </li> <li>Button for send the command to the device. This can be useful if you want to set the same value for several devices. Select the first device, set the new value, then select another device and without typing the value just push the button. This button is also useful if the device did not get the message for some reason and you want to resend.<ul> <li>Related Script: PulseTime Set New</li> </ul> </li> <li>This button updates the \"Debug\" Markdown filed (6.). The 5. and 6. elements are optional and only for debugging.<ul> <li>Related Script: Check Debug</li> </ul> </li> </ol> <p>I try to explain my solution as detailed as I can, and I hope you will be able to adopt it to your setup if you want.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#end-device-configuration-tasmota-mqtt","title":"End Device Configuration (Tasmota - MQTT)","text":""},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#topic","title":"Topic","text":"<p>The first and really important thing is to see how I use the mqtt device topic. </p> <p>This is my schema: <code>sonoff/[DEVICE MAC ADDRESS]/[%prefix%]</code></p> <p>Info</p> <p>I use the MAC address without colons. Example: <code>AABBCCDDEE</code> or <code>1E2F3D4C5A</code></p> <p>Screenshot Example</p> <p></p> <p>Topic examples:</p> <ul> <li>Command: <code>sonoff/600194AD2A3C/cmnd/power</code></li> <li>Command: <code>sonoff/tasmotas/cmnd/status</code></li> <li>State: <code>sonoff/600194AD2A3C/tele/STATE</code></li> <li>Status:<code>sonoff/600194AD2A3C/stat/STATUS</code></li> </ul> <p>Why I am using MAC address? Although MAC address is not so human friendly, but unique and clearly identifies the device. Example: You can easily find the device in your router if you are using DHCP. But, later you will see that no matter what topic you are using, it does not effect the <code>PulseTime</code> configuration much, you just have to understand how it works and adopt your to topic.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#home-assistant-integration","title":"Home Assistant Integration","text":"<p>Official documentation: https://www.home-assistant.io/integrations/tasmota/</p> <p>Important options:</p> <ul> <li>Native discovery: <code>SetOption19 0</code></li> <li><code>SetOption30 0</code> for switches. (default)</li> <li><code>SetOption30 1</code> for lights. </li> <li>(Optional) <code>GroupTopic1 tasmotas</code></li> </ul> <p>Other useful commands (Using <code>Grouptopic</code> and MQTT):</p> <ul> <li>Set Timezone according my country. <pre><code>sonoff/tasmotas/cmnd/Timezone -m '99'\nsonoff/tasmotas/cmnd/TimeDST -m '0,0,3,1,2,120'\nsonoff/tasmotas/cmnd/TimeSTD -m '0,0,10,1,3,60'\n</code></pre></li> <li>Set <code>teleperiod</code> to 30 secs <pre><code>sonoff/tasmotas/cmnd/TelePeriod -m '30'\n</code></pre></li> <li>Set Syslog server <pre><code>sonoff/tasmotas/cmnd/SysLog -m '3'\nsonoff/tasmotas/cmnd/LogHost -m '172.16.0.100'\nsonoff/tasmotas/cmnd/LogPort -m '514'\n</code></pre></li> <li>Display hostname and IP address in GUI <pre><code>sonoff/tasmotas/cmnd/SetOption53 -m '1'\n</code></pre></li> </ul>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#home-assistant-helpers","title":"Home Assistant - Helpers","text":"<p>We have to configure some helpers on the webUI: <code>Configuration--&gt;Helpers</code></p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#new-value","title":"New Value","text":"<ul> <li>Fig 1 / 3</li> <li>Helper name: <code>input_number.number</code> (Sorry for the naming :) )</li> </ul> <p>This Helper is responsible for storing the new <code>Pulsetime</code>  value. </p> <p></p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#actual-value","title":"Actual Value","text":"<ul> <li>Fig 1 / 2</li> <li>Helper name:  <code>input_number.pulsetimeactualvalue</code> </li> </ul> <p>This Helper holds the actual value of the selected Device.</p> <p></p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#actual-mac","title":"Actual MAC","text":"<ul> <li>Fig 1 / 6 (Actual MAC)</li> <li>Helper name: <code>input_text.pulsetimeactualmac</code> </li> </ul> <p>This will store the actual topic, and displayed in the debug box. (Sorry for the naming, again)</p> <p></p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#last-result-message","title":"Last Result Message","text":"<ul> <li>Fig 1 / 6 (Last Result)</li> <li>Helper name: <code>input_text.result</code> </li> </ul> <p>This stores the MQTT message from the device.</p> <p></p> <p>Example message: <pre><code>{\n  \"PulseTime1\": {\n    \"Set\": 0,\n    \"Remaining\": 0\n  }\n}\n</code></pre></p> <p>Unfortunately tasmota don't send the actual <code>PulseTime</code> value with any of the <code>tele</code> messages, thus manually trigger is required. There are two situation when tasmota reply with this message:</p> <ul> <li>When you set up a new value. <code>sonoff/BCDDC2802856/cmnd/pulsetime1 -m 1000</code></li> <li>Or send <code>null</code> message to the above topic. <code>sonoff/BCDDC2802856/cmnd/pulsetime1 -n</code></li> </ul>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#drop-down-list","title":"Drop Down list","text":"<ul> <li>Fig 1 / 1</li> <li>Helper name: <code>input_select.timer_set</code> </li> </ul> <p>Here we build a list contains all the device which we want to use with <code>Pulsetime</code>. You can use human readable name or whatever you want. </p> <p></p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#home-assistant-automations","title":"Home Assistant - Automations","text":""},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#changing-the-device-in-the-dropdown-list","title":"Changing The Device In The Dropdown List","text":""},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#trigger","title":"Trigger","text":"<p>This automation is triggered immediately after you have selected a new device form the drop down list.</p> <pre><code>trigger:\n  - platform: state\n    entity_id: input_select.timer_set\n</code></pre>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#action","title":"Action","text":"<pre><code>action:\n  - service: input_text.set_value\n    target:\n      entity_id: input_text.pulsetimeactualmac\n    data:\n      value: |\n        {% set mapper =\n          { 'Left Side Light':'sonoff/600194AD2A3C/cmnd/pulsetime1',\n            'Kitchen Spot':'sonoff/600194AD7228/cmnd/pulsetime1',\n            'Front Light': 'sonoff/DC4F22378237/cmnd/pulsetime1',\n            'S20 - 02 - Bathroom Fan': 'sonoff/BCDDC28027AD/cmnd/pulsetime1',\n            'S20 - 01 - Christmas Light': 'sonoff/BCDDC2802856/cmnd/pulsetime1'\n          } %}\n        {% set mac = mapper[states('input_select.timer_set')] %}\n        {% if mac == NULL %}\n          UNKNOWN\n        {% else %}\n          {{ mac }}\n        {% endif %}\n  - service: mqtt.publish\n    data:\n      topic: |\n        {{ states('input_text.pulsetimeactualmac') }}\n</code></pre> <p>As you can see we have one <code>trigger</code> and two <code>action</code>s.  The trigger is really simple: this automation is triggered when you select another device, as I wrote before.</p> <p>We have to create a <code>mapper</code> to know which topic is mapped to the selected device. This was the hardest part of the whole project, figure out how to create relation between the selected device name and the topic to which we send messages. You may think it would be easier if I used the name of the device in the topic instead of the MAC address. But no. I have some 4CH sonoff device, and in this case the we have different <code>Pulstime</code> values for each channel. (<code>Pulsetime1, Pulsetime2 . . .</code>) So we need exact match between the selected equipment  (switch) and the device plus the channel number. This <code>mapper</code> was the best solution I could find.</p> <p>Caution</p> <p>You have to specify the exact text here which you entered in the <code>input_select</code> helpers (case sensitive and check the leading and trailing white spaces twice)</p> <p>So the <code>input_text.pulsetimeactualmac</code> will be either one of the topic or <code>UNKNOWN</code> if no mapper found.</p> <p>The other action (<code>mqtt.publish</code>) publishes the previously selected topic using the mapper. Notice that we don't have <code>message</code> part here, just the topic. As I mentioned earlier this way the tasmota device will reply with the actual value in JSON format. To better understanding here is an example with Mosquitto:</p> <pre><code>mosquitto_pub -h [MQTT HOST] -u [MQTT USERNAME] -P [MQTT PASSWORD] -t 'sonoff/BCDDC2802856/cmnd/pulsetime1' -n\n</code></pre> <p>Reply: <pre><code>sonoff/BCDDC2802856/stat/RESULT {\"PulseTime1\":{\"Set\":0,\"Remaining\":0}}\n</code></pre></p> <p>We have two goals with this JSON message:</p> <ul> <li>Store the \"Set\" value (0) to <code>input_number.pulsetimeactualvalue</code> helper. </li> <li>Store the entire JSON message to <code>input_text.result</code> helper for debugging.</li> </ul> <p>But to achive this we need another automation which I named \"Catch MQTT Message\" and the subject of the next chapter.</p> <p>So the <code>mqtt.publish</code> action publishes <code>sonoff/BCDDC2802856/cmnd/pulsetime1</code> and we store the entire JSON <code>{\"PulseTime1\":{\"Set\":0,\"Remaining\":0}}</code> to <code>input_text.result</code> for debugging, and the '0' to <code>input_number.pulsetimeactualvalue</code>. </p> <p>Download entire <code>yaml</code>: hass-automation-dropdown.yaml</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#catch-mqtt-message","title":"Catch MQTT Message","text":"<p>Let's see how we handle the received message.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#trigger_1","title":"Trigger","text":"<pre><code>trigger:\n  - platform: mqtt\n    topic: sonoff/+/stat/RESULT\n    id: frommqtt\n</code></pre> <p>We subsribed for the <code>sonoff/+/stat/RESULT</code> topic. </p> <p>Info</p> <p>If you need explanation about what the <code>+</code> sign means please visit this site: https://www.hivemq.com/blog/mqtt-essentials-part-5-mqtt-topics-best-practices/ With one world: single-level wild card.</p> <p>Ok. But we have a problem. Tasmota may publish other result messages than <code>Pulsetime</code>. For example when a light turned on or off. Example:</p> <pre><code>sonoff/6001949C6548/stat/RESULT {\"POWER1\":\"ON\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER2\":\"ON\"}\nsonoff/DC4F22378237/stat/RESULT {\"POWER\":\"ON\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER3\":\"ON\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER4\":\"ON\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER1\":\"OFF\"}\nsonoff/DC4F22378237/stat/RESULT {\"POWER\":\"OFF\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER2\":\"OFF\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER3\":\"OFF\"}\nsonoff/6001949C6548/stat/RESULT {\"POWER4\":\"OFF\"}\n</code></pre> <p>This will cause error when we try to parese the json... I want to explain what I'm talking about through an example.</p> <p>As we discussed earlier we need the actual pulsetime from the device/channel. We can parse the JSON like this:</p> <p>Command<pre><code>echo -n '{\"PulseTime1\":{\"Set\":0,\"Remaining\":0}}' | jq -r '.PulseTime1.Set'\n</code></pre> Output<pre><code>0\n</code></pre></p> <p>But what happens with the <code>{\"POWER4\":\"OFF\"}</code> message:</p> <p>Command<pre><code>echo -n '{\"POWER4\":\"OFF\"}' | jq -r '.PulseTime1.Set'\n</code></pre> Output<pre><code>null\n</code></pre></p> <p>We got a big <code>null</code>. To avoid error or warn messages in the Home Assistant log we should filter the messages and process only the relevants. That's why we have condition.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#conditions","title":"Conditions","text":"<pre><code>condition:\n  - condition: template\n    value_template: '{{ ''PulseTime'' in  trigger.payload }}'\n  - condition: template\n    value_template: &gt;-\n      {{ trigger.topic.split('/')[1] in states('input_text.pulsetimeactualmac')\n      }}\n</code></pre> <p>The first is a really simple condition. If the received JSON contains <code>PulseTime</code> we proceed to the Actions.</p> <p>The second is a bit more complicated. We have to be sure that the MQTT message came from the right device. Why? I explain you: You selected the \"Kitchen Spot\" from the drop down list. The <code>input_number.pulseTimeActualValue</code> and <code>input_text.pulsetimeactualmac</code> set according to the selection. That's rigth. But if you send <code>PulseTime1</code> command to another device for example from the WebUI, the Automation is triggerd and set the wrong values. Simply: we need to check the MAC address and \"Pulstime\" word, as well.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#actions","title":"Actions","text":"<pre><code>action:\n  - service: input_number.set_value\n    target:\n      entity_id: input_number.pulseTimeActualValue\n    data:\n      value: |\n        {% if \"PulseTime1\" in trigger.payload_json %}\n          {% set pulseTime =  trigger.payload_json.PulseTime1.Set | int %}\n        {% elif \"PulseTime2\" in trigger.payload_json %}\n          {% set pulseTime =  trigger.payload_json.PulseTime2.Set | int %}\n        {% elif \"PulseTime3\" in trigger.payload_json %}\n          {% set pulseTime =  trigger.payload_json.PulseTime3.Set | int %}\n        {% elif \"PulseTime4\" in trigger.payload_json %}\n          {% set pulseTime =  trigger.payload_json.PulseTime4.Set | int %}\n        {% else %}\n          {% set pulseTime =  \"notset\" %}\n        {% endif %}\n\n        {% if  pulseTime != 'notset' %}\n          {% if  pulseTime == 0 %}\n            0\n          {% else %}\n            {{ ( pulseTime | int - 100 ) / 60 }}\n          {% endif %}\n        {% endif %}\n  - service: input_text.set_value\n    target:\n      entity_id: input_text.result\n    data:\n      value: '{{trigger.payload_json}}'\n</code></pre> <p>When a new device is selected from the drop down list (Fig 1 / 1) we want to tihngs:</p> <ul> <li>See the actual value (Fig 1 / 2) - The first action is responsible for this (<code>input_number.set_value</code>)</li> <li>Display the entire JSON in the Debug box (Fig 1 / 6) The second action will do this (<code>input_text.set_value</code>)</li> </ul> <p>Do remember that this Automation is closely connected to the previos one. We can say that the result of previously discussed Automation triggers this one: </p> <ol> <li>You select a new device from the drop down list</li> <li>\"Changing The Device In The Dropdown List\" Automation is triggerdm and publish MQTT message.</li> <li>This Automation catches the MQTT JSON message and process it.</li> </ol> <p>The first action set the value of <code>input_number.pulseTimeActualValue</code> helper. But we have problem again. What about the multi-channel devices? We want to cofigure each channel separately, right? So we have to figure out the received message related to which channel? I've solved this problem with some <code>if - elif</code> cndition: If the JSON playload contains \"PulseTime1\" than we parse the json and store the value of the \"Set\" into the <code>pulseTime</code> variable.</p> <p>Maybe this block have to be mentioned, as well: <pre><code>        {% if  pulseTime != 'notset' %}\n          {% if  pulseTime == 0 %}\n            0\n          {% else %}\n            {{ ( pulseTime | int - 100 ) / 60 }}\n          {% endif %}\n        {% endif %}\n</code></pre></p> <p>This all magic because of this behaviour:</p> <p>Quota</p> <p>112..64900 = set PulseTime for Relay, offset by 100, in 1 second increments. Add 100 to desired interval in seconds, e.g., PulseTime 113 = 13 seconds and PulseTime 460 = 6 minutes (i.e., 360 seconds) <ul> <li>If we get <code>0</code> it means <code>0</code> --&gt; PulseTime is disabled</li> <li>If we get for example <code>160</code>  it means <code>160 - 100=60</code> seconds (1min).</li> </ul> <p>The second action (<code>input_text.set_value</code>) simply set the <code>input_text.result</code> helper value to the entire JSON message.</p> <p>Downlaod entire <code>yaml</code>: hass-automation-catchmqtt.yaml</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#set-new-pulsetime-value","title":"Set New PulseTime Value","text":"<p>Now we have only one Automation left to discuss. Everything we did before is useless if we can't set new value. This Automation is intended for do this. I can say that this is the most simpler from all. Everithing is prepared in the previous automations, so the only purpose of this automation is to set the desired new value.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#trigger_2","title":"Trigger","text":"<pre><code>trigger:\n  - platform: state\n    entity_id: input_number.number\n</code></pre> <p>Simple trigger: when a new value is set. I think no explanation is required.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#action_1","title":"Action","text":"<pre><code>action:\n  - service: mqtt.publish\n    data:\n      topic: |\n        {{ states('input_text.pulsetimeactualmac') }}\n      payload: |\n        {% set timer = ( trigger.to_state.state |  int * 60  ) + 100  %}\n        {% if timer == 100 %}\n          0\n        {% else %}\n          {{ timer }}\n        {% endif %}\n</code></pre> <p>As we discussed earlier tasmota handle the pulsetime in a bit strange way. Do remember what we did, when we needed to transform the received value: <pre><code>{{ ( pulseTime | int - 100 ) / 60 }}\n</code></pre></p> <p>Now we should do the opposite: <pre><code>{% set timer = ( trigger.to_state.state |  int * 60  ) + 100  %}\n</code></pre></p> <p>What does <code>{% if timer == 100 %}</code>  mean? If you set the PulseTime value to zero (disable pulsetime): <code>0 * 60 + 100 = 100</code> That's why we need to specially handle the <code>100</code> value.</p> Quota <p>112..64900 = set PulseTime for Relay, offset by 100, in 1 second increments. Add 100 to desired interval in seconds, e.g., PulseTime 113 = 13 seconds and PulseTime 460 = 6 minutes (i.e., 360 seconds) <p>Download entire <code>yaml</code>: hass-automation-setnew.yaml</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#home-assistant-scipts","title":"Home Assistant Scipts","text":"<p>Ok we are almost done. Only two part we have to mention. Do you remember the \"PulseTime Set New\" and \"Check Debug\" buttons (Fig 1 / 4 and 5)? </p> <p>Hint</p> <p>I don't provide download link to scripts, because the entire scripts are copied here.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#pulsetime-set-new","title":"PulseTime Set New","text":"<pre><code>alias: PulseTime Set New\nsequence:\n  - service: mqtt.publish\n    data:\n      topic: |\n        {{ states('input_text.pulsetimeactualmac') }}\n      payload: |\n        {% set timer = ( states('input_number.number') |  int * 60  ) + 100  %}\n        {% if timer == 100 %}\n          0\n        {% else %}\n          {{ timer }}\n        {% endif %}\nmode: single\n</code></pre> <p>I think every part of this script is discussed earlier and I don't want to repeat myself. :D</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#check-debug","title":"Check Debug","text":"<pre><code>sequence:\n  - service: mqtt.publish\n    data:\n      topic: '{{ states(''input_text.pulsetimeactualmac'') }}'\nmode: single\nalias: Pulsetime Check Debug\nicon: mdi:update\n</code></pre> <p>Simply publish to topic stored in <code>input_text.pulsetimeactualmac</code> helper.</p>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#home-assistant-dashboard","title":"Home Assistant Dashboard","text":"<p>For your convenience  I share the dashboard I use (Fig 1). Here you can check how each components are used.</p> <pre><code>title: Otthon\nviews:\n  - title: Timer\n    path: prtoba\n    icon: mdi:hammer-wrench\n    badges: []\n    cards:\n      - type: entities\n        entities:\n          - input_select.timer_set\n      - type: horizontal-stack\n        cards:\n          - type: entity\n            entity: input_number.pulsetimeactualvalue\n            name: Actual\n          - type: entities\n            entities:\n              - input_number.number\n            title: Set New\n      - type: horizontal-stack\n        cards:\n          - type: button\n            tap_action:\n              action: toggle\n            entity: script.pulsetime_set_new\n            icon: mdi:check\n            icon_height: 50px\n          - type: button\n            tap_action:\n              action: toggle\n            entity: script.update_all_infos\n            show_state: false\n            show_icon: true\n            icon_height: 30px\n            name: Check Debug\n            icon: mdi:eye-check-outline\n      - type: markdown\n        content: |+\n          * Last Result:\n          ```json\n          {{ states('input_text.result') }}\n          ```\n\n          * Actual MAC:\n          ```json\n          {{ states('input_text.pulsetimeactualmac') }}\n          ```\n\n          * Split\n          ```json\n          {{ states('input_text.pulsetimeactualmac').split('/')[1] }}\n          ```\n        title: Debug\n</code></pre>"},{"location":"Blog/2021/12/11/Home_Assistant_Switch_Timer_With_Tasmota_Pulsetime/#summary","title":"Summary","text":"<p>I do know that there are several other easier ways to achieve this functionality, but I think using Tasmota native <code>Pulsetime</code> feature is a good approach.</p> <p>Thank you for reading, and hope this article (or some parts of it) was helpful for you.</p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/","title":"Yet Another Article About Docker Logging With Fluentd","text":""},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#motivation","title":"Motivation","text":"<p>I have three hosts with Docker + Portainer:</p> <ul> <li>Two VPS server with public IP address</li> <li>A home server behind NAT</li> </ul> <p>I want to show all the logs from containers in one place. </p> <p>I already have a Kubernetes cluster at home, on which I have Kibana and Elasticsearch deployed for the cluster logging. It is obvious to use my already existing logging solution to collect logs from the Docker hosts.</p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#install-fluentd-on-docker-hosts","title":"Install Fluentd On Docker Hosts","text":"<p>Official Documentation: https://docs.fluentd.org/installation/install-by-deb</p> <p>Example installation on Debian bullseye:</p> <pre><code>curl -fsSL https://toolbelt.treasuredata.com/sh/install-debian-bullseye-td-agent4.sh | sh\n</code></pre>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#configure-td-agent","title":"Configure Td-Agent","text":"<p>Default config file location: <code>/etc/td-agent/td-agent.conf</code></p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#input-for-docker-daemon","title":"Input For Docker Daemon","text":"<p>Documentation: https://docs.fluentd.org/input/forward</p> <p>This section is responsible for receiving the logs from the Docker daemon.</p> <pre><code>&lt;source&gt;\n  @type forward\n  port 24224\n  bind 0.0.0.0\n&lt;/source&gt;\n</code></pre>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#output-configuration","title":"Output Configuration","text":"<p>As I mentioned in the short motivation section I want to store the logs in my Elasticsearch cluster.</p> <p>Documentation: https://docs.fluentd.org/output/elasticsearch</p> <p>Configuration example:</p> <pre><code>&lt;match {syslog.**,dockerdaemon.**}&gt;\n  @type elasticsearch\n  suppress_type_name true\n  host \"10.8.0.30\"\n  scheme http\n  path \"\"\n  port 32367\n  include_tag_key true\n  reload_connections false\n  reconnect_on_error true\n  reload_on_failure false\n  logstash_format true\n  logstash_prefix \"vps10\"\n  &lt;buffer&gt;\n    @type file\n    path /var/log/td-agent/buffer\n    flush_thread_count 8\n    flush_interval 5s\n    chunk_limit_size 2M\n    queue_limit_length 32\n    retry_max_interval 30\n    retry_forever true\n  &lt;/buffer&gt;\n&lt;/match&gt;\n</code></pre> <p>Some important settings and its explanation:</p> <code>suppress_type_name</code> <p>In Elasticsearch 7.x, Elasticsearch cluster complains the following types removal warnings <pre><code>  {\n  \"type\": \"deprecation\",\n  \"timestamp\": \"2020-07-03T08:02:20,830Z\",\n  \"level\": \"WARN\",\n  \"component\": \"o.e.d.a.b.BulkRequestParser\",\n  \"cluster.name\": \"docker-cluster\",\n  \"node.name\": \"70dd5c6b94c3\",\n  \"message\": \"[types removal] Specifying types in bulk requests is deprecated.\",\n  \"cluster.uuid\": \"NoJJmtzfTtSzSMv0peG8Wg\",\n  \"node.id\": \"VQ-PteHmTVam2Pnbg7xWHw\"\n}\n</code></pre></p> <code>host \"10.8.0.30\"</code> <p>Elasticsearch hostname or IP address. This VPS server is connecting to Elasticsearch over Wireguard VPN.</p> <code>port 32367</code> <p>My Elasticsearch is running on my Kubernetes cluster, and I'm using NodePort to access it. </p> <code>logstash_format</code> <p>This is meant to make writing data into Elasticsearch indices compatible to what Logstash calls them. By doing this, one could take advantage of Kibana. See <code>logstash_prefix</code> and <code>logstash_dateformat</code> to customize this index name pattern. The index name will be <code>#{logstash_prefix}-#{formatted_date}</code></p> <code>reload_connections false</code> <p>You can tune how the elasticsearch-transport host reloading feature works. By default it will reload the host list from the server every 10,000th request to spread the load. This can be an issue if your Elasticsearch cluster is behind a Reverse Proxy, as Fluentd process may not have direct network access to the Elasticsearch nodes.</p> <code>reconnect_on_error</code> <p>Indicates that the plugin should reset connection on any error (reconnect on next send). By default it will reconnect only on \"host unreachable exceptions\". We recommended to set this true in the presence of elasticsearch shield.</p> <code>reload_on_failure</code> <p>Indicates that the elasticsearch-transport will try to reload the nodes addresses if there is a failure while making the request, this can be useful to quickly remove a dead node from the list of addresses.</p> <p>The <code>reload_connections</code>, <code>reconnect_on_error</code>, <code>reload_on_failure</code> setting are needed because may Elasticsearch cluster has only one node and Fluentd connects to it over VPN and <code>NodePort</code>. </p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#syslog-input","title":"Syslog Input","text":"<pre><code>&lt;source&gt;\n  @type tail\n  path /var/log/syslog,/var/log/messages\n  pos_file /var/log/td-agent/syslog.pos\n  tag syslog.*\n  &lt;parse&gt;\n    @type syslog\n  &lt;/parse&gt;\n&lt;/source&gt;\n</code></pre> <p>Parser documentation: https://docs.fluentd.org/parser/syslog</p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#syslog-filter","title":"Syslog Filter","text":"<pre><code>&lt;filter syslog.**&gt;\n  @type record_transformer\n  &lt;record&gt;\n    hostname \"#{Socket.gethostname}\"\n    tag ${tag}\n  &lt;/record&gt;\n&lt;/filter&gt;\n</code></pre> <p>What does this filter do?</p> <ul> <li>Adds the fluentd tag to the json message. (<code>Line 13</code>) This can be very useful for debugging, as well.</li> <li>Adds hostname field to the json message. (<code>Line 12</code>)</li> </ul> <p>Example: <pre><code>{\n  \"_index\": \"vps10-2021.12.18\",\n  \"_type\": \"_doc\",\n  \"_id\": \"KD5Azn0BkfuDokpII8aN\",\n  \"_version\": 1,\n  \"_score\": null,\n  \"_source\": {\n    \"host\": \"vps10\",\n    \"ident\": \"tailscaled\",\n    \"pid\": \"361\",\n    \"message\": \"netmap diff:\",\n    \"hostname\": \"vps10\",\n    \"tag\": \"syslog.var.log.syslog\",\n    \"@timestamp\": \"2021-12-18T16:54:03.000000000+01:00\"\n  },\n  \"fields\": {\n    \"@timestamp\": [\n      \"2021-12-18T15:54:03.000Z\"\n    ]\n  },\n  \"highlight\": {\n    \"tag\": [\n      \"@kibana-highlighted-field@syslog.var.log.syslog@/kibana-highlighted-field@\"\n    ]\n  },\n  \"sort\": [\n    1639842843000\n  ]\n} \n</code></pre></p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#docker-filter","title":"Docker Filter","text":"<pre><code>&lt;filter dockerdaemon.**&gt;\n  @type record_transformer\n  &lt;record&gt;\n    tag ${tag}\n  &lt;/record&gt;\n&lt;/filter&gt;\n</code></pre> <p>This is similar to the previous syslog filter.</p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#docker-daemon-config","title":"Docker Daemon Config","text":"<pre><code>{\n  \"log-driver\": \"fluentd\",\n  \"log-opts\": {\n    \"fluentd-address\": \"localhost:24224\",\n    \"fluentd-async\": \"true\",\n    \"tag\": \"dockerdaemon.{{.Name}}\"\n  }\n} \n</code></pre> Line 4 <p>Send logs to fluentd. Related fluentd config: input-for-docker-daemon</p> Line 5 <p>Docker connects to Fluentd in the background. Messages are buffered until the connection is established.   Doc: https://docs.docker.com/config/containers/logging/fluentd/#fluentd-async</p> Line 6 <p>Related Fluentd config: docker-filter </p> <p>Example JSON:</p> <pre><code>{\n  \"_index\": \"vps10-2021.12.18\",\n  \"_type\": \"_doc\",\n  \"_id\": \"JD9Qzn0BkfuDokpI9zL6\",\n  \"_version\": 1,\n  \"_score\": null,\n  \"_source\": {\n    \"source\": \"stderr\",\n    \"log\": \"level=info ts=2021-12-18T16:12:27.363614782Z caller=table_manager.go:171 msg=\\\"uploading tables\\\"\",\n    \"container_id\": \"a2afeb9b67029f94c0267f7e1d24adf1fa87fd13e8ab8aa232bcda44a951bff6\",\n    \"container_name\": \"/loki\",\n    \"tag\": \"dockerdaemon.loki\",\n    \"@timestamp\": \"2021-12-18T17:12:27.000000000+01:00\"\n  },\n  \"fields\": {\n    \"@timestamp\": [\n      \"2021-12-18T16:12:27.000Z\"\n    ]\n  },\n  \"highlight\": {\n    \"container_name\": [\n      \"/@kibana-highlighted-field@loki@/kibana-highlighted-field@\"\n    ]\n  },\n  \"sort\": [\n    1639843947000\n  ]\n}\n</code></pre> <p>Warning</p> <p>It's not enough to restart the docker daemon to take affects logging settings on containers. Every container have to be recreated. (not just restarted)</p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#benefits-of-using-proper-tags","title":"Benefits Of Using Proper Tags","text":"<p>In the examples above all of you containers are tagged with their name. It is useful when you want to parse default type of container logs. </p> <p>Another <code>/etc/docker/daemon.json</code> example:</p> <pre><code>{\n  \"log-driver\": \"fluentd\",\n  \"log-opts\": {\n    \"fluentd-address\": \"localhost:24224\",\n    \"fluentd-async\": \"true\",\n    \"tag\": \"docker.{{.Name}}.{{.ID}}\"\n  }\n}\n</code></pre> <p>On this host I have an apache web server container: <pre><code>docker ps --filter name=apache\nCONTAINER ID   IMAGE     COMMAND                  CREATED       STATUS      PORTS                                      NAMES\nd0379e25ca01   httpd     \"httpd-foreground -d\u2026\"   2 weeks ago   Up 2 days   0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp   apache\n</code></pre></p> <p>I want to parse the logs from apache container. Thanks to the proper tag settings I can write filter like this: <pre><code>&lt;filter docker.apache.**&gt;\n  @type parser\n  key_name log\n  reserve_data true\n  emit_invalid_record_to_error false\n  &lt;parse&gt;\n    @type apache\n    expression /^(?&lt;vhost&gt;[^ ]*) (?&lt;host&gt;[^ ]*) [^ ]* (?&lt;user&gt;[^ ]*) \\[(?&lt;time&gt;[^\\]]*)\\] \"(?&lt;method&gt;\\S+)(?: +(?&lt;path&gt;(?:[^\\\"]|\\\\.)*?)(?: +\\S*)?)?\" (?&lt;code&gt;[^ ]*) (?&lt;size&gt;[^ ]*) (?:\"(?&lt;referer&gt;(?:[^\\\"]|\\\\.)*)\" \"(?&lt;agent&gt;(?:[^\\\"]|\\\\.)*)\")?$/\n  &lt;/parse&gt;\n&lt;/filter&gt;\n</code></pre></p> <p>Matching apache log format: <pre><code>LogFormat \"%V:%p %h %l %u %t \\\"%r\\\" %&gt;s %O \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\"\" vhost_combined\n</code></pre></p> <p>Example JSON log message: <pre><code>{\n  \"_index\": \"nuc-2021.12.18\",\n  \"_type\": \"_doc\",\n  \"_id\": \"qj9Yzn0BkfuDokpIOGHF\",\n  \"_version\": 1,\n  \"_score\": null,\n  \"_source\": {\n    \"container_id\": \"d0379e25ca010ddcaa69890bde9561d2dd64433998cb0d310be47e965a95fbc9\",\n    \"container_name\": \"/apache\",\n    \"source\": \"stdout\",\n    \"log\": \"matrix.k8s.home.vinyosoft.info:443 68.183.156.15 - - [18/Dec/2021:17:20:25 +0100] \\\"PUT /_matrix/federation/v1/send/1639791664430 HTTP/1.1\\\" 403 6750 \\\"-\\\" \\\"Synapse/1.49.0\\\"\",\n    \"vhost\": \"matrix.k8s.home.vinyosoft.info:443\",\n    \"host\": \"68.183.156.15\",\n    \"user\": \"-\",\n    \"method\": \"PUT\",\n    \"path\": \"/_matrix/federation/v1/send/1639791664430\",\n    \"code\": \"403\",\n    \"size\": \"6750\",\n    \"referer\": \"-\",\n    \"agent\": \"Synapse/1.49.0\",\n    \"tag\": \"docker.apache.d0379e25ca01\",\n    \"@timestamp\": \"2021-12-18T17:20:25.000000000+01:00\"\n  },\n  \"fields\": {\n    \"@timestamp\": [\n      \"2021-12-18T16:20:25.000Z\"\n    ]\n  },\n  \"highlight\": {\n    \"container_name\": [\n      \"/@kibana-highlighted-field@apache@/kibana-highlighted-field@\"\n    ]\n  },\n  \"sort\": [\n    1639844425000\n  ]\n} \n</code></pre></p>"},{"location":"Blog/2021/12/18/Yet_Another_Article_About_Docker_Logging/#complete-fluentd-config","title":"Complete Fluentd Config","text":"Click Here For Raw Source<pre><code>&lt;source&gt;\n  @type forward\n  port 24224\n  #tag dockerdaemon.*\n  bind 0.0.0.0\n&lt;/source&gt;\n\n&lt;filter dockerdaemon.**&gt;\n  @type record_transformer\n  &lt;record&gt;\n    tag ${tag}\n  &lt;/record&gt;\n&lt;/filter&gt;\n\n\n&lt;filter dockerdaemon.caddy&gt;\n  @type parser\n  format json\n  reserve_data true\n  key_name  log\n&lt;/filter&gt;\n\n\n&lt;filter syslog.**&gt;\n  @type record_transformer\n  &lt;record&gt;\n    hostname \"#{Socket.gethostname}\"\n    tag ${tag}\n  &lt;/record&gt;\n&lt;/filter&gt;\n\n\n&lt;match {caddy.access,syslog.**,dockerdaemon.**}&gt;\n  @type elasticsearch\n  suppress_type_name true\n  host \"10.8.0.30\"\n  scheme http\n  path \"\"\n  port 32367\n  include_tag_key true\n  reload_connections false\n  reconnect_on_error true\n  reload_on_failure false\n  logstash_format true\n  logstash_prefix \"vps9\"\n  &lt;buffer&gt;\n    @type file\n    path /var/log/td-agent/buffer\n    flush_thread_count 8\n    flush_interval 5s\n    chunk_limit_size 2M\n    queue_limit_length 32\n    retry_max_interval 30\n    retry_forever true\n  &lt;/buffer&gt;\n&lt;/match&gt;\n</code></pre>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/","title":"How To Install Jitsi On Kubernetes","text":""},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#preface","title":"Preface","text":"<p>Jitsi can be installed several way, maybe the easiest way is using docker-compose.</p> <p>Link: https://jitsi.github.io/handbook/docs/devops-guide/devops-guide-docker/</p> <p>But if you have a Kubernetes cluster you may want to install Jitsi on you cluster. </p> <p>I found another article about thist topic, but it is a little different from my solution: https://sesamedisk.com/video-conferencing-with-jitsi-on-k8s/ The most notifable different that I use only one deployment for all component (web, prosody, jicofo and jvb). (One pod, multiple container.) However this way make almost impossible to scale your jitsi instance, but far enough for minimal deployment. Running multiple instance of Jitsi is not in my scope at this time.  Unfortunately the Official Jitsi documentation does not say too much about scaling: https://jitsi.github.io/handbook/docs/devops-guide/devops-guide-manual </p> <p>Kubernetes sclaing article: https://blog.mi.hdm-stuttgart.de/index.php/2021/03/11/how-to-scale-jitsi-meet/ I have never tried to scale jitsi, maybe later i give it a try.</p> <p>I don't want to write a lot of unnecessary lines here, so let's see how I deployed Jitsi on my K8S cluster.</p>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#configmap","title":"Configmap","text":"<p>Create a config maps which contains all the necessary environment variables.</p> <pre><code>kind: ConfigMap\napiVersion: v1\nmetadata:\n  name: jitsi-envs\n  namespace: matrix\ndata:\n  ETHERPAD_DEFAULT_PAD_TEXT: '\"Welcome to Web Chat!\\n\\n\"'\n  ETHERPAD_SKIN_NAME: colibris\n  ETHERPAD_SKIN_VARIANTS: '\"super-light-toolbar super-light-editor light-background full-width-editor\"'\n  ETHERPAD_TITLE: Video Chat\n  HTTPS_PORT: '443'\n  HTTP_PORT: '80'\n  JIBRI_RECORDER_PASSWORD: 57969777b35e6040791212b1aa26ff36\n  JIBRI_XMPP_PASSWORD: 38f7da1f75c78e27f7cf93fe772a79c1\n  JICOFO_AUTH_PASSWORD: fcd121582ec8322f5fc262a398fe7c8e\n  JIGASI_XMPP_PASSWORD: b984fb9c2509d4831419a1fa7e477dec\n  JVB_ADVERTISE_IPS: 23.82.62.51,138.42.98.6\n  JVB_AUTH_PASSWORD: 6ea25fdc9611b6df32a0dfac49fa773f\n  JVB_PORT: '30300'\n  PUBLIC_URL: https://jitsi.customdomain.com\n  TURN_CREDENTIALS: jr00QMMfECtfMCwKewZTPh23sdf3m3DeusUPkgQfFRTzUg1VC6KsBIiqiFFeP\n  TURN_HOST: 23.88.60.51\n  TURN_PORT: '3478'\n  TZ: Europe/Budapest\n  XMPP_BOSH_URL_BASE: http://127.0.0.1:5280\n  XMPP_SERVER: localhost\n</code></pre> <ul> <li>JVB_ADVERTISE_IPS &amp; JVB_PORT</li> </ul> <p>These options are really important. I have two kubernetes nodes with public, static ip addresses, and I'm using NodePort Service to access JVB service. That's why you can see two IP addresses in my example configuration. By default <code>JVB_PORT</code> is 10000, but NodePorts are between 30000 and 32767 by default, so you need to use port from this range. (And you should open this port on your firewall)</p> <ul> <li>PUBLIC_URL</li> </ul> <p>This is the public URL of your Jitsi instance. This domain will be set in the Kubernetes Ingress.</p> <ul> <li>TURN_CREDENTIALS &amp; TURN_HOST &amp; TURN_PORT</li> </ul> <p>These setting are for devices behind NAT. If you have your own TURN server, configure these values accordingly. If you don't have TURN server you may find a free one by a little Googling. At the and of this post I will share you an example coturn configuration.</p> <ul> <li>XMPP_BOSH_URL_BASE &amp; XMPP_SERVER</li> </ul> <p>Since we will have only one POD and multiple containers inside it, we should use localhost to reach another component. If you have multiple containers inside a POD, you have to use localhost to reach one container from another. So contained inside the POD cannot bind the same port.</p> <p>Link: https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/</p> <p>If you experiencing connection timeout error messages, check the nginx configuration in the web container. </p> <pre><code># BOSH\nlocation = /http-bind {\n    proxy_set_header X-Forwarded-For $remote_addr;\n    proxy_set_header Host meet.jitsi;\n\n    proxy_pass http://127.0.0.1:5280/http-bind?prefix=$prefix&amp;$args;\n}\n\n\n# xmpp websockets\nlocation = /xmpp-websocket {\n    tcp_nodelay on;\n\n    proxy_http_version 1.1;\n    proxy_set_header Connection $connection_upgrade;\n    proxy_set_header Upgrade $http_upgrade;\n    proxy_set_header Host meet.jitsi;\n    proxy_set_header X-Forwarded-For $remote_addr;\n\n    proxy_pass http://127.0.0.1:5280/xmpp-websocket?prefix=$prefix&amp;$args;\n}\n</code></pre> <p>Don't modify the nginx confuguration by hand, insted double check the XMPP_BOSH_URL_BASE &amp; XMPP_SERVER system environments.</p>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#persistent-volume","title":"Persistent Volume","text":"<p>The Jitsi Deployment needs a persistent volume. </p> <pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: jitsi-conf\n  namespace: matrix\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: # &lt;-- Your storageclass name goes here\n  volumeMode: Filesystem\n</code></pre> <p>Note</p> <p>We can use only one persistent volume for all component, since all containers are running in the same pod. Otherwise you need a volume that supports ReadWriteMany access mode, or shedule all pods to the same node, or have separate PVC for all component.</p>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#deployment","title":"Deployment","text":"Jitsi Deployment<pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: jitsi\n  namespace: matrix\n  labels:\n    k8s-app: jitsi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: jitsi\n  template:\n    metadata:\n      name: jitsi\n      creationTimestamp: null\n      labels:\n        k8s-app: jitsi\n    spec:\n      volumes:\n        - name: jitsi-conf\n          persistentVolumeClaim:\n            claimName: jitsi-conf\n      containers:\n        - name: web\n          image: jitsi/web:stable-7882\n          envFrom:\n            - configMapRef:\n                name: jitsi-envs\n          env:\n            - name: DOCKER_HOST_ADDRESS\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: spec.nodeName\n            - name: JVB_WS_SERVER_ID\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.name\n          resources: {}\n          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: web-config\n            - name: jitsi-conf\n              mountPath: /var/spool/cron/crontabs\n              subPath: crontabs\n            - name: jitsi-conf\n              mountPath: /usr/share/jitsi-meet/transcripts\n              subPath: transcripts\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n        - name: prosody\n          image: jitsi/prosody:stable-7882\n          envFrom:\n            - configMapRef:\n                name: jitsi-envs\n          resources: {}\n          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: prosody-config\n            - name: jitsi-conf\n              mountPath: /prosody-plugins-custom\n              subPath: prosody-plugins-custom\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n        - name: jicofo\n          image: jitsi/jicofo:stable-7882\n          envFrom:\n            - configMapRef:\n                name: jitsi-envs\n          resources: {}\n          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: jicofo-config\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n        - name: jvb\n          image: jitsi/jvb:stable-7882\n          envFrom:\n            - configMapRef:\n                name: jitsi-envs\n          resources: {}\n          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: jvb-config\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <p>Common parts:</p> <ul> <li>System environments</li> </ul> <p>All containers use the same ConfigMap as System Environments. It should not cause any problem because there is no overlap across the containers. (There is no container using tha same property, but different value)</p> <pre><code>          envFrom:\n            - configMapRef:\n                name: jitsi-envs\n</code></pre> <ul> <li>Peristent Volume</li> </ul> <p>We use the same PersistentVolume, but different Subpath:</p> <ul> <li>Web</li> </ul> <pre><code>          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: web-config\n            - name: jitsi-conf\n              mountPath: /var/spool/cron/crontabs\n              subPath: crontabs\n            - name: jitsi-conf\n              mountPath: /usr/share/jitsi-meet/transcripts\n              subPath: transcripts\n</code></pre> <ul> <li>Prosody</li> </ul> <pre><code>          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: prosody-config\n            - name: jitsi-conf\n              mountPath: /prosody-plugins-custom\n              subPath: prosody-plugins-custom\n</code></pre> <ul> <li>Jicofo</li> </ul> <pre><code>          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: jicofo-config\n</code></pre> <ul> <li>Jvb</li> </ul> <pre><code>          volumeMounts:\n            - name: jitsi-conf\n              mountPath: /config\n              subPath: jvb-config\n</code></pre>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#services","title":"Services","text":""},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#web","title":"Web","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jitsi-web\n  namespace: matrix\nspec:\n  internalTrafficPolicy: Cluster\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: 443\n  selector:\n    k8s-app: jitsi\n  sessionAffinity: None\n  type: ClusterIP\n</code></pre>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#jvb-nodeport","title":"JVB (NodePort)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: jitsi-jvb\n  namespace: matrix\nspec:\n  externalTrafficPolicy: Cluster\n  internalTrafficPolicy: Cluster\n  ipFamilyPolicy: SingleStack\n  ports:\n  - nodePort: 30300\n    port: 30300\n    protocol: UDP\n    targetPort: 30300\n  selector:\n    k8s-app: jitsi\n  sessionAffinity: None\n  type: NodePort\n</code></pre>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#ingress","title":"Ingress","text":"<pre><code>kind: Ingress\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: jitsi-web\n  namespace: matrix\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/proxy-connect-timeout: '3600'\n    nginx.ingress.kubernetes.io/proxy-read-timeout: '3600'\n    nginx.ingress.kubernetes.io/proxy-send-timeout: '3600'\n    nginx.ingress.kubernetes.io/server-snippet: |\n      location / {\n      proxy_set_header Upgrade $http_upgrade;\n      proxy_http_version 1.1;\n      proxy_set_header X-Forwarded-Host $http_host;\n      proxy_set_header X-Forwarded-Proto $scheme;\n      proxy_set_header X-Forwarded-For $remote_addr;\n      proxy_set_header Host $host;\n      proxy_set_header Connection \"upgrade\";\n      proxy_cache_bypass $http_upgrade;\n      }\nspec:\n  tls:\n    - hosts:\n        - jitsi.customdomain.com\n      secretName: jitsi-https\n  rules:\n    - host: jitsi.customdomain.com\n      http:\n        paths:\n          - pathType: ImplementationSpecific\n            backend:\n              service:\n                name: jitsi-web\n                port:\n                  name: http\n</code></pre> <p>Note</p> <p>The ingress should work without the <code>nginx.ingress.kubernetes.io/server-snippet</code> annotation, but I leave it here for example.</p>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#matrix-element-integration","title":"Matrix Element Integration","text":"<p>If you have Synapse Matrix server, you can use your newly create Jitsi deployment for conference calls, if you are using Elemnt clients.</p> <p>Link: https://element.io/get-started</p> <p>You need to add the following annotations to the Ingress of the Matrix server:</p> <pre><code>    nginx.ingress.kubernetes.io/server-snippet: |\n      location /.well-known/matrix/client {\n        return 200 '{\"m.homeserver\":{\"base_url\":\"https://matrix.customdomain.com\"},\"im.vector.riot.jitsi\":{\"preferredDomain\": \"jitsi.customdomain.com\"}}';\n        default_type application/json;\n        add_header Access-Control-Allow-Origin *;\n      }\n</code></pre> <p>This server-snippet will instruct the Element client (web/ios/android) to use your Jitsi URL insted of the default one.  Android and IOS clients may required to reinstall.</p> <p>Related docs:</p> <ul> <li>https://github.com/vector-im/element-web/blob/develop/docs/jitsi.md</li> <li>https://github.com/vector-im/element-android/issues/7230</li> </ul>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#bonus-coturn-configuration","title":"[Bonus] - Coturn Configuration","text":""},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#install","title":"Install","text":"<pre><code>apt install coturn\nmkdir /var/log/turn\nchown -R turnserver:turnserver /var/log/turn\n</code></pre>"},{"location":"Blog/2022/10/17/jitsi_on_kubernetes/#etcturnserverconf","title":"/etc/turnserver.conf","text":"<pre><code>listening-ip=123.118.161.21\nuse-auth-secret\nstatic-auth-secret=jr00QMMfECtfMCwKewZTPh23sdf3m3DeusUPkgQfFRTzUg1VC6KsBIiqiFFeP\ntotal-quota=1200\nno-tls\nno-dtls\nno-tcp-relay\nlog-file=/var/log/turn/turn.log\nnew-log-timestamp\nno-multicast-peers\ndenied-peer-ip=10.0.0.0-10.255.255.255\ndenied-peer-ip=0.0.0.0-0.255.255.255\ndenied-peer-ip=100.64.0.0-100.127.255.255\ndenied-peer-ip=127.0.0.0-127.255.255.255\ndenied-peer-ip=169.254.0.0-169.254.255.255\ndenied-peer-ip=192.0.0.0-192.0.0.255\ndenied-peer-ip=192.0.2.0-192.0.2.255\ndenied-peer-ip=192.88.99.0-192.88.99.255\ndenied-peer-ip=198.18.0.0-198.19.255.255\ndenied-peer-ip=198.51.100.0-198.51.100.255\ndenied-peer-ip=203.0.113.0-203.0.113.255\ndenied-peer-ip=240.0.0.0-255.255.255.255\n</code></pre>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/","title":"How To Use Mkdocs + Meterial In Kubernetes","text":""},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#preface","title":"Preface","text":"<p>In this guide I show you how I use mkdocs in my Kubernetes cluster.  There are uncountable ways to do something similar, but I think this post can be useful for you. The main ascpect of my solution is to build the documentation in an init container, and serve the page with nginx. This way the only thing you have to do is rollout the deployment after new version of your mkdocs is released (pushed to your repository).</p>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#required-containers","title":"Required Containers","text":""},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#git-downloader","title":"Git Downloader","text":"<p>This container aims to download the git repository to the ephemeral storage. If you don't want to use my repository you can build your own image.</p>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#dockerfile","title":"Dockerfile","text":"quay.io/jvincze84/mkdocs-init:v0.7<pre><code>FROM alpine:latest\nRUN apk add \\\nbash \\\ngit\nADD run.sh /\nCMD [\"/run.sh\"]\n</code></pre> <p>As you can see It's a really simple Dockerfile based on the latest alpine image.</p>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#shell-scirpt","title":"Shell Scirpt","text":"run.sh<pre><code>#!/bin/bash\nGIT_URL=\"${GIT_URL:-gogs.vincze.work}\"\nGIT_REPO=\"${GIT_REPO:-jvincze/priv-knowledge-mkdocs}\"\nGIT_USER=\"${GIT_USER:-jvincze}\"\nGIT_PASS=\"${GIT_PASS:-admin}\"\nGIT_AUTH=\"${GIT_AUTH:-true}\"\nUSER_UID=\"${USER_UID:-1000}\"\n\nSTORAGE=\"${STORAGE:-/storage}\"\n\necho \"=== DEBUG &amp; Avaiable Variables===\"\necho \"GIT_URL: $GIT_URL\"\necho \"GIT_REPO: $GIT_REPO\"\necho \"GIT_USER: $GIT_USER\"\necho \"GIT_AUTH: $GIT_AUTH\"\necho \"GIT_PASS: ********\"\necho \"USER_UID: $USER_UID\"\n\n[ ! -d $STORAGE ] &amp;&amp; mkdir -p $STORAGE\n\n\ncd $STORAGE\n\nif [ $GIT_AUTH == \"true\" ]\nthen\n  git clone https://$GIT_USER:$GIT_PASS@$GIT_URL/$GIT_REPO .\nelse\n  git clone https://$GIT_URL/$GIT_REPO .\nfi\n\n\necho\necho \"--- Setting permissions\"\nchown -R $USER_UID:$USER_UID $STORAGE\n\nls -la $STORAGE\necho \"Container Done\"\n</code></pre> <p>Warning</p> <p>You should not modify the <code>$STORAGE</code> environment  variable unless you change the Deployment as well. </p> <p>Important</p> <p>When you create the <code>run.sh</code> file, don't forget to add the executable bit. (<code>chmod +x run.sh</code>)</p>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#mkdocs-builder","title":"Mkdocs Builder","text":"<p>This container will build the site for the nginx web server.</p>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#dockerfile_1","title":"Dockerfile","text":"quay.io/jvincze84/mkdocs-build:2.14<pre><code>FROM python:3-alpine3.14\n\n\nARG USER=1000\n\nRUN adduser -h /usr/src/mkdocs -D -u $USER mkdocs \\\n&amp;&amp; apk add bash \\\n&amp;&amp; apk add git \n\nENV PATH=\"${PATH}:/usr/src/mkdocs/.local/bin\"\n\nUSER mkdocs\nRUN mkdir -p /usr/src/mkdocs/build\nWORKDIR /usr/src/mkdocs/build\n\nRUN pip install --upgrade pip\n\nRUN pip install pymdown-extensions \\\n&amp;&amp; pip install mkdocs \\\n&amp;&amp; pip install mkdocs-material \\\n&amp;&amp; pip install mkdocs-rtd-dropdown \\\n&amp;&amp; pip install mkdocs-git-revision-date-plugin \\\n&amp;&amp; pip install mkdocs-git-revision-date-localized-plugin \\\n&amp;&amp; pip install mkdocs-blog-plugin \\\n&amp;&amp; pip3 install mkdocs-blogging-plugin\n\n# The following line is bcause of mkdocs build error: Unable to read git logs of\nRUN git config --global --add safe.directory '*'\n\nENTRYPOINT [\"/usr/src/mkdocs/.local/bin/mkdocs\"]\n\nLABEL mkdocs.image=\"3-alpine3.14\"\n</code></pre> <p>Warning</p> <p>The <code>USER</code> argument must match with the <code>USER_UID</code> in the previous step. (Git Downloader / Shell Scirpt)</p> <p>You can add extra extension(s) by modifying  the <code>pip install</code> section, any other parts of the Dockerfile should not be modified.</p>"},{"location":"Blog/2022/10/20/mkdocs-kubernetes/#deployment","title":"Deployment","text":"<p>Let's see the Depolyment yaml file.</p> <pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: read-the-docs\n  namespace: mkdocs\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: mkdocs-priv\n  template:\n    metadata:\n      name: mkdocs-priv\n      creationTimestamp: null\n      labels:\n        k8s-app: mkdocs-priv\n    spec:\n      volumes:\n        - name: shared\n          emptyDir: {}\n      initContainers:\n        - name: git\n          image: quay.io/jvincze84/mkdocs-init:v0.7\n          env:\n            - name: GIT_AUTH\n              value: 'false'  \n            - name: GIT_REPO\n              value: jvincze84/jvincze84.github.io\n            - name: GIT_URL\n              value: github.com\n          resources: {}\n          volumeMounts:\n            - name: shared\n              mountPath: /storage\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n        - name: build\n          image: quay.io/jvincze84/mkdocs-build:2.14\n          args:\n            - build\n            - '--clean'\n            - '--site-dir'\n            - /usr/src/mkdocs/build/srv\n          resources: {}\n          volumeMounts:\n            - name: shared\n              mountPath: /usr/src/mkdocs/build\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n      containers:\n        - name: nginx\n          image: nginx:latest\n          resources: {}\n          volumeMounts:\n            - name: shared\n              mountPath: /usr/share/nginx/html\n              subPath: srv\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: Always\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <p>The following environment variabels in the <code>git initContainer</code>  must be set accoring to your Git repository:</p> <ul> <li>GIT_AUTH: <code>true</code> or <code>false</code>. If your git repository uses auth set to <code>true</code> otherwise <code>false</code>.</li> <li>GIT_URL: URL of your repository. (Example: github.com) Only <code>https</code> is supported. If you need plain http connection you have to modify the  <code>run.sh</code> shell script (git clone).</li> <li>GIT_REPO: You can copy paste this value from the git URL in your browser. Example: <code>https://github.com/jvincze84/jvincze84.github.io</code> --&gt; <code>jvincze84/jvincze84.github.io</code></li> <li>GIT_USER: Your git username. Mandatory if you set <code>GIT_AUTH</code> to <code>true</code>.</li> <li>GIT_PASS: Your git password or token. Mandatory if you set <code>GIT_AUTH</code> to <code>true</code>. </li> </ul> <p>Additionally you may want to use kuberntes secret for storing the git password. In this case you have to add the following lines to the Deployment:</p> <pre><code>          env:\n            - name: GIT_PASS\n              valueFrom:\n                secretKeyRef:\n                  name: gitpass\n                  key: gitpass\n</code></pre> <p>You can create the secret with the following command:</p> <pre><code>kubectl -n mkdocs create secret generic gitpass --from-literal=gitpass=1234\n</code></pre> <p>Info</p> <ul> <li>The Deployment uses ephemeral storage (<code>emptyDir: {}</code>) so every time you deploy a new version or rollout the deployment the entire webpage is regenerated and the previous version won't be kept</li> <li><code>volumeMounts</code>, <code>mountPath</code> and <code>subPath</code> should not be modified, unless you know what, how and why you do that.</li> </ul> <p>Tip</p> <p>After you have done with the modification on your docs and pushed back to git, you can rollout the deployment with this command: <code>kubectl rollout restart -n mkdocs deployment read-the-docs</code>. Or simply delete the pod. :)</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/","title":"Deploy Elasticsearch Cluster &amp; Kibaba On Kubernetes","text":""},{"location":"Blog/2022/11/18/elk_on_kubernetes/#preface","title":"Preface","text":"<p>Nowadays maybe the most advanced and widly used log management and analizis system is the ELK stack. I have to  mention Graylog and Grafana Loki which are also great and advanced tools for montioring your environments and collect log files from them.</p> <p>There is another enterprise ready and feature rich log management system which based on Elasticsearch and Kibana: OpenSearch. If you are looking for a free alternaive to Elasticsearch you may want to give OpenSearch a try. I'm going to post about OpenSearch as well, but at this time I want to show you a method to install Elasticsearch &amp; Kibana on your Kubernetes cluster.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#requirements","title":"Requirements","text":"<ul> <li>A working Kubernetes cluster. The current version of my cluster: v1.24.4</li> <li>Kubectl cli tool</li> <li>Installed and ready to use Persistent Volume solution (Example Longhorn, OpenEBS, rook, etc)</li> <li>At least 2GB of free memory for Elasticsearch instances.</li> </ul>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#set-vmmax_map_count-to-at-least-262144","title":"Set <code>vm.max_map_count</code> To At Least 262144","text":"<p>This is a strict requirements of Elasticsearch. You have to set this value on each node you are planning to run Elasticsearch. You can select the nodes where to run Elasticsearch with nodeselectors and node labels.</p> <p>Add the following line to <code>/etc/sysctl.conf</code> file:</p> <pre><code>vm.max_map_count=262144\n</code></pre> <p>To apply the setting on a live system, run:</p> <pre><code>sysctl -w vm.max_map_count=262144\n</code></pre>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#prepareing","title":"Prepareing","text":"<p>The first and most important thing is to choose a names of your Elasticsearch cluster and Instances.  We will deploy Elasticsearch cluster as StatefulSet, so the name of instances will be sequential.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#create-certificates","title":"Create Certificates","text":"<ul> <li>Create a directory for your certificates:</li> </ul> <pre><code>mkdir /tmp/es-certs\nchown 1000:1000 /tmp/es-certs\n</code></pre> <ul> <li>Create the <code>instances.yml</code> file.</li> </ul> <pre><code>cat &lt;&lt;EOF&gt;/tmp/es-certs/instances.yml\ninstances:\n- name: elastic-0\n  dns:\n    - elastic-0.es-cluster\n    - localhost\n    - es-cluster\n  ip:\n    - 127.0.0.1\n- name: elastic-1\n  dns:\n    - elastic-1.es-cluster\n    - localhost\n    - es-cluster\n  ip:\n    - 127.0.0.1\n- name: elastic-2\n  dns:\n    - elastic-3.es-cluster\n    - localhost\n    - es-cluster\n  ip:\n    - 127.0.0.1\nEOF\n</code></pre> <p>Important</p> <p>The <code>- name: elastic-0</code> is must mach the StatefulSet name plus the sequence number appended by dash. The DNS (<code>- name: elastic-1 ... elastic-n</code>) name must mach the name of StatfulSet: <code>metadata.name: elastic</code> and the headless service name. [STATFULSET_NAME]-[NUMBER].[STATEFUL_SERVICE_NAME] The third DNS record is the neme of the Kubernetes (headless) Service. This will be used for Kubernetes internal use, for example for Kibana.</p> <ul> <li>Generate the certificates</li> </ul> <p>Run a temporary contianer to work in it:</p> <pre><code>docker run -v /tmp/es-certs:/usr/share/elasticsearch/config/certs -it --rm docker.elastic.co/elasticsearch/elasticsearch:8.5.1 /bin/sh\n</code></pre> <p>Run the following commands inside the container:</p> <pre><code># Generate CA certificates\nbin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip\nunzip config/certs/ca.zip -d config/certs\n\n# Generate Elasticsearch Certificates\nbin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key\nunzip config/certs/certs.zip -d config/certs\n</code></pre> <p>Exit from the container.</p> <p>After the certificate generation your folder and file should look like that:</p> <pre><code>/tmp/es-certs/\n/tmp/es-certs/certs.zip\n/tmp/es-certs/elastic-2\n/tmp/es-certs/elastic-2/elastic-2.key\n/tmp/es-certs/elastic-2/elastic-2.crt\n/tmp/es-certs/ca.zip\n/tmp/es-certs/elastic-0\n/tmp/es-certs/elastic-0/elastic-0.key\n/tmp/es-certs/elastic-0/elastic-0.crt\n/tmp/es-certs/instances.yml\n/tmp/es-certs/elastic-1\n/tmp/es-certs/elastic-1/elastic-1.crt\n/tmp/es-certs/elastic-1/elastic-1.key\n/tmp/es-certs/ca\n/tmp/es-certs/ca/ca.key\n/tmp/es-certs/ca/ca.crt\n</code></pre> <ul> <li>Move all files to the <code>/tmp/es-certs/</code></li> </ul> <pre><code>cd /tmp/es-certs\nfind . -mindepth 2 -maxdepth 2 -type f -ls -exec mv \"{}\" . \\;\nfind . -mindepth 1 -maxdepth 1 -type d -ls -exec rmdir \"{}\" \\;\n</code></pre> <p>Now your folder should be similar to this:</p> <pre><code>total 56\ndrwxr-xr-x  2 vinyo vinyo 4096 Nov 18 14:07 .\ndrwxrwxrwt 25 root  root  4096 Nov 18 14:08 ..\n-rw-rw-r--  1 vinyo root  1200 Nov 18 14:02 ca.crt\n-rw-rw-r--  1 vinyo root  1679 Nov 18 14:02 ca.key\n-rw-------  1 vinyo root  2519 Nov 18 14:02 ca.zip\n-rw-------  1 vinyo root  7851 Nov 18 14:04 certs.zip\n-rw-rw-r--  1 vinyo root  1220 Nov 18 14:04 elastic-0.crt\n-rw-rw-r--  1 vinyo root  1679 Nov 18 14:04 elastic-0.key\n-rw-rw-r--  1 vinyo root  1220 Nov 18 14:04 elastic-1.crt\n-rw-rw-r--  1 vinyo root  1679 Nov 18 14:04 elastic-1.key\n-rw-rw-r--  1 vinyo root  1220 Nov 18 14:04 elastic-2.crt\n-rw-rw-r--  1 vinyo root  1675 Nov 18 14:04 elastic-2.key\n-rw-r--r--  1 vinyo vinyo  299 Nov 18 14:04 instances.yml\n</code></pre>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#create-kubernetes-secrets-namespace","title":"Create Kubernetes Secrets &amp; Namespace","text":"<ul> <li>Certificates</li> </ul> <pre><code># Create the Namespace\nkubectl create ns logging\n\n# Delete the secret if it is already exists:\n# kubectl -n logging delete secret es-certs\nkubectl -n logging create secret generic es-certs --from-file=/tmp/es-certs\n</code></pre> <ul> <li>Elastic Password</li> </ul> <p><pre><code>kubectl -n logging create secret generic elastic-password --from-literal=elastic=Admin1234\n</code></pre> You shoud replace <code>Admin1234</code> (of course).</p> <p>You will use this username/password to login to Kiabana.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#elasticsearch-statefulset-service","title":"ElasticSearch StatefulSet &amp; Service","text":""},{"location":"Blog/2022/11/18/elk_on_kubernetes/#statefulset","title":"StatefulSet","text":"docs/files/es-statefulset.yaml<pre><code>kind: StatefulSet\napiVersion: apps/v1\nmetadata:\n  name: elastic\n  namespace: logging\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: elastic\n  template:\n    metadata:\n      name: elastic\n      creationTimestamp: null\n      labels:\n        k8s-app: elastic\n    spec:\n      volumes:\n        - name: es-certs\n          secret:\n            secretName: es-certs\n            defaultMode: 420\n      containers:\n        - name: elastic\n          image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n          env:\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.name\n            - name: SERVICENAME\n              value: es-cluster\n            - name: cluster.name\n              value: $(SERVICENAME)              \n            - name: node.name\n              value: $(NODENAME).$(SERVICENAME)\n            - name: discovery.seed_hosts\n              value: elastic-0.es-cluster,elastic-1.es-cluster,elastic-3.es-cluster\n            - name: cluster.initial_master_nodes\n              value: elastic-0.es-cluster,elastic-1.es-cluster,elastic-3.es-cluster\n            - name: ES_JAVA_OPTS\n              value: '-Xms2g -Xmx2g'\n            - name: xpack.security.enabled\n              value: 'true'\n            - name: xpack.security.http.ssl.enabled\n              value: 'true'\n            - name: xpack.security.http.ssl.key\n              value: certs/$(NODENAME).key\n            - name: xpack.security.http.ssl.certificate\n              value: certs/$(NODENAME).crt\n            - name: xpack.security.http.ssl.certificate_authorities\n              value: certs/ca.crt\n            - name: xpack.security.http.ssl.verification_mode\n              value: certificate\n            - name: xpack.security.transport.ssl.enabled\n              value: 'true'\n            - name: xpack.security.transport.ssl.key\n              value: certs/$(NODENAME).key\n            - name: xpack.security.transport.ssl.certificate\n              value: certs/$(NODENAME).crt\n            - name: xpack.security.transport.ssl.certificate_authorities\n              value: certs/ca.crt\n            - name: xpack.security.transport.ssl.verification_mode\n              value: certificate\n            - name: ELASTIC_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: elastic-password\n                  key: elastic\n          resources:\n            limits:\n              cpu: 1500m\n              memory: 3Gi\n            requests:\n              cpu: 250m\n              memory: 2Gi\n          volumeMounts:\n            - name: es-data\n              mountPath: /usr/share/elasticsearch/data\n              subPath: data\n            - name: es-certs\n              readOnly: true\n              mountPath: /usr/share/elasticsearch/config/certs\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: true\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 1000\n      schedulerName: default-scheduler\n  volumeClaimTemplates:\n    - kind: PersistentVolumeClaim\n      apiVersion: v1\n      metadata:\n        name: es-data\n        creationTimestamp: null\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1Gi\n        storageClassName: local-hostpath\n        volumeMode: Filesystem\n  serviceName: es-cluster\n  podManagementPolicy: OrderedReady\n  updateStrategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  minReadySeconds: 10\n</code></pre>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#important-parts-of-the-manifests","title":"Important Parts Of The Manifests","text":""},{"location":"Blog/2022/11/18/elk_on_kubernetes/#persistentvolumeclaim","title":"PersistentVolumeClaim","text":"<pre><code>  volumeClaimTemplates:\n    - kind: PersistentVolumeClaim\n      apiVersion: v1\n      metadata:\n        name: es-data\n        creationTimestamp: null\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 1Gi\n        storageClassName: local-hostpath\n        volumeMode: Filesystem\n</code></pre> <p>I really recommend to use some kind of hostpath volume, for example OpenEBS, since Elasticsearch operations can be IO heavy. If you decide to use OpenEBS hostpath all the POD will be scheduled to the same host all the time.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#environment-variables","title":"Environment Variables","text":"<pre><code>            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.name\n</code></pre> <p>This variable is not used direrctly by the pod itself. It is just for this manifest. The value is the name of the StatefulSet. It's purpose to use in other variables. (<code>metadata.name</code> could not be nested)</p> <pre><code>            - name: SERVICENAME\n              value: es-cluster\n</code></pre> <p>This must mach with the <code>serviceName: es-cluster</code> in this manifest, and the neme of the headless Service.</p> <pre><code>            - name: node.name\n              value: $(NODENAME).$(SERVICENAME)\n</code></pre> <p>Each Elasticsearch instance created by the StatefulSet get the node name like elastic-0.es-clsuster, elastic-1.es-clsuster, etc. This is really important for the next parameters:</p> <pre><code>            - name: discovery.seed_hosts\n              value: elastic-0.es-cluster,elastic-1.es-cluster,elastic-3.es-cluster\n            - name: cluster.initial_master_nodes\n              value: elastic-0.es-cluster,elastic-1.es-cluster,elastic-3.es-cluster\n</code></pre> <p>Important</p> <p>Now you can see that how important to decide the names of each component.  As I wrote above the DNS names in the <code>instances.yml</code> must mach these names. <code>elastic-0.es-cluster</code> means the [POD_NAME].[HEADLESS_SERVICE:metadata.name]. In our case the pod name is always the name of the StatefulSet + sequence number (because of the StatefulSet). This way the <code>elastic-[n].es-cluster</code> always points to the actual IP address of the pods create by the StatefulSet. </p> <p>Note</p> <p>You can increase or decrease the number of Elasticsearch instances, but keep in mind to modify these values:</p> <ul> <li>Certificate generation: Modify the <code>instances.yml</code>, and regenerate the certificates, but only <code>certs.zip</code> not the CA! Don't forget to update the Kubernetes secret.</li> <li>StatefulSet: <ul> <li><code>spec.replicas</code> </li> <li>Variables: <code>discovery.seed_hosts</code> &amp; <code>cluster.initial_master_nodes</code> According to the <code>instances.yml</code> file.</li> </ul> </li> </ul> <pre><code>            - name: xpack.security.http.ssl.key\n              value: certs/$(NODENAME).key\n            - name: xpack.security.http.ssl.certificate\n              value: certs/$(NODENAME).crt\n</code></pre> <p>Every node has its own certificate. That's why we need the <code>$(NODENAME)</code> variable. This way the <code>certs/$(NODENAME).crt</code> will be <code>certs/elastic-0.crt</code> for the first pod and <code>certs/elastic-1.crt</code> for the second one, etc.</p> <p>Note</p> <p>You can create a single certificate which holds all of the DNS record for all nodes, but it is antipattern and not recommended for security reason.</p> <pre><code>            - name: ELASTIC_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: elastic-password\n                  key: elastic\n</code></pre> <p>This is the password for the built-in <code>elastic</code> superuser.</p> <ul> <li>Volumes</li> </ul> <pre><code>          volumeMounts:\n            ...\n            ...\n            - name: es-certs\n              readOnly: true\n              mountPath: /usr/share/elasticsearch/config/certs\n</code></pre> <p>Here we mount the previously created Kubernetes secret which contains all of the necessary certificates.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#checks","title":"Checks","text":"<p>Get into the <code>elastic-0</code> pod:</p> <pre><code>kubectl -n logging exec -it elastic-0 -- /bin/sh\n</code></pre> <p>And run the following commands:</p> <pre><code>curl -i -k  -XGET https://localhost:9200/_cat/nodes?v -u 'elastic:Admin1234'\n</code></pre> ouptut<pre><code>HTTP/1.1 200 OK\nX-elastic-product: Elasticsearch\ncontent-type: text/plain; charset=UTF-8\ncontent-length: 302\n\nip          heap.percent ram.percent cpu load_1m load_5m load_15m node.role   master name\n10.26.6.107           14          83   1    1.57    1.83     1.59 cdfhilmrstw -      elastic-0.es-cluster\n10.26.4.230           37          83   2    0.58    0.76     0.62 cdfhilmrstw *      elastic-1.es-cluster\n</code></pre> <pre><code>curl -i -k  -XGET https://localhost:9200/_cat/allocation?v -u 'elastic:Admin1234'\n</code></pre> output<pre><code>HTTP/1.1 200 OK\nX-elastic-product: Elasticsearch\ncontent-type: text/plain; charset=UTF-8\ncontent-length: 314\n\nshards disk.indices disk.used disk.avail disk.total disk.percent host        ip          node\n     4       39.9mb    19.2gb     89.3gb    108.5gb           17 10.26.4.230 10.26.4.230 elastic-1.es-cluster\n     4       39.8mb    65.8gb     50.2gb    116.1gb           56 10.26.6.107 10.26.6.107 elastic-0.es-cluster\n</code></pre> <p>Hint</p> <p>As you can see I have only two nodes at the moment. But everything looks fine.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#deploy-kibana","title":"Deploy Kibana","text":""},{"location":"Blog/2022/11/18/elk_on_kubernetes/#prepare","title":"Prepare","text":"<p>First prepare the <code>kibana_system</code> built-in user password:</p> <p>Important</p> <p>Run the following command inside one of your elastic pod!!!</p> <pre><code>curl -k -i -X POST -u \"elastic:Admin1234\" -H \"Content-Type: application/json\" https://localhost:9200/_security/user/kibana_system/_password -d \"{\\\"password\\\":\\\"Admin123\\\"}\" \n</code></pre> output<pre><code>HTTP/1.1 200 OK\nX-elastic-product: Elasticsearch\ncontent-type: application/json\ncontent-length: 2\n\n{}\n</code></pre> <p>Warning</p> <p>Do not use the cli tools (/usr/share/elasticsearch/bin/elasticsearch-*) to update/reseet paswword. . This will create a file inside the /usr/share/elasticsearch/config directory, and after the pod restart this file will be gone.</p> <p>Note</p> <p>Please note that the password (<code>elastic:Admin1234</code>) comes from the <code>ELASTIC_PASSWORD</code> environment variable (pre-created secret).</p> <p>Create a Kuernetes secret:</p> <pre><code>kubectl -n logging create secret generic kibanasystem --from-literal=kibana_system=Admin123\n</code></pre>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#manifest","title":"Manifest","text":"docs/files/kibana-deployment.yaml<pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: kibana\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kibana\n  template:\n    metadata:\n      name: kibana\n      creationTimestamp: null\n      labels:\n        k8s-app: kibana\n    spec:\n      volumes:\n        - name: es-certs\n          secret:\n            secretName: es-certs\n            defaultMode: 420\n      containers:\n        - name: kibana\n          image: kibana:8.5.1\n          env:\n            - name: ELASTICSEARCH_HOSTS\n              value: https://es-cluster:9200\n            - name: ELASTICSEARCH_USERNAME\n              value: kibana_system\n            - name: ELASTICSEARCH_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: kibanasystem\n                  key: kibana_system\n            - name: SERVER_PUBLICBASEURL\n              value: https://kibana.vincze.work\n            - name: ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES\n              value: config/certs/ca.crt\n          resources: {}\n          volumeMounts:\n            - name: es-certs\n              readOnly: true\n              mountPath: /usr/share/kibana/config/certs\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <p>Noticeable parts:</p> <ul> <li>Kibana use the same secret to mount the certificate as  Elasticsearch. (volumeMounts: es-certs), but different mountPath: /usr/share/kibana/config/certs</li> <li>Set <code>SERVER_PUBLICBASEURL</code> to the hostname that you will use in your ingress. If you miss this step Kibana will warn you to correct this.</li> <li><code>ELASTICSEARCH_HOSTS</code>: This value points to the headless service. That's why we need to add <code>es-cluster</code> as DNS record in <code>instances.yml</code>.</li> <li><code>ELASTICSEARCH_USERNAME</code>: Do NOT modify this value. Older versions of Elasticsearch used <code>kibana</code>, but it is deprecated. The username should be <code>kibana_system</code>.</li> </ul>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#service","title":"Service","text":"<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: kibana\n  namespace: logging\nspec:\n  ports:\n    - name: web\n      protocol: TCP\n      port: 5601\n      targetPort: 5601\n  selector:\n    k8s-app: kibana\n  type: ClusterIP\n  sessionAffinity: None\n  ipFamilies:\n    - IPv4\n  internalTrafficPolicy: Cluster\n</code></pre>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#ingress","title":"Ingress","text":"<pre><code>kind: Ingress\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: kibana\n  namespace: logging\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/proxy-body-size: 100m\nspec:\n  tls:\n    - hosts:\n        - kibana.******.com\n      secretName: kibana-https\n  rules:\n    - host: kibana.vincze.work\n      http:\n        paths:\n          - pathType: ImplementationSpecific\n            backend:\n              service:\n                name: kibana\n                port:\n                  name: web\n</code></pre> <p>This is only an example ingress, so modify according to your needs.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#send-logs-to-the-elasticsearch-cluster","title":"Send Logs To The Elasticsearch Cluster","text":"<p>From inside the Kubernetes cluster it is really simple, just create a headless service:</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: es-cluster\n  namespace: logging\nspec:\n  ports:\n    - name: rest\n      protocol: TCP\n      port: 9200\n      targetPort: 9200\n    - name: inter-node\n      protocol: TCP\n      port: 9300\n      targetPort: 9300\n  selector:\n    k8s-app: elastic\n  clusterIP: None\n  type: ClusterIP\n  sessionAffinity: None\n  ipFamilies:\n    - IPv4\n  ipFamilyPolicy: SingleStack\n  internalTrafficPolicy: Cluster\n</code></pre> <p>Now you can use the <code>es-cluster.logging.svc.cluster.local</code> address.</p> <p>Accessing Elasticsearch from outside the Kubernetes cluster a bit more complicated and highly depends on your environment. I have never tried, but you may create an Ingress, since the port 9200 for API calls over HTTP. https://discuss.elastic.co/t/what-are-ports-9200-and-9300-used-for/238578</p> <p>Caution</p> <p>This way your Elasticsearch cluster may be exposed to the public Internet.</p> <p>Another way can be using NodePort serivce, or MetalLB LoadBalancer serivce.  Example MetalLB service:</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch\n  namespace: logging\n  annotations:\n    metallb.universe.tf/address-pool: default\nspec:\n  ports:\n    - name: tcp-9200\n      protocol: TCP\n      port: 9200\n      targetPort: 9200\n  selector:\n    k8s-app: elastic\n  type: LoadBalancer\n  sessionAffinity: None\n  externalTrafficPolicy: Cluster\n  ipFamilyPolicy: SingleStack\n  allocateLoadBalancerNodePorts: true\n  internalTrafficPolicy: Cluster\n</code></pre> <p>Info</p> <p>MetalLB will create nodeport(s), as well.</p> <p>Important</p> <p>Remember the DNS config in <code>instances.yaml</code>! When you accessing your Elasticsearch cluster the DNS or IP address must mach the entries in the <code>instances.yaml</code>. So if you create a DNS entry with <code>es.example.com</code> domain, this must present in the DNS entries. Or if you accessing the ES cluster over MetalLB service, the ip address of the service must be added to the IP sections.  Because of the Kubernetes service you don't know which pod will get the request that's why all node certificate should contain all possible domain name and/or IP address.</p>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#bonus-single-node-deployment","title":"Bonus - Single Node Deployment","text":"<p>If you want to test Elasticsarch or you don't need multinode environment you can deploy Elasticearch as a single node environment.</p> <pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: elastic\n  namespace: logging\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: elastic\n  template:\n    metadata:\n      name: elastic\n      creationTimestamp: null\n      labels:\n        k8s-app: elastic\n    spec:\n      volumes:\n        - name: es-data\n          persistentVolumeClaim:\n            claimName: es-data\n      containers:\n        - name: elastic\n          image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n          env:\n            - name: discovery.type\n              value: single-node\n            - name: cluster.name\n              value: es-single\n            - name: node.name\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.name\n            - name: ES_JAVA_OPTS\n              value: '-Xms2g -Xmx2g'\n            - name: xpack.security.enabled\n              value: 'true'\n            - name: xpack.security.http.ssl.enabled\n              value: 'false'\n            - name: xpack.security.transport.ssl.enabled\n              value: 'false'\n            - name: ELASTIC_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: elastic-password\n                  key: elastic\n          resources:\n            limits:\n              cpu: 1500m\n              memory: 3Gi\n            requests:\n              cpu: 250m\n              memory: 2Gi\n          volumeMounts:\n            - name: es-data\n              mountPath: /usr/share/elasticsearch/data\n              subPath: data\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: true\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 1000\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  minReadySeconds: 10\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <p>This is very similar to the StatefulSet, but notice the following parameters:</p> <ul> <li><code>discovery.type: single-node</code> --&gt; This indicates that only one ES node will be present.</li> <li><code>xpack.security.enabled: true</code> --&gt; Without this you won't be able to create users, and must find another way to protect Kibana. (Example: Ingress basic auth)</li> <li><code>xpack.security.*.ssl.enabled: false</code> --&gt; Use plain HTTP. If you set them true, you have to generate certificates and set up them as in the StatefulSet.</li> <li>The PersistentVolumeClaim must be pre-created.</li> </ul>"},{"location":"Blog/2022/11/18/elk_on_kubernetes/#references","title":"References","text":"<ul> <li>https://www.elastic.co/guide/en/kibana/8.5/docker.html</li> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html</li> <li>https://hub.docker.com/_/elasticsearch</li> </ul>"},{"location":"Blog/2023/04/21/install-kube-tailscale/","title":"Install Kubernetes Cluster Behind Tailscale VPN","text":""},{"location":"Blog/2023/04/21/install-kube-tailscale/#tldr","title":"TL;DR","text":"<p>In this post we will install a multi-master Kubernetes cluster behind Tailscale VPN. This scenario can be useful when:</p> <ul> <li>Your Kubernetes nodes are not in the same subnet. </li> <li>You are building a home-lab system, and the nodes are behind two or more NAT-ted network, or even behind CGNAT.</li> <li>Your nodes are running in separate data centers, and don't want to publish API ports on the public internet. </li> <li>You want to access your cluster only from private VPN network.</li> <li>You want extra security by encrypted connection between nodes.</li> <li>Or the mixture of above scenarios. </li> </ul> <p>Why Tailscale VPN?</p> <p>You can use any other VPN solution like Wireguard, OpenVPN, IPSec, etc. But nowadays I think Tailscale is the easiest way to bring up a VPN network. With a free registration you get 100 device, subnet routers, exit nodes, (Magic)DNS, and so many useful features. </p> <p>For more information check the following links:</p> <ul> <li>TailScale</li> <li>Tailscale Pricing</li> </ul> <p>But as I mentioned you can use any other VPN solution, personally I'm using Wireguard in my home-lab system.</p> <p>Warning</p> <p>Tailscale assigns IP address from <code>100.64.0.0/10</code> range! IP Address Assignment If you are planning to use Kube-OVN networking don't forget to change the CIDR, because Kube-OVN is also use this subnet!</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#infrastructure","title":"Infrastructure","text":"<p>As I mentioned we will deploy a multi-master Kubernetes cluster:</p> <ul> <li>3 master|worker nodes, without worker nodes. Later additional worker nodes can be added to the cluster, but for the simplicity we won't deploy extra worker nodes.</li> <li>We need an additional TCP load balancer for the API requests. I prefer HAProxy for this purpose, because it is easy to set up and lightweight. <ul> <li>For this lab I will deploy only one Load Balancer, but if you need HA solution, at least two Load Balancers are needed. This can be achieved by using Keppalived. Or you can use external load balancer like F5. But this demo is not about HA Load balancers, so it is just enough to have only one LB.</li> </ul> </li> </ul> Hostname Role IP Address VPN IP Address kube02-m1 Control Plane Node 1 172.16.1.77 Later kube02-m2 Control Plane Node 2 172.16.1.78 Later kube02-m3 Control Plane Node 3 172.16.1.79 Later kube02-haproxy HAProxy Load Balancer 172.16.1.80 Later ansible Ansible Host 172.16.0.252 --- <p>Note</p> <p>You don't need the additional Ansible host, if you preparing the OS manually. </p> <p>Note</p> <p>You can use one of the kubernetes node for HAProxy, but in this case you need to configure either the HAProxy listen port or <code>--apiserver-bind-port</code> (kubadm init).</p> <p>The nodes in this test environment are connected each other on the same subnet.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#hardware","title":"Hardware","text":"<p>These nodes are completely identical both on hardware and OS level, running on Proxmox Virtualization platform with KVM.</p> <ul> <li>2 CPU cores</li> <li>2GB memory</li> <li>32B system disk</li> <li>Debian OS</li> <li>Debian GNU/Linux 11 (bullseye)</li> <li>Kernel: <code>5.10.0-21-amd64</code></li> </ul>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#preparing-the-os","title":"Preparing The OS","text":"<p>In this post I'm using Ansible to prepare the Debian OSes for Kubernetes installation. I'm highly recommend to use some kind of automatization tool(s) or scirpt(s) to maintain your infrastructure, especially if you planning to have a bunch of nodes, not just a home-lab. And if something goes wrong you can start it over in a minute.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#ansible","title":"Ansible","text":"<p>Just a quick overview about my Ansible configuration and variables.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#ansiblecfg","title":"<code>ansible.cfg</code>","text":"Click Here For Raw Source<pre><code>[defaults]\ninventory = hosts\nhost_key_checking = False\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#myvarsyml","title":"<code>myvars.yml</code>","text":"Click Here For Raw Source<pre><code>yq_url: https://github.com/mikefarah/yq/releases/download/v4.30.8/yq_linux_amd64\nkube_version: 1.26.4-00\n\ncommon_packages:\n  - apt-transport-https\n  - sudo\n  - vim\n  - vnstat\n  - xz-utils\n  - tcpdump\n  - screen\n  - rsync\n  - sshpass\n  - git\n  - mc\n  - wget\n  - curl\n  - jq\n  - htop\n  - net-tools\n  - dnsutils\n  - ca-certificates\n  - telnet\n  - htop\n  - iotop\n  - curl\n  - gnupg\n  - lsb-release\n  - wireguard\n  - wireguard-tools\n  - whois\n  - samba-common\n  - smbclient\n  - cifs-utils\n  - nfs-common\n  - open-iscsi\n  - gnupg-agent\n  - software-properties-common\n\ndocker_packages:\n  - docker-ce\n  - docker-ce-cli\n  - containerd.io\n  - docker-compose-plugin\n</code></pre> <p>Details:</p> <ul> <li><code>yq_url</code>: Yq binary URL. This version of yq will be installed on the hosts.</li> <li><code>kube_version</code>: Here you can define which version of Kubernetes you want to install. (kubelet, kubeadm and kubectl)</li> <li><code>common_packages</code>: These packages will be installed on the hosts. \"Common packages\" because usually I install these packages on my VMs, regardless of deploying Docker or Kubernetes. </li> <li><code>docker_packages</code>: Packages for installing Docker/Containerd engine.</li> </ul>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#hosts","title":"<code>hosts</code>","text":"Click Here For Raw Source<pre><code>[pve-kube02]\n172.16.1.77\n172.16.1.78\n172.16.1.79\n172.16.1.80\n\n[pve-kube02:vars]\nansible_connection=ssh\nansible_user=root\nansible_password=rootpassword\n</code></pre> <p>Important</p> <p>Ansible host must access the VMs over ssh. Before you run any of the playbooks please enable root login. For example: <code>sed -i -e 's/^#\\(PermitRootLogin \\).*/\\1 yes/' /etc/ssh/sshd_config</code> and restart sshd daemon.  It is highly recommended to use dedicated ansible user (with sudo right) and ssh key authentication!  And don't forget to accept ssh key by login to the remotes systems before run the playbooks. If you are using other user than root, you may want to use <code>become: 'yes'</code> option it the plays.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#update","title":"Update","text":"<p>I usually start with updating the OS to the latest version, unless the application to be installed has strict requirements.</p> <p>playbook-upgrade-debian.yaml</p> Click Here For Raw Source<pre><code>- hosts: pve-kube02\n  name: Install\n  tasks:\n    - name: Run Included Task - Upgrade Ddebian\n      ansible.builtin.import_tasks:\n        file: task_allow_release_info_change.yaml\n    - name: Reboot the machine \n      ansible.builtin.reboot:\n</code></pre> <p>task_allow_release_info_change.yaml</p> Click Here For Raw Source<pre><code>- name: Allow release info change\n  lineinfile:\n    path: /etc/apt/apt.conf.d/99releaseinfochange\n    state: present\n    create: yes\n    line: Acquire::AllowReleaseInfoChange::Suite \"true\";\n\n- name: Update apt cache\n  ansible.builtin.apt:\n    update_cache: yes\n    upgrade: full\n</code></pre> <p>Run this playbook:</p> <pre><code>ansible-playbook playbook-upgrade-debian.yaml\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#install-common-packages","title":"Install Common Packages","text":"<p>playbook-install-common-packages.yaml</p> Click Here For Raw Source<pre><code>- name: Gather Facts\n  hosts: 127.0.0.1\n  connection: local\n  tasks:\n    - include_vars: myvars.yml\n    - name: Download Yq\n      ansible.builtin.get_url:\n        url: \"{{ yq_url }}\"\n        dest: /tmp/yq\n        mode: '755'\n    - name: Calculate MD5\n      ansible.builtin.stat:\n        path: /tmp/yq\n        checksum_algorithm: md5\n      register: yq_md5\n    - name: Delete /tmp/yq\n      ansible.builtin.file:\n        path: /tmp/yq\n        state: absent\n\n- hosts: pve-kube02\n  name: Install\n  become: 'yes'\n  tasks:\n    - include_vars: myvars.yml\n    - name: Run the equivalent of \"apt-get update\" as a separate step\n      ansible.builtin.apt:\n        update_cache: yes\n    - name: Set Fact For YQ md5\n      set_fact:\n        yq_checksum: \"{{ hostvars['127.0.0.1']['yq_md5'].stat.checksum }}\"\n    - name: debug\n      debug:\n        msg: \"MD5 hash : {{ yq_checksum }}\"\n    - name: Ensure a list of packages installed\n      ansible.builtin.apt:\n        name: \"{{ common_packages }}\"\n        state: present\n    - name: All done!\n      debug:\n        msg: Packages have been successfully installed\n    - name: Calculate Already Existing jq hash\n      ansible.builtin.stat:\n        path: /usr/bin/yq\n        checksum_algorithm: md5\n      register: exist_yq_md5\n    - name: Print Existing yq md5 hash\n      debug:\n        msg: \"MD5 hash of existing : {{ exist_yq_md5.stat.checksum }}\"\n      when: exist_yq_md5.stat.exists == true\n    - name: Remove Old Version Of YQ\n      ansible.builtin.file:\n        path: /usr/bin/yq\n        state: absent\n      when: exist_yq_md5.stat.exists == false or exist_yq_md5.stat.checksum != yq_checksum\n    - name: Download Yq\n      ansible.builtin.get_url:\n        url: \"{{ yq_url }}\"\n        dest: /usr/bin/yq\n        mode: '755'\n      when: exist_yq_md5.stat.exists == false or exist_yq_md5.stat.checksum != yq_checksum\n    - name: Fix Vimrc\n      ansible.builtin.replace:\n        path: /etc/vim/vimrc\n        regexp: '^\"\\s?(let g:skip_defaults_vim.*)'\n        replace: '\\1'\n    - name: Fix Vimrc 2\n      ansible.builtin.replace:\n        path: /etc/vim/vimrc\n        regexp: '^\"\\s?(set compatible.*)'\n        replace: '\\1'\n    - name: Fix Vimrc 3\n      ansible.builtin.replace:\n        path: /etc/vim/vimrc\n        regexp: '^\"\\s?(set background).*'\n        replace: '\\1=dark'\n    - name: Fix Vimrc 4\n      ansible.builtin.replace:\n        path: /etc/vim/vimrc\n        regexp: '^\"\\s?(syntax on).*'\n        replace: '\\1'\n    - name: Fix Vimrc 4\n      ansible.builtin.replace:\n        path: /etc/vim/vimrc\n        regexp: '^\"\\s?(set mouse).*'\n        replace: '\\1=c'\n    - name: Allow 'sudo' group to have passwordless sudo\n      lineinfile:\n        dest: /etc/sudoers\n        state: present\n        regexp: '^%sudo'\n        line: '%sudo   ALL=(ALL:ALL) NOPASSWD:ALL'\n        validate: visudo -cf %s\n</code></pre> <p>Run this playbook:</p> <pre><code>ansible-playbook playbook-install-common-packages.yaml\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#install-container-engine","title":"Install Container Engine","text":"<p>playbook-install-docker.yaml</p> Click Here For Raw Source<pre><code>- hosts: pve-kube02\n  become: 'yes'\n  tasks:\n    - include_vars: myvars.yml\n    - name: determine codeversion\n      command: \"lsb_release -cs\"\n      register: release_output\n    - set_fact:\n        codename: \"{{ release_output.stdout }}\"\n\n    - name: Run the equivalent of \"apt-get update\" as a separate step\n      ansible.builtin.apt:\n        update_cache: yes\n    - name: add Docker GPG key\n      apt_key:\n        url: https://download.docker.com/linux/debian/gpg\n        state: present\n    - name: add docker repository to apt\n      apt_repository:\n        repo: deb https://download.docker.com/linux/debian \"{{ codename }}\" stable\n        state: present\n    - name: add tailscale gpg key\n      apt_key:\n        url: https://pkgs.tailscale.com/stable/debian/bullseye.noarmor.gpg\n        state: present\n    - name: add tailscale repository to apt\n      apt_repository:\n        repo: deb https://pkgs.tailscale.com/stable/debian \"{{ codename }}\" main\n        state: present\n    - name: install docker\n      ansible.builtin.apt:\n        name: \"{{ docker_packages }}\"\n        state: present\n        update_cache: yes\n\n    - name: install tailscale\n      apt:\n        name: tailscale\n        state: latest\n\n    - name: check if docker is started properly\n      service:\n        name: docker\n        state: started\n        enabled: yes\n</code></pre> <p>Run this playbook:</p> <pre><code>ansible-playbook playbook-install-docker.yaml\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#install-kubernetes-tools","title":"Install Kubernetes Tools","text":"<p>playbook-install-kubernetes.yaml</p> Click Here For Raw Source<pre><code>- hosts: pve-kube02\n  become: 'yes'\n  vars:\n    kubepackages:\n      - kubelet={{ kube_version }}\n      - kubeadm={{ kube_version }}\n      - kubectl={{ kube_version }}\n\n  tasks:\n    - include_vars: myvars.yml\n    - name: Register architecture (dpkg_output)\n      command: \"dpkg --print-architecture\"\n      register: dpkg_output\n    - set_fact:\n        arch: \"{{ dpkg_output.stdout }}\"\n\n    - name: Register lsb_release\n      command: \"lsb_release -cs\"\n      register: release_output\n    - set_fact:\n        codename: \"{{ release_output.stdout }}\"\n\n\n    - name: Add Kubernetes gpg to keyring\n      apt_key:\n        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg\n        state: present\n\n    - name: add kubernetes repository to apt\n      apt_repository:\n        repo: deb https://apt.kubernetes.io/ kubernetes-xenial main\n\n    - name: Disable SWAP since kubernetes can't work with swap enabled (1/2)\n      shell: |\n        swapoff -a\n\n\n    - name: Disable SWAP in fstab since kubernetes can't work with swap enabled (2/2)\n      replace:\n        path: /etc/fstab\n        regexp: '^([^#].*?\\sswap\\s+sw\\s+.*)$'\n        replace: '# \\1'\n\n    - name: Enable overlay &amp; br_netfilter module\n      ansible.builtin.copy:\n        content: |\n          overlay\n          br_netfilter\n        dest: /etc/modules-load.d/k8s.conf\n\n    - name: Running modprobe\n      shell: |\n        modprobe overlay\n        modprobe br_netfilter\n\n\n    - name: Set up sysctl /etc/sysctl.d/k8s.conf\n      ansible.builtin.copy:\n        content: |\n          net.bridge.bridge-nf-call-iptables  = 1\n          net.bridge.bridge-nf-call-ip6tables = 1\n          net.ipv4.ip_forward                 = 1\n        dest: /etc/sysctl.d/k8s.conf\n\n    - name: Add the overlay module\n      community.general.modprobe:\n        name: overlay\n        state: present\n    - name: Add the br_netfilter module\n      community.general.modprobe:\n        name: br_netfilter\n        state: present\n    - name: sysctl\n      ansible.builtin.shell: \"sysctl --system\"\n\n    - name: Generate default containerd config\n      ansible.builtin.shell: \"containerd config default &gt; /etc/containerd/config.toml\"\n\n    - name: Change /etc/containerd/config.toml file SystemdCgroup  to true\n      ansible.builtin.replace:\n        path: /etc/containerd/config.toml\n        after: 'plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options'\n        before: 'plugins.\"io.containerd.grpc.v1.cri\".containerd.untrusted_workload_runtime]'\n        regexp: 'SystemdCgroup.*'\n        replace: 'SystemdCgroup = true'\n      diff: yes\n\n\n    - name: Run the equivalent of \"apt-get update\" as a separate step\n      ansible.builtin.apt:\n        update_cache: yes\n\n    - name: Install Kubernetes Packages\n      ansible.builtin.apt:\n        name: \"{{ kubepackages }}\"\n        state: present\n\n\n    - name: Prevent kubelet from being upgraded\n      ansible.builtin.dpkg_selections:\n        name: kubelet\n        selection: hold\n\n    - name: Prevent kubeadm from being upgraded\n      ansible.builtin.dpkg_selections:\n        name: kubeadm\n        selection: hold\n\n    - name: Prevent kubectl from being upgraded\n      ansible.builtin.dpkg_selections:\n        name: kubectl\n        selection: hold\n\n    - name: Prevent containerd.io from being upgraded\n      ansible.builtin.dpkg_selections:\n        name: containerd.io\n        selection: hold\n\n\n    - name: FIX CRICTRL error\n      ansible.builtin.copy:\n        content: |\n          runtime-endpoint: unix:///run/containerd/containerd.sock\n          image-endpoint: unix:///run/containerd/containerd.sock\n          timeout: 2\n          debug: false\n          pull-image-on-create: false\n        dest: /etc/crictl.yaml\n\n\n    - name: Restart service cron on centos, in all cases, also issue daemon-reload to pick up config changes\n      ansible.builtin.systemd:\n        state: restarted\n        daemon_reload: true\n        name: containerd\n\n    - name: Install docker-compose from official github repo\n      get_url:\n        url : https://github.com/docker/compose/releases/download/v2.15.1/docker-compose-linux-x86_64\n        dest: /usr/local/bin/docker-compose\n        mode: 'u+x,g+x'\n</code></pre> <p>Run this playbook:</p> <pre><code>ansible-playbook playbook-install-kubernetes.yaml\n</code></pre> <p>Now we have 3 identical nodes which are waiting for us to install &amp; configure Tailscale VPN and Kubernetes cluster.</p> <p>Before we proceed, I would like to advise you some really useful links and tips. These are helpful especially if you are not familiar with Ansible and don't want to bother with that:</p> <ul> <li>Update OS: <code>apt-get update --allow-releaseinfo-change</code> &amp;&amp; <code>apt-get upgrade</code></li> <li>Common Packages: You can install all necessary packages with <code>apt-get install</code> command.</li> <li>Install Container Engine</li> <li>Install Tailscale</li> <li>Install Kubernetes:<ul> <li>container-runtimes</li> <li>install-kubeadm</li> <li>crictl</li> <li>crictl usage</li> </ul> </li> </ul> <p>If you follow these links you should be able to install everything without Ansible.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#tailscale-vpn","title":"Tailscale VPN","text":"<p>I assume that Tailscale is successfully  installed on every node. Before you begin please register a free account: Tailscale</p> <p>Info</p> <p>I recommend you to enable \"MagicDNS\" on the Tailscale web interface. </p> <p>Run the following command on all 4 nodes (kube0-m[1,2,3] and kube02-haproxy):</p> <pre><code>tailscale up --accept-dns=true\n</code></pre> <p>Check if all your nodes have VPN IP address:</p> <pre><code>root@kube02-m3:~# tailscale status | grep kube02\n100.124.70.97   kube02-m3            jvincze84@   linux   -\n100.121.89.125  kube02-haproxy       jvincze84@   linux   -\n100.122.123.2   kube02-m1            jvincze84@   linux   -\n100.103.128.9   kube02-m2            jvincze84@   linux   -\n</code></pre> <p>Try ping:</p> <pre><code>root@kube02-m3:~# ping kube02-m1\nPING kube02-m1.tailnet-a5cd.ts.net (100.122.123.2) 56(84) bytes of data.\n64 bytes from kube02-m1.tailnet-a5cd.ts.net (100.122.123.2): icmp_seq=1 ttl=64 time=1.33 ms\n64 bytes from kube02-m1.tailnet-a5cd.ts.net (100.122.123.2): icmp_seq=2 ttl=64 time=0.976 ms\n^C\n--- kube02-m1.tailnet-a5cd.ts.net ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 0.976/1.151/1.327/0.175 ms\n\nroot@kube02-m3:~# ping kube02-m2\nPING kube02-m2.tailnet-a5cd.ts.net (100.103.128.9) 56(84) bytes of data.\n64 bytes from kube02-m2.tailnet-a5cd.ts.net (100.103.128.9): icmp_seq=1 ttl=64 time=1.49 ms\n64 bytes from kube02-m2.tailnet-a5cd.ts.net (100.103.128.9): icmp_seq=2 ttl=64 time=1.06 ms\n^C\n--- kube02-m2.tailnet-a5cd.ts.net ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1000ms\nrtt min/avg/max/mdev = 1.055/1.272/1.490/0.217 ms\n\nroot@kube02-m3:~# ping kube02-haproxy\nPING kube02-haproxy.tailnet-a5cd.ts.net (100.121.89.125) 56(84) bytes of data.\n64 bytes from kube02-haproxy.tailnet-a5cd.ts.net (100.121.89.125): icmp_seq=1 ttl=64 time=1.27 ms\n^C\n--- kube02-haproxy.tailnet-a5cd.ts.net ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 1.269/1.269/1.269/0.000 ms\n</code></pre> <p>It seems that everything is fine.</p> <p>Now check <code>tailscale status</code> command again:</p> <pre><code>root@kube02-m3:~# tailscale status | grep kube02\n100.124.70.97   kube02-m3            jvincze84@   linux   -\n100.121.89.125  kube02-haproxy       jvincze84@   linux   active; direct 172.16.1.80:41641, tx 468 rx 348\n100.122.123.2   kube02-m1            jvincze84@   linux   active; direct 172.16.1.77:41641, tx 724 rx 604\n100.103.128.9   kube02-m2            jvincze84@   linux   active; direct 172.16.1.78:41641, tx 596 rx 476\n</code></pre> <p>After you connect to a host, the status command will show extra information: <code>active; direct 172.16.1.80:41641, tx 468 rx 348</code></p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#updated-ip-address-table","title":"Updated IP Address Table","text":"Hostname Role IP Address VPN IP Address kube02-m1 Control Plane Node 1 172.16.1.77 100.122.123.2 kube02-m2 Control Plane Node 2 172.16.1.78 100.103.128.9 kube02-m3 Control Plane Node 3 172.16.1.79 100.124.70.97 kube02-haproxy HAProxy Load Balancer 172.16.1.80 100.121.89.125 ansible Ansible Host 172.16.0.252 ---"},{"location":"Blog/2023/04/21/install-kube-tailscale/#optional-disable-direct-access","title":"(Optional) Disable Direct Access","text":"<p>This step is optional. I want to simulate the situation when the nodes are not sitting in the same subnet, and can talk to each other only over the Tailscale VPN. This way maybe easier to understand what we doing with the VPN.</p> <p>I don't want to make it complicated, so simply disable the communication between node with iptables.</p> <p>kube02-m1</p> <pre><code>iptables -I INPUT -s 172.16.1.78 -j DROP\niptables -I INPUT -s 172.16.1.79 -j DROP\n</code></pre> <p>kube02-m2</p> <pre><code>iptables -I INPUT -s 172.16.1.77 -j DROP\niptables -I INPUT -s 172.16.1.79 -j DROP\n</code></pre> <p>kube02-m3</p> <pre><code>iptables -I INPUT -s 172.16.1.77 -j DROP\niptables -I INPUT -s 172.16.1.78 -j DROP\n</code></pre> <p>Note</p> <p>These rules are not permanent. So, if you restart the machine you should apply them again.</p> <p>Check the Tailscale Connection</p> <p>Don't forget the ping hosts before the <code>tailscale status</code> command.</p> <pre><code>root@kube02-m1:~# tailscale status | grep kube02-m\n100.122.123.2   kube02-m1            jvincze84@   linux   -\n100.103.128.9   kube02-m2            jvincze84@   linux   active; relay \"fra\", tx 308 rx 220\n100.124.70.97   kube02-m3            jvincze84@   linux   active; relay \"waw\", tx 308 rx 220\n</code></pre> <p>Now you can see that the hosts are connected to each other via relay servers (<code>active; relay \"fra\", tx 308 rx 22</code>) provided by Tailscale.</p> <p>To see the available relays, run the <code>tailscale netcheck</code> command.</p> <pre><code>root@kube02-m1:~# tailscale netcheck\n\nReport:\n        * UDP: true\n        * IPv4: yes, 176.*.*.107:58839\n        * IPv6: no, but OS has support\n        * MappingVariesByDestIP: false\n        * HairPinning: false\n        * PortMapping:\n        * Nearest DERP: Frankfurt\n        * DERP latency:\n                - fra: 18.4ms  (Frankfurt)\n                - waw: 19.6ms  (Warsaw)\n                - ams: 25.1ms  (Amsterdam)\n                - par: 32ms    (Paris)\n                - mad: 44.7ms  (Madrid)\n                - lhr: 59.5ms  (London)\n                - nyc: 119.4ms (New York City)\n                - tor: 119.5ms (Toronto)\n                - ord: 119.9ms (Chicago)\n                - dbi: 130.8ms (Dubai)\n                - mia: 132.8ms (Miami)\n                - den: 140.8ms (Denver)\n                - dfw: 145.8ms (Dallas)\n                - sfo: 166.8ms (San Francisco)\n                - lax: 170.1ms (Los Angeles)\n                - sin: 170.5ms (Singapore)\n                - sea: 177.8ms (Seattle)\n                - blr: 193.9ms (Bangalore)\n                - jnb: 198.7ms (Johannesburg)\n                - hkg: 209ms   (Hong Kong)\n                - sao: 214.5ms (S\u00e3o Paulo)\n                - hnl: 215.8ms (Honolulu)\n                - syd:         (Sydney)\n                - tok:         (Tokyo)\nroot@kube02-m1:~#\n</code></pre> <p>This is one of my favorite feature of Tailscale. You don't have to have stable static public IP address to use VPN service. But keep in mind, that connection over relay server can be significantly slower than direct connection. </p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#init-the-cluster","title":"Init The Cluster","text":""},{"location":"Blog/2023/04/21/install-kube-tailscale/#prepare-kubelet","title":"Prepare Kubelet","text":"<p>Before you do anything, prepare the kubelet to use Tailscale VPN IP address as node IP address. </p> <p>Run this command on all Kubernetes nodes:</p> <pre><code>echo \"KUBELET_EXTRA_ARGS=--node-ip=$(tailscale ip --4)\" | tee -a /etc/default/kubelet\n</code></pre> Example Commands: <pre><code>root@kube02-m1:~# echo \"KUBELET_EXTRA_ARGS=--node-ip=$(tailscale ip --4)\" | tee -a /etc/default/kubelet\nKUBELET_EXTRA_ARGS=--node-ip=100.122.123.2\nroot@kube02-m1:~#\n\nroot@kube02-m2:~# echo \"KUBELET_EXTRA_ARGS=--node-ip=$(tailscale ip --4)\" | tee -a /etc/default/kubelet\nKUBELET_EXTRA_ARGS=--node-ip=100.103.128.9\nroot@kube02-m2:~#\n\nroot@kube02-m3:~# echo \"KUBELET_EXTRA_ARGS=--node-ip=$(tailscale ip --4)\" | tee -a /etc/default/kubelet\nKUBELET_EXTRA_ARGS=--node-ip=100.124.70.97\nroot@kube02-m3:~#\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#prepare-the-load-balancer","title":"Prepare The Load Balancer","text":"<p>I won't want to waste a lot time for this task, since this is only a lab env with just one function: demonstrate the installation. HAProxy is a really good example about how to configure an external Load Balancer for kubernetes control plane nodes. </p> <p>Check if MagicDNS is working fine</p> <pre><code>root@kube02-haproxy:~# nslookup kube02-m1\nServer:         100.100.100.100\nAddress:        100.100.100.100#53\n\nName:   kube02-m1.tailnet-a5cd.ts.net\nAddress: 100.122.123.2\n\nroot@kube02-haproxy:~# nslookup kube02-m2\nServer:         100.100.100.100\nAddress:        100.100.100.100#53\n\nName:   kube02-m2.tailnet-a5cd.ts.net\nAddress: 100.103.128.9\n\nroot@kube02-haproxy:~# nslookup kube02-m3\nServer:         100.100.100.100\nAddress:        100.100.100.100#53\n\nName:   kube02-m3.tailnet-a5cd.ts.net\nAddress: 100.124.70.97\n</code></pre> <p>/etc/haproxy.conf config file:</p> <pre><code>frontend kubeapi\n  log global\n  bind *:6443\n  mode tcp\n  option tcplog\n  default_backend kubecontroleplain\n\nbackend kubecontroleplain\n  option httpchk GET /healthz\n  http-check expect status 200\n  mode tcp\n  log global\n  balance roundrobin\n  #option tcp-check\n  option ssl-hello-chk\n  server kube02-m1 kube02-m1.tailnet-a5cd.ts.net:6443 check\n  server kube02-m2 kube02-m2.tailnet-a5cd.ts.net:6443 check\n  server kube02-m3 kube02-m3.tailnet-a5cd.ts.net:6443 check\n\n\nfrontend stats\n    mode http\n    bind *:8404\n    stats enable\n    stats uri /stats\n    stats refresh 10s\n    stats admin if LOCALHOST\n</code></pre> <p>Warning</p> <p>As I know HAProxy resolve DNS only once at startup. So use DNS name in <code>server</code> section with caution. If the IP address has changed, do not forget to restart HAProxy.</p> <p>Run <code>HAProxy</code>:</p> <pre><code>docker run --name haproxy -d -p 6443:6443 -p 8404:8404 -v /etc/haproxy.conf:/usr/local/etc/haproxy/haproxy.cfg haproxy\n</code></pre> <p>Info</p> <p><code>*.tailnet-a5cd.ts.net</code> is my MagicDNS name.</p> <p>Tip</p> <p>If you are looking for a solution without external Load Balancer you may want to take a look at Kube-Vip</p> Quote <p>kube-vip provides Kubernetes clusters with a virtual IP and load balancer for both the control plane (for building a highly-available cluster) and Kubernetes Services of type LoadBalancer without relying on any external hardware or software.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#init-the-first-control-plane-node","title":"Init The First Control Plane Node","text":"<p>The command:</p> <pre><code>kubeadm init --cri-socket /var/run/containerd/containerd.sock \\\n--control-plane-endpoint kube02-haproxy.tailnet-a5cd.ts.net \\\n--apiserver-advertise-address $(tailscale ip --4) \\\n--pod-network-cidr 10.25.0.0/16 \\\n--service-cidr 10.26.0.0/16 \\\n--upload-certs\n</code></pre> <p>Important</p> <p>If you don't have separate HAProxy node, and you are using one kubernetes node, you should consider changing the <code>--apiserver-bind-port</code> port or the listen port of the HAProxy.</p> <p>Important</p> <p><code>pod-network-cidr</code> and <code>service-cidr</code> is required by flannel CNI. </p> <p>Important</p> <p>Do not forget the <code>--upload-certs</code> option, otherwise additional control plane nodes won't be able to join the cluster without extra steps.</p> Command Output <pre><code>root@kube02-m1:# kubeadm init --cri-socket /var/run/containerd/containerd.sock \\\n--control-plane-endpoint kube02-haproxy.tailnet-a5cd.ts.net \\\n--apiserver-advertise-address $(tailscale ip --4) \\\n--pod-network-cidr 10.25.0.0/16 \\\n--service-cidr 10.26.0.0/16 \\\n--upload-certs\nW0421 16:13:16.891232   25655 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme \"unix\" to the \"criSocket\" with value \"/var/run/containerd/containerd.sock\". Please update your configuration!\nI0421 16:13:17.241235   25655 version.go:256] remote version is much newer: v1.27.1; falling back to: stable-1.26\n[init] Using Kubernetes version: v1.26.4\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kube02-haproxy.tailnet-a5cd.ts.net kube02-m1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.26.0.1 100.122.123.2]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [kube02-m1 localhost] and IPs [100.122.123.2 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [kube02-m1 localhost] and IPs [100.122.123.2 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[kubelet-check] Initial timeout of 40s passed.\n[apiclient] All control plane components are healthy after 101.038704 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config\" in namespace kube-system with the configuration for the kubelets in the cluster\n[upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace\n[upload-certs] Using certificate key:\n2f2caa21e13d7f4bece27faa2515d024c8b4e93e08d8d21612113a7ebacff5ea\n[mark-control-plane] Marking the node kube02-m1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node kube02-m1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n[bootstrap-token] Using token: 1q32dn.swfpr7qj89hl2g4j\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running the following command on each as root:\n\n  kubeadm join kube02-haproxy.tailnet-a5cd.ts.net:6443 --token 1q32dn.swfpr7qj89hl2g4j \\\n        --discovery-token-ca-cert-hash sha256:11c669ee4e4e27b997ae5431133dd2cd7c6a2050ddd16b38bee8bee544bbe680 \\\n        --control-plane --certificate-key 2f2caa21e13d7f4bece27faa2515d024c8b4e93e08d8d21612113a7ebacff5ea\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n\"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join kube02-haproxy.tailnet-a5cd.ts.net:6443 --token 1q32dn.swfpr7qj89hl2g4j \\\n        --discovery-token-ca-cert-hash sha256:11c669ee4e4e27b997ae5431133dd2cd7c6a2050ddd16b38bee8bee544bbe680\n</code></pre> <p>Run these commands:</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>And finally check the nodes:</p> <pre><code>kubectl get nodes -o wide\nNAME        STATUS     ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION    CONTAINER-RUNTIME\nkube02-m1   NotReady   control-plane   2m45s   v1.26.4   100.122.123.2   &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.10.0-21-amd64   containerd://1.6.20\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#init-additional-control-plane-nodes","title":"Init Additional Control Plane Nodes","text":"<p>Command:</p> <pre><code>kubeadm join kube02-haproxy.tailnet-a5cd.ts.net:6443 --token 1q32dn.swfpr7qj89hl2g4j \\\n--apiserver-advertise-address $(tailscale ip --4) \\\n--cri-socket /var/run/containerd/containerd.sock \\\n--discovery-token-ca-cert-hash sha256:11c669ee4e4e27b997ae5431133dd2cd7c6a2050ddd16b38bee8bee544bbe680 \\\n--control-plane --certificate-key 2f2caa21e13d7f4bece27faa2515d024c8b4e93e08d8d21612113a7ebacff5ea\n</code></pre> <p>Important</p> <p>Important that the nodes must use their own VPN address as <code>apiserver-advertise-address</code></p> Command Output <pre><code>root@kube02-m2:# kubeadm join kube02-haproxy.tailnet-a5cd.ts.net:6443 --token 1q32dn.swfpr7qj89hl2g4j --apiserver-advertise-address $(tailscale ip --4) --cri-socket /var/run/containerd/containerd.sock --discovery-token-ca-cert-hash sha256:11c669ee4e4e27b997ae5431133dd2cd7c6a2050ddd16b38bee8bee544bbe680 --control-plane --certificate-key 2f2caa21e13d7f4bece27faa2515d024c8b4e93e08d8d21612113a7ebacff5ea\nW0421 16:23:11.602945   26931 initconfiguration.go:119] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme \"unix\" to the \"criSocket\" with value \"/var/run/containerd/containerd.sock\". Please update your configuration!\n[preflight] Running pre-flight checks\n[preflight] Reading configuration from the cluster...\n[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks before initializing the new control plane instance\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[download-certs] Downloading the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace\n[download-certs] Saving the certificates to the folder: \"/etc/kubernetes/pki\"\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kube02-haproxy.tailnet-a5cd.ts.net kube02-m2 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.26.0.1 100.103.128.9]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [kube02-m2 localhost] and IPs [100.103.128.9 127.0.0.1 ::1]\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [kube02-m2 localhost] and IPs [100.103.128.9 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[certs] Using the existing \"sa\" key\n[kubeconfig] Generating kubeconfig files\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[check-etcd] Checking that the etcd cluster is healthy\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Starting the kubelet\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n[etcd] Announced new etcd member joining to the existing etcd cluster\n[etcd] Creating static Pod manifest for \"etcd\"\n[etcd] Waiting for the new etcd member to join the cluster. This can take up to 40s\n\nThe 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation\n[mark-control-plane] Marking the node kube02-m2 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node kube02-m2 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]\n\nThis node has joined the cluster and a new control plane instance was created:\n\n* Certificate signing request was sent to apiserver and approval was received.\n* The Kubelet was informed of the new secure connection details.\n* Control plane label and taint were applied to the new node.\n* The Kubernetes control plane instances scaled up.\n* A new etcd member was added to the local/stacked etcd cluster.\n\nTo start administering your cluster from this node, you need to run the following as a regular user:\n\n        mkdir -p $HOME/.kube\n        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n        sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nRun 'kubectl get nodes' to see this node join the cluster.\n</code></pre> <p>Finally check the nodes:</p> <pre><code>root@kube02-m1:~# kubectl get nodes -o wide\nNAME        STATUS     ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION    CONTAINER-RUNTIME\nkube02-m1   NotReady   control-plane   10m   v1.26.4   100.122.123.2   &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.10.0-21-amd64   containerd://1.6.20\nkube02-m2   NotReady   control-plane   98s   v1.26.4   100.103.128.9   &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.10.0-21-amd64   containerd://1.6.20\nkube02-m3   NotReady   control-plane   16s   v1.26.4   100.124.70.97   &lt;none&gt;        Debian GNU/Linux 11 (bullseye)   5.10.0-21-amd64   containerd://1.6.20\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#optional-post-init-steps","title":"(optional) Post Init Steps","text":"<p>These steps are useful when you won't have any worker nodes, just control-planes.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#mark-nodes-as-worker","title":"Mark Nodes As Worker","text":"<pre><code>kubectl label node kube02-m1 node-role.kubernetes.io/worker=\nkubectl label node kube02-m2 node-role.kubernetes.io/worker=\nkubectl label node kube02-m3 node-role.kubernetes.io/worker=\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#untaint-the-nodes","title":"Untaint The Nodes","text":"<pre><code>kubectl taint nodes kube02-m1 node-role.kubernetes.io/control-plane=:NoSchedule-\nkubectl taint nodes kube02-m2 node-role.kubernetes.io/control-plane=:NoSchedule-\nkubectl taint nodes kube02-m3 node-role.kubernetes.io/control-plane=:NoSchedule-\n</code></pre> <p>Our nodes are in <code>NotReady</code> state, because no network plugin is installed.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#install-network-plugin","title":"Install Network Plugin","text":"<p>In this post I will show two network plugin: flannel and weave.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#weave","title":"Weave","text":"<p>Download The Manifest</p> <pre><code>wget https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml\n</code></pre> <p>You should consider to change iptables mode:</p> <p>IPTABLES_BACKEND - set to nft to use nftables backend for iptables (default is iptables)</p> <p>In my case I use nft, so I have to add <code>IPTABLES_BACKEND</code> environment variable and set to <code>nft</code></p> <pre><code>          containers:\n            - name: weave\n              command:\n                - /home/weave/launch.sh\n              env:\n                - name: IPTABLES_BACKEND\n                  value: nft\n</code></pre> <p>Apply the manifest</p> <pre><code>kubectl  apply -f weave-daemonset-k8s.yaml\n</code></pre> <p>Check the status of the Weave PODs:</p> <pre><code>kubectl -n kube-system get pods -l name=weave-net -o wide\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME              READY   STATUS    RESTARTS      AGE   IP              NODE        NOMINATED NODE   READINESS GATES\nweave-net-drz44   2/2     Running   1 (59s ago)   68s   100.103.128.9   kube02-m2   &lt;none&gt;           &lt;none&gt;\nweave-net-p72nl   2/2     Running   1 (59s ago)   68s   100.124.70.97   kube02-m3   &lt;none&gt;           &lt;none&gt;\nweave-net-zzj9p   2/2     Running   1 (59s ago)   68s   100.122.123.2   kube02-m1   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>If everything went good, you should see that the coredns pods are running:</p> <pre><code>root@kube02-m1:~# kubectl -n kube-system get pods -l k8s-app=kube-dns -o wide\nNAME                       READY   STATUS    RESTARTS   AGE   IP          NODE        NOMINATED NODE   READINESS GATES\ncoredns-787d4945fb-2d5zm   1/1     Running   0          21m   10.40.0.0   kube02-m1   &lt;none&gt;           &lt;none&gt;\ncoredns-787d4945fb-gmtbr   1/1     Running   0          21m   10.40.0.1   kube02-m1   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#flannel","title":"Flannel","text":"<p>Download the manifest:</p> <pre><code>wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n</code></pre> <p>Modify the manifest to use the tailscale0 interface (<code>iface=tailscale0</code>) and our network CIDR:</p> <pre><code>--- kube-flannel-orig.yml       2023-03-22 15:48:52.000000000 +0100\n+++ kube-flannel.yml    2023-04-25 08:27:05.183037370 +0200\n@@ -81,21 +81,21 @@\n         {\n           \"type\": \"portmap\",\n           \"capabilities\": {\n             \"portMappings\": true\n           }\n         }\n       ]\n     }\n   net-conf.json: |\n     {\n-      \"Network\": \"10.244.0.0/16\",\n+      \"Network\": \"10.25.0.0/16\",\n       \"Backend\": {\n         \"Type\": \"vxlan\"\n       }\n     }\n kind: ConfigMap\n metadata:\n   labels:\n     app: flannel\n     k8s-app: flannel\n     tier: node\n@@ -129,20 +129,21 @@\n             nodeSelectorTerms:\n             - matchExpressions:\n               - key: kubernetes.io/os\n                 operator: In\n                 values:\n                 - linux\n       containers:\n       - args:\n         - --ip-masq\n         - --kube-subnet-mgr\n+        - --iface=tailscale0\n         command:\n         - /opt/bin/flanneld\n         env:\n         - name: POD_NAME\n           valueFrom:\n             fieldRef:\n               fieldPath: metadata.name\n         - name: POD_NAMESPACE\n           valueFrom:\n             fieldRef:\nroot@kube02-m1:~#\n</code></pre> <p>Apply the manifest:</p> <pre><code>kubectl apply -f kube-flannel.yml\n</code></pre> Example flannel-ds logs <code>kubectl -n kube-flannel logs kube-flannel-ds-h2rp2</code> <pre><code>Defaulted container \"kube-flannel\" out of: kube-flannel, install-cni-plugin (init), install-cni (init)\nI0425 06:34:24.144883       1 main.go:211] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[tailscale0] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true useMultiClusterCidr:false}\nW0425 06:34:24.146218       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\nI0425 06:34:24.207098       1 kube.go:144] Waiting 10m0s for node controller to sync\nI0425 06:34:24.207158       1 kube.go:485] Starting kube subnet manager\nI0425 06:34:24.220083       1 kube.go:506] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.25.0.0/24]\nI0425 06:34:24.220116       1 kube.go:506] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.25.1.0/24]\nI0425 06:34:24.220245       1 kube.go:506] Creating the node lease for IPv4. This is the n.Spec.PodCIDRs: [10.25.2.0/24]\nI0425 06:34:25.207246       1 kube.go:151] Node controller sync successful\nI0425 06:34:25.207266       1 main.go:231] Created subnet manager: Kubernetes Subnet Manager - kube02-m1\nI0425 06:34:25.207272       1 main.go:234] Installing signal handlers\nI0425 06:34:25.207991       1 main.go:542] Found network config - Backend type: vxlan\nI0425 06:34:25.208718       1 match.go:259] Using interface with name tailscale0 and address 100.122.123.2\nI0425 06:34:25.208762       1 match.go:281] Defaulting external address to interface address (100.122.123.2)\nI0425 06:34:25.208917       1 vxlan.go:140] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false\nI0425 06:34:25.210776       1 main.go:356] Setting up masking rules\nI0425 06:34:25.252429       1 main.go:407] Changing default FORWARD chain policy to ACCEPT\nI0425 06:34:25.253690       1 iptables.go:290] generated 7 rules\nI0425 06:34:25.256468       1 main.go:435] Wrote subnet file to /run/flannel/subnet.env\nI0425 06:34:25.256489       1 main.go:439] Running backend.\nI0425 06:34:25.256646       1 iptables.go:290] generated 3 rules\nI0425 06:34:25.257126       1 vxlan_network.go:64] watching for new subnet leases\nI0425 06:34:25.258108       1 watch.go:51] Batch elem [0] is { subnet.Event{Type:0, Lease:subnet.Lease{EnableIPv4:true, EnableIPv6:false, Subnet:ip.IP4Net{IP:0xa190100, PrefixLen:0x18}, IPv6Subnet:ip.IP6Net{IP:(*ip.IP6)(nil), PrefixLen:0x0}, Attrs:subnet.LeaseAttrs{PublicIP:0x64678009, PublicIPv6:(*ip.IP6)(nil), BackendType:\"vxlan\", BackendData:json.RawMessage{0x7b, 0x22, 0x56, 0x4e, 0x49, 0x22, 0x3a, 0x31, 0x2c, 0x22, 0x56, 0x74, 0x65, 0x70, 0x4d, 0x41, 0x43, 0x22, 0x3a, 0x22, 0x35, 0x32, 0x3a, 0x39, 0x65, 0x3a, 0x32, 0x62, 0x3a, 0x63, 0x33, 0x3a, 0x38, 0x34, 0x3a, 0x63, 0x37, 0x22, 0x7d}, BackendV6Data:json.RawMessage(nil)}, Expiration:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Asof:0}} }\nI0425 06:34:25.258185       1 watch.go:51] Batch elem [0] is { subnet.Event{Type:0, Lease:subnet.Lease{EnableIPv4:true, EnableIPv6:false, Subnet:ip.IP4Net{IP:0xa190200, PrefixLen:0x18}, IPv6Subnet:ip.IP6Net{IP:(*ip.IP6)(nil), PrefixLen:0x0}, Attrs:subnet.LeaseAttrs{PublicIP:0x647c4661, PublicIPv6:(*ip.IP6)(nil), BackendType:\"vxlan\", BackendData:json.RawMessage{0x7b, 0x22, 0x56, 0x4e, 0x49, 0x22, 0x3a, 0x31, 0x2c, 0x22, 0x56, 0x74, 0x65, 0x70, 0x4d, 0x41, 0x43, 0x22, 0x3a, 0x22, 0x33, 0x61, 0x3a, 0x38, 0x31, 0x3a, 0x36, 0x30, 0x3a, 0x63, 0x64, 0x3a, 0x66, 0x63, 0x3a, 0x62, 0x63, 0x22, 0x7d}, BackendV6Data:json.RawMessage(nil)}, Expiration:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Asof:0}} }\nI0425 06:34:25.290612       1 iptables.go:283] bootstrap done\nI0425 06:34:25.311445       1 iptables.go:283] bootstrap done\nI0425 06:34:25.322841       1 main.go:460] Waiting for all goroutines to exit\n</code></pre> <p>Warning</p> <p>Choose only one CNI plugin, do not install both flannel and weave. If you want to replace weave you should remove it: </p> <ul> <li><code>kubectl delete -f weave-daemonset-k8s.yaml</code></li> <li><code>rm /etc/cni/net.d/10-weave.conflist</code></li> <li>Additionally rebooting the nodes may be necessary.</li> <li>After you applied the kube manifest run <code>kubectl delete pods --all -A</code> command. This will recreate all pods in the entire cluster!!!</li> </ul> <p>I really recommend you to choose CNI network plugin carefully before you getting use your cluster, and avoid changing CNI in the future.</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#switch-between-iptables-legacy-and-iptables-nft","title":"Switch Between iptables-legacy And iptables-nft","text":"<p>If you want or need to change iptables to or from iptables-legacy please check this link: https://wiki.debian.org/iptables</p> <p>Example, how to change to legacy:</p> <pre><code>apt-get install arptables\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy\nupdate-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nupdate-alternatives --set arptables /usr/sbin/arptables-legacy\nupdate-alternatives --set ebtables /usr/sbin/ebtables-legacy\n</code></pre>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#bonus-persistent-storage","title":"(bonus) - Persistent Storage","text":"<p>Almost all Kubernetes Cluster have some kind of PersistentVolume solution for storing data. Now we will deploy Longhorn</p> <p>It is just a simple command:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.1/deploy/longhorn.yaml\n</code></pre> <p>Info</p> <p>If you want to use RWX volumes NFSv4 client must be installed on all Kubernetes nodes. LINK</p>"},{"location":"Blog/2023/04/21/install-kube-tailscale/#create-pvc","title":"Create PVC","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: test-pvc-02\nspec:\n  storageClassName: longhorn #(1)\n  accessModes:\n    - ReadWriteMany # (2)\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n</code></pre> <ol> <li>This is the default Storage Class</li> <li>Use one of: <code>ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see AccessModes</code></li> </ol> <p>Create Pod To Consume The Storage</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: hello-pvc\n  namespace: default\nspec:\n  volumes:\n  - name: storage\n    persistentVolumeClaim:\n      claimName: test-pvc-02\n  containers:\n  - name: hello-container\n    image: busybox\n    command:\n       - sh\n       - -c\n       - 'while true; do echo \"`date` [`hostname`] Hello from Longhorn PV.\" &gt;&gt; /mnt/store/greet.txt; sleep $(($RANDOM % 5 + 300)); done'\n    volumeMounts:\n    - mountPath: /mnt/store\n      name: storage\nEOF\n</code></pre>"},{"location":"Blog/2023/04/28/postgres_upgrade/","title":"Upgrade PostgresSQL On Kubernetes With Job","text":"<p>Danger</p> <p>UNFINISHED POST!!!!</p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#preface","title":"Preface","text":"<p>Initially I wrote this script &amp; job to myself, but I found that it can be useful for others as well. This article is not a kind of \"use as is\", and need some deeper knowledge to adapt it to your environment. So use my post as a skeleton, and modify according to your needs.</p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#starting-point-requirements","title":"Starting Point &amp; Requirements","text":"<p>In order to understand what I'm doing you need to know some detailed information about my environment.</p> <p>Kubernetes &amp; OS:</p> <ul> <li>Kuberenets version: <code>v1.25.6</code></li> <li>OS: <code>Debian GNU/Linux 11 (bullseye)</code></li> <li>Kernel: <code>5.10.0-21-amd64</code></li> <li>Container Engine: <code>containerd://1.6.6</code></li> <li>Persistent volume provider: <code>Longhorn v1.4.1</code></li> <li>Old Postgres Version (docker image): <code>postgres:12</code></li> <li>New Postgres Version (docker image): <code>postgres:14</code></li> </ul>"},{"location":"Blog/2023/04/28/postgres_upgrade/#requirements","title":"Requirements","text":"<ul> <li>Running both old and new Postgres Deployment<ul> <li>I highly recommend to create new PVC for the new cluster.</li> </ul> </li> <li>Temporary PVC for the dump<ul> <li>You can use ephemeral (emptydir) volume for the dump, but you will lose your dump file after the Job finished. </li> </ul> </li> </ul>"},{"location":"Blog/2023/04/28/postgres_upgrade/#current-state-new-postgres","title":"Current State &amp; New Postgres","text":"<p>So I have a relatively old Postgres 12 server. </p> Deployment OLD <pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: postgres\n  template:\n    metadata:\n      name: postgres\n      creationTimestamp: null\n      labels:\n        k8s-app: postgres\n    spec:\n      volumes:\n        - name: pgdata\n          persistentVolumeClaim:\n            claimName: pgdata\n      containers:\n        - name: postgres\n          image: postgres:12\n          env:\n            - name: TZ\n              value: Europe/Budapest\n            - name: LC_ALL\n              value: C.UTF-8\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgres\n                  key: admin-pass\n            - name: POSTGRES_USER\n              value: admin\n            - name: PGDATA\n              value: /var/lib/postgresql/data/pgdata\n          resources: {}\n          volumeMounts:\n            - name: pgdata\n              mountPath: /var/lib/postgresql/data\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        location: vps\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <p>As you can this is really simple Postgres deployment.  My new Postgres Deployment is very similar to this:</p> Deployment NEW <pre><code>kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: postgres-v14\n  namespace: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: postgres-v14\n  template:\n    metadata:\n      name: postgres-v14\n      creationTimestamp: null\n      labels:\n        k8s-app: postgres-v14\n    spec:\n      volumes:\n        - name: pgdata-v14\n          persistentVolumeClaim:\n            claimName: pgdata-v14\n      containers:\n        - name: postgres-v14\n          image: postgres:14\n          env:\n            - name: TZ\n              value: Europe/Budapest\n            - name: LC_ALL\n              value: C.UTF-8\n            - name: POSTGRES_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgres\n                  key: admin-pass\n            - name: POSTGRES_USER\n              value: pg-14-admin\n            - name: PGDATA\n              value: /var/lib/postgresql/data/pgdata\n          resources: {}\n          volumeMounts:\n            - name: pgdata-v14\n              mountPath: /var/lib/postgresql/data\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            privileged: false\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 30\n      dnsPolicy: ClusterFirst\n      nodeSelector:\n        location: vps\n      securityContext: {}\n      schedulerName: default-scheduler\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 10\n  progressDeadlineSeconds: 600\n</code></pre> <p>For better understanding here are the differences:</p> <pre><code>--- old  2023-04-27 13:31:24.633567033 +0200\n+++ new  2023-04-27 13:32:21.653176863 +0200\n@@ -1,29 +1,29 @@\n     kind: Deployment\n     apiVersion: apps/v1\n     metadata:\n-      name: postgres\n+      name: postgres-v14\n       namespace: postgres\n     spec:\n       replicas: 1\n       selector:\n         matchLabels:\n-          k8s-app: postgres\n+          k8s-app: postgres-v14\n       template:\n         metadata:\n-          name: postgres\n+          name: postgres-v14\n           creationTimestamp: null\n           labels:\n-            k8s-app: postgres\n+            k8s-app: postgres-v14\n         spec:\n           volumes:\n-            - name: pgdata\n+            - name: pgdata-v14\n               persistentVolumeClaim:\n-                claimName: pgdata\n+                claimName: pgdata-v14\n           containers:\n-            - name: postgres\n-              image: postgres:12\n+            - name: postgres-v14\n+              image: postgres:14\n               env:\n                 - name: TZ\n                   value: Europe/Budapest\n                 - name: LC_ALL\n                   value: C.UTF-8\n@@ -31,16 +31,16 @@\n                   valueFrom:\n                     secretKeyRef:\n                       name: postgres\n                       key: admin-pass\n                 - name: POSTGRES_USER\n-                  value: admin\n+                  value: pg-14-admin\n                 - name: PGDATA\n                   value: /var/lib/postgresql/data/pgdata\n               resources: {}\n               volumeMounts:\n-                - name: pgdata\n+                - name: pgdata-v14\n                   mountPath: /var/lib/postgresql/data\n               terminationMessagePath: /dev/termination-log\n               terminationMessagePolicy: File\n               imagePullPolicy: IfNotPresent\n               securityContext:\n</code></pre> <p>Danger</p> <p>Do not use the same <code>POSTGRES_USER</code> for the old and new cluster. In my case the old postgres used <code>md5</code> auth, but the new <code>SCRAM-SHA-256</code>.  In this situation you will overwrite the <code>admin</code> user's password, and won't be able to load the database, and login. Later in this post I'm going to write some details about this problem.</p> <p>Kubernetes <code>Services</code> (namespace: postgres)</p> <pre><code>NAME           TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE    SELECTOR\npgadmin        ClusterIP      10.25.29.142    &lt;none&gt;         80/TCP           247d   k8s-app=pgadmin\npostgres       LoadBalancer   10.25.155.130   172.16.2.108   5432:32418/TCP   247d   k8s-app=postgres\npostgres-v14   ClusterIP      10.25.109.68    &lt;none&gt;         5432/TCP         11d    k8s-app=postgres-v14\n</code></pre> <p>So I have two services: one for the old and one for the new Postgres:</p> Postgres Version IP Address Namespace Service Cluster DNS FQDN postgres:12 10.25.155.130 postgres postgres svc.cluster.local postgres.postgres.svc.cluster.local postgres:14 10.25.109.68 postgres postgres-v14 svc.cluster.local postgres-v14.postgres.svc.cluster.local"},{"location":"Blog/2023/04/28/postgres_upgrade/#upgrade-process","title":"Upgrade Process","text":""},{"location":"Blog/2023/04/28/postgres_upgrade/#shell-script","title":"Shell Script","text":"<p>First of all we need to prepare the shell script.</p> <pre><code>#!/usr/bin/env bash\n\nif [ \"$1\" == \"dump\" ]\nthen\n  echo \"Check variables..\"\n  [[ -z \"$PGDUMP_PATH\" ]] &amp;&amp; { echo \"Missing PGDUMP_PATH\" ; exit 1 ; }\n  echo \"PGDUMP_PATH is OK\"\n  [[ -z \"$PG_USER\" ]] &amp;&amp; { echo \"Missing PG_USER\" ; exit 1; }\n  echo \"PG_USER is OK\"\n  [[ -z \"$PG_PASS\" ]] &amp;&amp; { echo \"Missing PG_PASS\" ;exit 1 ; }\n  echo \"PG_PASS is OK\"\n  [[ -z \"$PG_HOST\" ]] &amp;&amp; { echo \"Missing PG_HOST\" ; exit 1 ; }\n  echo \"PG_HOST is OK\"\n  echo \"Prepare .pgpass file\"\n  echo \"${PG_HOST}:5432:*:${PG_USER}:${PG_PASS}\" &gt;$HOME/.pgpass\n  chmod 600 $HOME/.pgpass\n\n  # Check if pg_dumpall.sql file is already exists or not. If yes skip dump.\n  [[ -f $PGDUMP_PATH/pg_dumpall.sql ]] &amp;&amp; { echo \"$PGDUMP_PATH/pg_dumpall.sql is already exists. Exiting\" ; exit 0 ; }\n  echo \"Dumping Old database to $PGDUMP_PATH/pg_dumpall.sql\"\n\n  /usr/bin/pg_dumpall -w -h ${PG_HOST} -U ${PG_USER} &gt;$PGDUMP_PATH/pg_dumpall.sql\n  echo \"Printing result\"\n  ls -lah $PGDUMP_PATH\nfi\n\nif [ \"$1\" == \"load\" ]   \nthen\n  echo \"Checking Variables\"\n  [[ -z \"$NEW_PG_HOST\" ]] &amp;&amp; { echo \"Missing NEW_PG_HOST\" ; exit 1 ; }\n  echo \"NEW_PG_HOST is OK\"\n  NEW_PG_USER=\"${NEW_PG_USER:-${PG_USER}}\"\n  NEW_PG_PASS=\"${NEW_PG_PASS:-${PG_PASS}}\"  \n  echo \"Prepare .pgapss file\"\n  echo \"${NEW_PG_HOST}:5432:*:${NEW_PG_USER}:${NEW_PG_PASS}\" &gt;$HOME/.pgpass\n  chmod 600 $HOME/.pgpass\n\n  echo \"Loading Data Into The New Database\"\n  psql -h ${NEW_PG_HOST} -U ${NEW_PG_USER} &lt;$PGDUMP_PATH/pg_dumpall.sql\nfi\n</code></pre> <p>The script has two modes depend on the first argument:</p> <ul> <li><code>dump</code><ul> <li>Dump the old database. </li> </ul> </li> <li><code>load</code><ul> <li>Load the previously created dump to the new database.</li> </ul> </li> </ul>"},{"location":"Blog/2023/04/28/postgres_upgrade/#variables","title":"Variables","text":"<code>PGDUMP_PATH</code> <p>Mandatory varible.</p> <ul> <li>Used by: dump &amp; load</li> <li>Where the dump file will be saved to, and load from.</li> <li>Must match in dump and load container.</li> </ul> <code>PG_USER | PG_PASS | PG_HOST</code> <p>Mandatory varibles.</p> <ul> <li>Used by: dump</li> <li>Related to the old database.</li> <li>You may want to use Kubernetes <code>secret</code> instead of plain text here.</li> </ul> <code>NEW_PG_HOST</code> <p>Mandatory varible.</p> <ul> <li>Used by: load</li> <li>Releted to the new database</li> </ul> <code>NEW_PG_USER | NEW_PG_PASS</code> <p>Optional But Recommended variables.</p> <ul> <li>Used by: load</li> <li>Related to the new database</li> <li>If you skip to define these variables the script will use the <code>PG_USER | PG_PASS</code>. Despite I don't recommend to use the same user, you can do it if you know what you are doing.<ul> <li><code>NEW_PG_USER=\"${NEW_PG_USER:-${PG_USER}}\"</code></li> <li><code>NEW_PG_PASS=\"${NEW_PG_PASS:-${PG_PASS}}\"</code></li> </ul> </li> </ul> <p>Note About <code>PGDUMP_PATH</code> Usage</p> <p>As I mentioned earlier <code>PGDUMP_PATH</code> can be either a Persistent Volume or Ephemeral mount. If your Postgres instance consumes only a few hundreds of megabytes ephemeral volume can be suitable for you. But keep in mind that the contetns of the ephemeral volume will be destroyed after the job finished, regardless of success or failure. I think createing a new Persistent Volume for the migration process is better choice. This way you can keep your dump file. You can see that the scirpt always create the <code>pg_dumpall.sql</code> file, and check if it is existst, and if yes, skip the dump process. This behavior can be useful when the dump is successfull but you need to rerun the load process, for some reason. Of course feel free to modify the script according to your needs.</p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#create-configmap","title":"Create ConfigMap","text":"<p>You will see in the next section that the Job using Configmap as source of the script. So first we need to create this CM.</p> <p>Save the script to <code>/tmp/pg_dump.sh</code>, and run:</p> <pre><code>kubectl -n postgres create cm pg-dump-script --from-file=pg_dump.sh=/tmp/pg_dump.sh\n</code></pre> <p>Note</p> <p>You can use other name for the cm and script, but if you do this, don't forget modify the Job as well.</p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#kubernetes-job","title":"Kubernetes Job","text":"<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: pg-update\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      backoffLimit: 0\n      initContainers:\n        - name: prepare-script\n          image: busybox\n          command:\n            - /bin/sh\n            - -c\n          args: \n            - |\n              echo \"Copy pg_dump.sh From CM to Ephemeral Storage And Change Permission\"\n              cp /tmp/pg_dump.sh /ephemeral/pg_dump.sh\n              chmod +x /ephemeral/pg_dump.sh\n              echo \"Done\"\n          volumeMounts:\n            - name: script-cm\n              mountPath: /tmp/pg_dump.sh\n              subPath: pg_dump.sh\n            - name: ephemeral\n              mountPath: /ephemeral\n        - name: pgdump\n          image: postgres:12\n          command: [\"/ephemeral/pg_dump.sh\"]\n          args:\n            - dump\n          env:\n            - name: TZ\n              value: Europe/Budapest\n            - name: PGDUMP_PATH\n              value: /pg-migrate\n            - name: PG_USER\n              value: admin\n            - name: PG_PASS\n              value: siXddddddMuhWS0PSXfa\n            - name: PG_HOST\n              value: postgres.postgres.svc.cluster.local\n          volumeMounts:\n            - name: ephemeral\n              mountPath: /ephemeral\n            - name: pg-migrate\n              mountPath: /pg-migrate\n      containers:\n        - name: new-db\n          image: postgres:14\n          volumeMounts:\n            - name: ephemeral\n              mountPath: /ephemeral\n            - name: pg-migrate\n              mountPath: /pg-migrate              \n          command: [\"/ephemeral/pg_dump.sh\"]\n          args:\n            - load\n          env:\n            - name: TZ\n              value: Europe/Budapest\n            - name: PGDUMP_PATH\n              value: /pg-migrate\n            - name: PG_PASS\n              valueFrom:\n                secretKeyRef:\n                  name: postgres\n                  key: admin-pass\n            - name: NEW_PG_USER\n              value: pg-14-admin\n            - name: NEW_PG_HOST\n              value: postgres-v14.postgres.svc.cluster.local\n      volumes:\n        - name: ephemeral\n          emptyDir: {}   \n        - name: script-cm\n          configMap:\n            name: pg-dump-script\n            items:\n              - key: pg_dump.sh\n                path: pg_dump.sh\n        - name: pg-migrate\n          persistentVolumeClaim:\n            claimName: pgdata-migrate\n</code></pre> <p>READ IT CAREFULLY</p> <p>ALWAYS DOUBLE CHECK <code>PG_HOST</code> AND <code>NEW_PG_HOST</code> VARIABLES!!! MAYBE THIS IS THE ONLY MISTAKE WITH WHICH YOU CAN DESTROY YOUR LIVE DATABASE! To avoid overwriting the database you may want to create and use a readonly user.</p> <p>Tip</p> <p>If the import process is failed for some reason, the easiest way to start over, is \"reset\" the new database: * Scale the deployment to 0 * Delete the PVC * Recreate the PVC * Scale the deployment to 1</p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#containers","title":"Containers","text":"<ul> <li><code>prepare-script</code> initContainer</li> </ul> <p>The purpose of this container to prepare the shell script for running: copy to ephemeral volume and set executable permission. This step is not necessary if you mount the CM with <code>defaultMode 0555</code> option. </p> <ul> <li><code>pgdump</code> initContainer</li> </ul> <p>You have to modify the <code>image</code> to match your old postgres version. This will run the script with \"dump\" parameter:</p> <pre><code>          command: [\"/ephemeral/pg_dump.sh\"]\n          args:\n            - dump\n</code></pre> <ul> <li><code>new-db</code> container</li> </ul> <p>You have to modify the <code>image</code> to match your new postgres version. And will run the script with \"load\" parameter:</p> <pre><code>          command: [\"/ephemeral/pg_dump.sh\"]\n          args:\n            - load\n</code></pre>"},{"location":"Blog/2023/04/28/postgres_upgrade/#volumes","title":"Volumes","text":"<ul> <li>name: ephemeral</li> </ul> <p>Out of the box my soultion use this volume only for store the bash script, but you can use it to store the dump file as well.</p> <ul> <li>name: script-cm</li> </ul> <p>The Configmap itself which contains the bash script.</p> <ul> <li>name: pg-migrate</li> </ul> <p>I use this PersistentVolume to store the dump file. </p> <p>Info</p> <p>You can vary the ephemeral and pvc, but you need at least one of them. </p> <p>I think the Job is pretty straighforward, but you need to be extra careful to match all paths and volumes across the containers. </p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#why-you-should-not-use-the-same-username","title":"Why You Should Not Use The Same Username?","text":"<p>First of all, you must know your environment.  In my case the old Postgres is relatively old (v12), and uses md5 hashing to store the password.</p> <p>Get into the old database, and check config:</p> <pre><code>root@host:~# kubectl -n postgres exec -it postgres-795d9f59db-gl8rg -- /bin/bash\nroot@postgres-795d9f59db-gl8rg:/# cd /var/lib/postgresql/data/pgdata/\nroot@postgres-795d9f59db-gl8rg:/var/lib/postgresql/data/pgdata# cat pg_hba.conf  | grep -v \\# | grep -v ^$\nlocal   all             all                                     trust\nhost    all             all             127.0.0.1/32            trust\nhost    all             all             ::1/128                 trust\nlocal   replication     all                                     trust\nhost    replication     all             127.0.0.1/32            trust\nhost    replication     all             ::1/128                 trust\nhost all all all md5\n</code></pre> <p>Do the same on the database:</p> <pre><code>root@vps11:~# kubectl -n postgres exec -it postgres-v14-6659c6f47c-4jpzs -- /bin/bash\nroot@postgres-v14-6659c6f47c-4jpzs:/# cd /var/lib/postgresql/data/pgdata/\nroot@postgres-v14-6659c6f47c-4jpzs:/var/lib/postgresql/data/pgdata# cat pg_hba.conf  | grep -v \\# | grep -v ^$\nlocal   all             all                                     trust\nhost    all             all             127.0.0.1/32            trust\nhost    all             all             ::1/128                 trust\nlocal   replication     all                                     trust\nhost    replication     all             127.0.0.1/32            trust\nhost    replication     all             ::1/128                 trust\nhost all all all scram-sha-256\n</code></pre> <p>Note the difference in the last line! If you are in the same situation I pretty sure your import process will fail, thats why you should you different admin usernames.</p> <p>Unfortunately I faild to find a soultion for in-place convert passwords from md5 to scram-sha-256. As I see you have two options:</p> <ul> <li>Modify the <code>pg_hba.conf</code> to accept old <code>md5</code> passwords (at least for already existing users.) </li> <li>Alter all user's password. Example:<ul> <li><code>ALTER USER admin WITH PASSWORD 'sdfgdfgdfgd';</code></li> <li>I think this is the preffered way, because this will be an issue on every further updates, and because of security reasons.</li> </ul> </li> </ul> <p>Example of changeing password:</p> <pre><code>root@host:~# kubectl -n postgres exec -it postgres-v14-6659c6f47c-4jpzs -- /bin/bash\nroot@postgres-v14-6659c6f47c-4jpzs:/# psql -U admin\npsql (14.7 (Debian 14.7-1.pgdg110+1))\nType \"help\" for help.\n\nadmin=# ALTER USER admin WITH PASSWORD 'siXtEKffdduhWS0PSXfa';\nALTER ROLE\nadmin=#\n\\q\nroot@postgres-v14-6659c6f47c-4jpzs:/#\nexit\n</code></pre>"},{"location":"Blog/2023/04/28/postgres_upgrade/#after-the-upgrade","title":"After The Upgrade","text":"<p>You can configure all your apps to use the new database host, or modify the selector of the old <code>Service</code>. </p>"},{"location":"Blog/2023/04/28/postgres_upgrade/#conclusion","title":"Conclusion","text":"<p>I don't think this is the best solution ever, but worked in my case.  Although this method should not hurt the old database, please use the <code>job</code> with extra care. </p>"},{"location":"Kubernetes_Templates/ConfigMap/","title":"ConfigMap","text":""},{"location":"Kubernetes_Templates/ConfigMap/#create-config-map","title":"Create Config Map","text":"<p>First create an example property file:</p> <pre><code>cat &lt;&lt;EOF&gt;/tmp/cm.txt\nMYSQL_HOST=mysql.default.cluster.svc\nMYSQL_PORT=3306\nWEB_ADMIN=false\nEXT_URL=https://external.web.local\nEOF\n</code></pre> <pre><code>kubectl -n default create configmap app-props --from-env-file=/tmp/cm.txt\n</code></pre> <p>How is it look like?</p> <pre><code>apiVersion: v1\ndata:\n  EXT_URL: https://external.web.local\n  MYSQL_HOST: mysql.default.cluster.svc\n  MYSQL_PORT: \"3306\"\n  WEB_ADMIN: \"false\"\nkind: ConfigMap\nmetadata:\n  creationTimestamp: \"2022-03-29T11:59:18Z\"\n  name: app-props\n  namespace: default\n  resourceVersion: \"130190991\"\n  uid: 4170dcea-dc38-43de-aaaa-28b1fd958ed9\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#use-as-system-environments","title":"Use As System Environments","text":""},{"location":"Kubernetes_Templates/ConfigMap/#entire-configmap","title":"Entire Configmap","text":""},{"location":"Kubernetes_Templates/ConfigMap/#patch-file","title":"Patch File","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      containers:\n        - name: debian-example\n          envFrom:\n            - configMapRef:\n                name: app-props\nEOF\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#apply","title":"Apply","text":"<pre><code>kubectl -n default patch deployment debian-test --patch-file /tmp/patch.yaml\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#specify-single-env","title":"Specify Single Env","text":"<p>You can use only the specific key, value pair.</p>"},{"location":"Kubernetes_Templates/ConfigMap/#patch-file_1","title":"Patch File","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      containers:\n        - name: debian-example\n          env:\n            - name: EXT_URL_SINGLE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-props\n                  key: EXT_URL\nEOF\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#apply_1","title":"Apply","text":"<pre><code>kubectl -n default patch deployment debian-test --patch-file /tmp/patch.yaml\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#use-case-apache-config","title":"Use Case: Apache Config","text":""},{"location":"Kubernetes_Templates/ConfigMap/#get-the-necessary-file","title":"Get The Necessary File","text":"<pre><code>docker run --rm httpd:latest cat /usr/local/apache2/conf/httpd.conf &gt;httpd-custom.conf\ndocker run --rm httpd:latest cat /usr/local/apache2/conf/mime.types &gt;mime-custom.types\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#create-configmap-from-these-files","title":"Create Configmap From These Files","text":"<pre><code>kubectl -n default create configmap apache-custom \\\n--from-file=httpd-custom.conf \\\n--from-file=mime-custom.types\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#attach-these-files-to-the-container","title":"Attach These Files To The Container","text":""},{"location":"Kubernetes_Templates/ConfigMap/#patch-file_2","title":"Patch File","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      volumes:\n        - name: apache-custom-config\n          configMap:\n            name: apache-custom\n            items:\n              - key: httpd-custom.conf\n                path: httpd.conf\n              - key: mime-custom.types\n                path: mime.types\n            defaultMode: 420        \n      containers:\n        - name: httpd\n          volumeMounts:\n            - name: apache-custom-config\n              mountPath: /usr/local/apache2/conf/httpd.conf\n              subPath: httpd.conf           \n            - name: apache-custom-config\n              mountPath: /usr/local/apache2/conf/mime.types \n              subPath: mime.types                         \nEOF\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#create-sample-deployment","title":"Create Sample Deployment","text":"<pre><code>kubectl -n default create deployment httpd-test --image=httpd:latest --replicas=1\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#apply-the-patch-file","title":"Apply The Patch File","text":"<pre><code>kubectl -n default patch deployment httpd-test --patch-file /tmp/patch.yaml\n</code></pre> <p>Caution</p> <p>A container using a ConfigMap as a subPath volume will not receive ConfigMap updates.</p>"},{"location":"Kubernetes_Templates/ConfigMap/#mount-confgimap-into-directory","title":"Mount Confgimap Into Directory","text":""},{"location":"Kubernetes_Templates/ConfigMap/#patch-file_3","title":"Patch File","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      volumes:\n        - name: apache-custom-config\n          configMap:\n            name: apache-custom\n            defaultMode: 420        \n      containers:\n        - name: httpd\n          volumeMounts:\n            - name: apache-custom-config\n              mountPath: /sampe-config\nEOF\n</code></pre>"},{"location":"Kubernetes_Templates/ConfigMap/#apply-the-patch-file_1","title":"Apply The Patch File","text":"<pre><code>kubectl -n default patch deployment httpd-test --patch-file /tmp/patch.yaml\n</code></pre> <p>Warning</p> <p>This will replace everything inside the original <code>/sampe-config</code> directory.</p>"},{"location":"Kubernetes_Templates/Deployment/","title":"Deployments","text":""},{"location":"Kubernetes_Templates/Deployment/#debian-with-infinite-loop","title":"Debian With Infinite Loop","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: debian-example\n  name: replace_me #(1)\n  namespace: replace_me # (2)\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: debian-example\n  strategy:\n    type: Recreate \n  template:\n    metadata:\n      labels:\n        k8s-app: debian-example\n      name: debian-example\n    spec:\n      containers:\n      - args:\n        - -c\n        - while true; do echo \"$(date +%F\\ %T) - hello\"; sleep 10;done\n        command:\n        - /bin/sh\n        image: debian:latest\n        imagePullPolicy: Always\n        name: debian-example\n        securityContext:\n          privileged: false\n</code></pre> <ol> <li>Name Of The Deployment</li> <li>Namespace name where you want to Deploy.</li> </ol>"},{"location":"Kubernetes_Templates/Deployment/#minio-deployment-with-hostpath","title":"Minio Deployment With hostPath","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: minio\n  name: minio-server\n  namespace: minio\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      k8s-app: minio\n  strategy:\n    type: Recreate \n  template:\n    metadata:\n      labels:\n        k8s-app: minio\n      name: minio\n    spec:\n      nodeName: k8s-admin.loc\n      volumes:\n        - name: minio-storage\n          hostPath:\n            type: Directory\n            path: /srv/raid5_safe/k8s_storage/minio    \n      containers:\n        - name: minio-server\n          env:\n            - name: MINIO_ROOT_USER\n              value: admin\n            - name: MINIO_ROOT_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  key: adminpassword\n                  name: minio-admin-user \n          image: bitnami/minio:2022.5.23\n          imagePullPolicy: Always\n          securityContext:\n            privileged: false\n          volumeMounts:\n            - name: minio-storage\n              mountPath: /data      \n</code></pre>"},{"location":"Kubernetes_Templates/Deployment/#bonus-secret","title":"Bonus - Secret","text":"<pre><code>kubectl -n minio create secret generic minio-admin-user --from-literal=adminpassword=IvoTSZW8Fr4kjkdRsL36\n</code></pre>"},{"location":"Kubernetes_Templates/Deployment/#bobus-service-ingress-for-web-access","title":"Bobus - Service &amp; Ingress (For Web Access)","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: mini-web\n  namespace: minio\nspec:\n  ports:\n  - port: 80\n    targetPort: 9000\n    name: http\n  selector:\n     k8s-app: minio\n  type: ClusterIP\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mini-web\n  namespace: minio\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: # Replace \n    http:\n      paths:\n      - backend:\n          service:\n            name: mini-web\n            port:\n              name: http\n        pathType: ImplementationSpecific\n</code></pre>"},{"location":"Kubernetes_Templates/Deployment/#bonus-metallb-service","title":"Bonus - MetalLB Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: minio\n  annotations:\n    metallb.universe.tf/address-pool: default\nspec:\n  ports:\n  - port: 9000\n    targetPort: 9000\n    name: tcp-9000\n  - port: \n    targetPort: 9001\n    port: 9001\n    name: web\n  selector:\n     k8s-app: minio\n  type: LoadBalancer\n</code></pre>"},{"location":"Kubernetes_Templates/Ingress/","title":"Ingress","text":""},{"location":"Kubernetes_Templates/Ingress/#nginx-ingress-class","title":"Nginx Ingress Class","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: nginx\n  name: replace_me\n  namespace: replace_me\nspec:\n  #ingressClassName: nginx\n  rules:\n  - host: replace_me\n    http:\n      paths:\n      - backend:\n          service:\n            name: replace_me\n            port:\n              name: replace_me\n        pathType: ImplementationSpecific\n</code></pre>"},{"location":"Kubernetes_Templates/Ingress/#kubernetes-dashboard","title":"Kubernetes Dashboard","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n  labels:\n    app.kubernetes.io/instance: kubernetes-dashboard\n  name: dashboard\n  namespace: kubernetes-dashboard\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: # replace with host name\n    http:\n      paths:\n      - backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n        path: /\n        pathType: ImplementationSpecific\n</code></pre>"},{"location":"Kubernetes_Templates/Ingress/#extensionsv1beta1","title":"<code>extensions/v1beta1</code>","text":"<p>Attention</p> <p>Deprecated Api Version</p> <pre><code>kind: Ingress\napiVersion: extensions/v1beta1\nmetadata:\n  name: replace_me\n  namespace: replace_me\nspec:\n  rules:\n    - host: replace_me\n      http:\n        paths:\n          - pathType: ImplementationSpecific\n            backend:\n              serviceName: replace_me\n              servicePort: 5601 \n</code></pre>"},{"location":"Kubernetes_Templates/Ingress/#nginx-ingress-https-backend","title":"Nginx Ingress HTTPS backend","text":"<pre><code>  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/backend-protocol: HTTPS\n    nginx.ingress.kubernetes.io/proxy-body-size: 100m\n</code></pre>"},{"location":"Kubernetes_Templates/PersistentVolumeClaim/","title":"PersistentVolumeClaim","text":""},{"location":"Kubernetes_Templates/PersistentVolumeClaim/#list-storage-classes","title":"List Storage Classes","text":"<pre><code>kubectl get sc\n</code></pre>"},{"location":"Kubernetes_Templates/PersistentVolumeClaim/#example-claim","title":"Example Claim","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: replace_me\nspec:\n  storageClassName: replace_me\n  accessModes:\n    - ReadWriteOnce # (1)\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <ol> <li>Avaiable Access Modes: ReadWriteOnce, ReadOnlyMany, ReadWriteMany, ReadWriteOncePod Link</li> </ol>"},{"location":"Kubernetes_Templates/PersistentVolumeClaim/#create-and-attach-pvc-shell-script","title":"Create And Attach PVC (Shell Script)","text":"<p>Caution</p> <p>Highlighted Lines must be changed!</p> <pre><code>#!/bin/bash\n\nPVC=$( mktemp /tmp/pvc-XXXXXX.yaml)\nPATCH=$( mktemp /tmp/patch-XXXXXX.yaml)\n\nread -p \"Namespace?: \" NS\nread -p \"PVC name: \" PVC\nread -p \"PVC size (mb): \" SIZE\nread -p \"Deployment?: \" DEPLOY\nread -p \"Container?: \" CONTAINER\nread -p \"Mount?: \" MOUNT\n\n\n\ncat &lt;&lt;EOF&gt;$PVC\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: $PVC\n  namespace: $NS\nspec:\n  accessModes:\n    - ReadWriteOnce \n  resources:\n    requests:\n      storage: ${SIZE}Mi\n  storageClassName: longhorn-one-replica\n  volumeMode: Filesystem\nEOF\n\nkubectl apply -f $PVC \n\ncat &lt;&lt;EOF&gt;$PATCH\nspec:\n  template:\n    spec:\n      volumes:\n        - name: pvc-$PVC\n          persistentVolumeClaim:\n            claimName: $PVC\n      containers:\n        - name: $CONTAINER\n          volumeMounts:\n            - name: pvc-$PVC\n              mountPath: $MOUNT\nEOF\n\n\necho \"Sleep 5 secs\"\nsleep 5\nkubectl -n $NS patch deployment $DEPLOY --patch-file $PATCH \n</code></pre>"},{"location":"Kubernetes_Templates/PersistentVolumeClaim/#example-attach","title":"Example Attach","text":"<pre><code>    spec:\n      volumes:\n        - name: ohab-data\n          persistentVolumeClaim:\n            claimName: ohab-data\n</code></pre> <pre><code>    spec:\n      containers:\n        - name: openhab3\n          volumeMounts:\n            - name: ohab-data\n              mountPath: /openhab/addons\n              subPath: addons\n</code></pre>"},{"location":"Kubernetes_Templates/ReverseProxy/","title":"Kubernetes Reverse Proxy With Ingress, Service And Endpoint","text":""},{"location":"Kubernetes_Templates/ReverseProxy/#tldr","title":"TL;DR","text":"<p>Maybe you are wondering if Kubernetes capable of proxying requests to an external service. In which situation can it be useful? What is the benefit of it? Imagine the situation that you have a Kubernetes cluster, perfectly configured Ingresses, Services, applications, etc. Everything goes well. But you have a service which is not running inside the Kubernetes cluster, and you want to access it form the internet. Port 443 and 80 are reserved for the IngressController, so your application could not bind these ports. My situation is similar to this, but it is a little difference.</p> <p>I have a Kubernetes cluster running on some VPS and at home: </p> <ul> <li>2 VPS node have static, public ip address</li> <li>2 node at my home are behind NAT, and don't have static ip address.</li> </ul> <p>That's why my ingress pods are runnung only on the two VPS node; the IngressController DaemonSet has nodeSelector:</p> <pre><code>      nodeSelector:\n        kubernetes.io/os: linux\n        nginxIngress: \"true\"\n</code></pre> <p>Ingress PODs:</p> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE   IP          NODE                   NOMINATED NODE   READINESS GATES\ningress-nginx-controller-bplnb   1/1     Running   0          23d   10.8.0.33   vps11                  &lt;none&gt;           &lt;none&gt;\ningress-nginx-controller-qwr4c   1/1     Running   0          23d   10.8.0.2    vps9                   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>For simplicity I'm using dns loadbalancer between my two ingress pods. Another important thing, that the entire Kubernetes cluster is behind Wireguard VPN, so the nodes are connected to each other inside this VPN connection. </p> <p>I have a separate HomeAssistant server, and I want to access it through my static ip addresses. I think it is better than user some kind of DynDns service, and it is even impossible when you are behind CGNAT. All of my PCs (servers) connected to the same Wireguard VPN. So I want to access the HomeAssistant server through the nginx ingress. </p> <p>Note</p> <p>I know that there are several other ways to access services running  behind NAT or CGNAT. The simpiest is to install Wireguard to all the device from which I want to access the home assistnat server, but I think this is an interesting way to achieve my goal.</p> <p>Ok let's see the solution.</p>"},{"location":"Kubernetes_Templates/ReverseProxy/#create-the-service-and-endpoints","title":"Create The Service And Endpoints","text":""},{"location":"Kubernetes_Templates/ReverseProxy/#service","title":"Service","text":"<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: hassio-ext-test\n  namespace: default\nspec:\n  ports:\n    - name: hassio\n      protocol: TCP\n      port: 80\n      targetPort: 80\n  type: ClusterIP\n</code></pre>"},{"location":"Kubernetes_Templates/ReverseProxy/#endpoints","title":"Endpoints","text":"<pre><code>apiVersion: v1\nkind: Endpoints\nmetadata:\n  namespace: default\n  name: hassio-ext-test\nsubsets:\n- addresses:\n  - ip: 10.8.0.1\n  ports:\n  - name: hassio\n    port: 8123\n    protocol: TCP\n</code></pre> <p>The following fileds must mach:</p> Service Endpoints metadata.name metadata.name spec.ports.name subsets.ports.name spec.ports.protocol subsets.ports.protocol <p>The Service must not have any selector in the spec.</p> <p>The HomeAssistant service is running on <code>10.8.0.1:8123</code>.</p> Note <p>This approch can also be useful if you want to access external service from inside the Kubernetes cluster. For example you have external Elasticsearch cluster with multiple ip addresses and you want to access it from a POD. You can simply define multiple ip address in the service <code>subnet.addresses</code> section.</p>"},{"location":"Kubernetes_Templates/ReverseProxy/#ingress","title":"Ingress","text":"<p>In the last step we define the Ingress.</p> <pre><code>kind: Ingress\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: hassio-test\n  namespace: default\n  annotations:\n    kubernetes.io/ingress.class: nginx\nspec:\n  rules:\n    - host: hassio-test.vincze.work\n      http:\n        paths:\n          - pathType: ImplementationSpecific\n            backend:\n              service:\n                name: hassio-ext-test\n                port:\n                  number: 80\n</code></pre> <p>That's all. Now you can access the HomeAssistant service running outside the Kubernetes cluster.</p> <p>If you have configured cert-manager you may want to get a valid, public trust certificate for this ingress. You can achieve this by adding an extra annotation:</p> <pre><code>metadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n</code></pre>"},{"location":"Kubernetes_Templates/Secret/","title":"Secret","text":""},{"location":"Kubernetes_Templates/Secret/#create-secret","title":"Create Secret","text":"<pre><code>kubectl -n default create secret generic test-password --from-literal=pass=vqMSr49PyimubUDIGxvA\n</code></pre>"},{"location":"Kubernetes_Templates/Secret/#check","title":"Check","text":"<p>Command<pre><code>kubectl -n default get secret test-password -o yaml\n</code></pre> Output<pre><code>apiVersion: v1\ndata:\n  pass: dnFNU3I0OVB5aW11YlVESUd4dkE=\nkind: Secret\nmetadata:\n  creationTimestamp: \"2022-03-29T10:33:02Z\"\n  name: test-password\n  namespace: default\n  resourceVersion: \"130167833\"\n  uid: 269f72f5-2539-437d-877e-29959a754bef\ntype: Opaque\n</code></pre></p>"},{"location":"Kubernetes_Templates/Secret/#secret-as-environment-variable","title":"Secret As Environment Variable","text":"<p>First Create a patch file:</p> <pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      containers:\n        - name: debian-example\n          env:\n           - name: PASSWORD\n             valueFrom:\n               secretKeyRef:\n                 name: test-password\n                 key: pass\nEOF\n</code></pre> <p>Run patch command <pre><code>kubectl -n default patch deployment debian-test --patch-file /tmp/patch.yaml\n</code></pre></p>"},{"location":"Kubernetes_Templates/Secret/#secret-as-file-mount-simple","title":"Secret As File Mount (Simple)","text":""},{"location":"Kubernetes_Templates/Secret/#patch-file","title":"Patch File","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      volumes:\n        - name: secretpassword\n          secret:\n            secretName: test-password\n      containers:\n        - name: debian-example\n          volumeMounts:\n            - name: secretpassword\n              mountPath: \"/etc/foo\"\n              readOnly: true\nEOF\n</code></pre>"},{"location":"Kubernetes_Templates/Secret/#apply","title":"Apply","text":"<pre><code>kubectl -n default patch deployment debian-test --patch-file /tmp/patch.yaml\n</code></pre>"},{"location":"Kubernetes_Templates/Secret/#check-inside-the-container","title":"Check (Inside The Container)","text":"<p>Commands<pre><code>find /etc/foo/\ncat /etc/foo/pass ; echo\n</code></pre> Ouptuts<pre><code># find\n/etc/foo/\n/etc/foo/..data\n/etc/foo/pass\n/etc/foo/..2022_03_29_10_54_40.729006392\n/etc/foo/..2022_03_29_10_54_40.729006392/pass\n\n# cat\nvqMSr49PyimubUDIGxvA\n</code></pre></p>"},{"location":"Kubernetes_Templates/Secret/#secret-as-file-mount-mountpath","title":"Secret As File Mount (mountPath)","text":""},{"location":"Kubernetes_Templates/Secret/#patch-file_1","title":"Patch File","text":"<pre><code>cat &lt;&lt;EOF&gt;/tmp/patch.yaml\nspec:\n  template:\n    spec:\n      volumes:\n        - name: secretpasswordpath\n          secret:\n            secretName: test-password\n            items:\n              - key: pass\n                path: custompath/password\n      containers:\n        - name: debian-example\n          volumeMounts:\n            - name: secretpasswordpath\n              mountPath: \"/etc/bar\"\n              readOnly: true\nEOF\n</code></pre>"},{"location":"Kubernetes_Templates/Secret/#apply_1","title":"Apply","text":"<pre><code>kubectl -n default patch deployment debian-test --patch-file /tmp/patch.yaml\n</code></pre>"},{"location":"Kubernetes_Templates/Secret/#check-inside-the-container_1","title":"Check (Inside The Container)","text":"<p>Info</p> <p>See the highlighted lines in the patch file and below.</p> <p>Commands<pre><code>find /etc/bar/ -ls\ncat /etc/bar/custompath/password  ; echo \n</code></pre> Ouptuts<pre><code># find /etc/bar/ -ls\n925320365      0 drwxrwxrwt   3 root     root          100 Mar 29 11:08 /etc/bar/\n925316004      0 lrwxrwxrwx   1 root     root           31 Mar 29 11:08 /etc/bar/..data -&gt; ..2022_03_29_11_08_55.729800752\n925316003      0 lrwxrwxrwx   1 root     root           17 Mar 29 11:08 /etc/bar/custompath -&gt; ..data/custompath\n925316000      0 drwxr-xr-x   3 root     root           60 Mar 29 11:08 /etc/bar/..2022_03_29_11_08_55.729800752\n925316001      0 drwxr-xr-x   2 root     root           60 Mar 29 11:08 /etc/bar/..2022_03_29_11_08_55.729800752/custompath\n925316002      4 -rw-r--r--   1 root     root           20 Mar 29 11:08 /etc/bar/..2022_03_29_11_08_55.729800752/custompath/password\n\n# cat /etc/bar/custompath/password  ; echo  \nvqMSr49PyimubUDIGxvA\n</code></pre></p> <p>Reference:</p> <ul> <li>https://kubernetes.io/docs/concepts/configuration/secret/</li> </ul>"},{"location":"Kubernetes_Templates/StatefulSet/","title":"StatefulSet","text":""},{"location":"Kubernetes_Templates/StatefulSet/#example-elasticsearch-cluster","title":"Example: Elasticsearch Cluster","text":"<pre><code>kind: StatefulSet\napiVersion: apps/v1\nmetadata:\n  name: es-cluster\n  namespace: kibana\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      k8s-app: es-cluster\n  template:\n    metadata:\n      name: es-cluster\n      labels:\n        k8s-app: es-cluster\n    spec:\n      containers:\n        - resources:\n            requests:\n              cpu: '1'\n              memory: 5G\n          name: es-cluster\n          env:\n            - name: NODENAME\n              valueFrom:\n                fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.name\n            - name: SERVICENAME\n              value: es-cluster\n            - name: node.name\n              value: $(NODENAME).$(SERVICENAME)\n            - name: cluster.name\n              value: es-cluster\n            - name: ES_JAVA_OPTS\n              value: '-Xms4g -Xmx4g'\n            - name: discovery.seed_hosts\n              value: &gt;-\n                es-cluster-0.es-cluster,es-cluster-1.es-cluster,es-cluster-2.es-cluster,es-cluster-3.es-cluster,es-cluster-4.es-cluster\n            - name: cluster.initial_master_nodes\n              value: &gt;-\n                es-cluster-0.es-cluster,es-cluster-1.es-cluster,es-cluster-2.es-cluster,es-cluster-3.es-cluster,es-cluster-4.es-cluster\n          ports:\n            - name: http\n              containerPort: 9200\n              protocol: TCP\n            - name: tcp\n              containerPort: 9300\n              protocol: TCP\n          imagePullPolicy: Always\n          volumeMounts:\n            - name: es-data\n              mountPath: /usr/share/elasticsearch/data\n              subPath: etc\n          image: 'docker.elastic.co/elasticsearch/elasticsearch:7.16.2'\n  volumeClaimTemplates:\n    - kind: PersistentVolumeClaim\n      apiVersion: v1\n      metadata:\n        name: es-data\n        creationTimestamp: null\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 20Gi\n        storageClassName: cephfs-ec\n        volumeMode: Filesystem\n  serviceName: es-cluster\n  podManagementPolicy: OrderedReady\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 0\n  revisionHistoryLimit: 10\n</code></pre> <code>- name: NODENAME</code> The Downward API <code>serviceName: es-cluster</code> serviceName: serviceName is the name of the service that governs this StatefulSet. This service must exist before the StatefulSet, and is responsible for the network identity of the set. Pods get DNS/hostnames that follow the pattern: pod-specific-string.serviceName.default.svc.cluster.local where \"pod-specific-string\" is managed by the StatefulSet controller."},{"location":"Kubernetes_Templates/services/","title":"Services","text":""},{"location":"Kubernetes_Templates/services/#nginx-ingress-metallb","title":"Nginx Ingress - MetalLB","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  namespace: ingress-nginx\n  annotations:\n    metallb.universe.tf/address-pool: default\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n    name: http\n  - port: 443\n    targetPort: 443\n    name: https\n  selector:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/name: ingress-nginx\n  type: LoadBalancer\n</code></pre>"},{"location":"Kubernetes_Templates/services/#bonus-metallb-config","title":"Bonus - MetalLB config","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  namespace: metallb-system\n  name: config\ndata:\n  config: |\n    address-pools:\n    - name: default\n      protocol: layer2\n      addresses:\n      - 172.16.1.33 - 172.16.1.62\n</code></pre>"},{"location":"Kubernetes_Templates/services/#bonus-result","title":"Bonus - Result","text":"<pre><code># kubectl -n ingress-nginx describe svc nginx\n\nName:                     nginx\nNamespace:                ingress-nginx\nLabels:                   &lt;none&gt;\nAnnotations:              metallb.universe.tf/address-pool: default\nSelector:                 app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx\nType:                     LoadBalancer\nIP Family Policy:         SingleStack\nIP Families:              IPv4\nIP:                       10.253.161.102\nIPs:                      10.253.161.102\nLoadBalancer Ingress:     172.16.1.33\nPort:                     http  80/TCP\nTargetPort:               80/TCP\nNodePort:                 http  30154/TCP\nEndpoints:                10.32.0.13:80\nPort:                     https  443/TCP\nTargetPort:               443/TCP\nNodePort:                 https  31568/TCP\nEndpoints:                10.32.0.13:443\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:\n  Type    Reason        Age                From                Message\n  ----    ------        ----               ----                -------\n  Normal  IPAllocated   3h47m              metallb-controller  Assigned IP [\"172.16.1.33\"]\n  Normal  nodeAssigned  3h47m              metallb-speaker     announcing from node \"k8s-nuc-test\"\n  Normal  nodeAssigned  61m                metallb-speaker     announcing from node \"k8s-nuc-test\"\n  Normal  nodeAssigned  26m (x2 over 26m)  metallb-speaker     announcing from node \"k8s-nuc-test\"\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/","title":"Compile Apache HTTPD 2.4.X &amp; PHP","text":"<p>Caution</p> <p>This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/#what-is-needed-requirements","title":"What Is Needed / Requirements","text":"<p>Compiling apache from source is very easy you have to follow some steps. In case of almost all linux distributions Apache http server can be installed via its package manager. There are some advantages of compiling Apache (or any other application) from source, but If you don't have any special requirement I advise to install Apache from package manager. But yet in certain cases you have to compile from source:  </p> <ul> <li>You do not have root access. In this case it will be hard to install the necessary packages. :) But If every needed packages are installed you can compile apache. And run it with your own user. If you install apache from package manager, apache will write its log files with root user, and the config files will be writable for only root user. Of course you can give (r/w) permission to other user to these files. I think this scenario is rare, and without root access your possibilities are very limited.</li> <li>You can install different version of the application, which cannot be install via package manager. </li> <li>You can complie the application with certain flags and options which may be missing  in the repository. </li> </ul> <p>Disadvantages:</p> <ul> <li>You have to install all dependencies manually. </li> <li>Your package manager will unaware of the changes. </li> <li>Your applications will have to be updated manually.</li> <li>Maybe you package manager will be overwrite the dependencies.</li> </ul> <p>In my case I had to install the following packages:</p> <ul> <li><code>apt-get install libpcre3-dev</code></li> <li><code>apt-get install libxml2-dev</code></li> </ul> <p>My OS: Debian GNU/Linux 8  Linux vps10 2.6.32-042stab116.2 #1 SMP Fri Jun 24 15:33:57 MSK 2016 x86_64 GNU/Linux</p> <p>These packages must be downloaded:</p> <ul> <li>Apache HTTPD source. </li> <li>APR and APR util</li> <li>PHP 5</li> </ul>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/#preparation","title":"Preparation","text":"<p>I created a certain user which will be used all over the whole install process and this user will run the apace process.</p> <pre><code>adduser apache2\nmkdir /opt/apache2\nchown apache2:apache2 /opt/apache2/\n</code></pre> <p>I modified this user's shell to <code>/bin/false</code> and home directory to <code>/opt/apache2</code>. <pre><code>root@vps10:/home/vinyo# cat /etc/passwd|grep apa\napache2:x:1002:1002:,,,:/opt/apache2/:/bin/false\n</code></pre></p> <p>So this user won't be able to login to the linux box, but you can use <code>sudo</code> to switch to apache2 user: <code>sudo -u apache2 bash</code></p>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/#download-the-necessary-sources","title":"Download the necessary sources","text":"<p>I put all sources to: <code>/opt/apache2/sources</code></p> <pre><code>cd /opt/apache2/\nmkdir sources\ncd sources/\nwget http://xenia.sote.hu/ftp/mirrors/www.apache.org//httpd/httpd-2.4.23.tar.gz\nwget http://xenia.sote.hu/ftp/mirrors/www.apache.org//apr/apr-1.5.2.tar.gz\nwget http://xenia.sote.hu/ftp/mirrors/www.apache.org//apr/apr-util-1.5.4.tar.gz\nwget http://fr2.php.net/distributions/php-5.6.25.tar.gz\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/#compile-apache-whit-apr-and-apr-util","title":"Compile Apache whit apr and apr-util","text":"<ul> <li> <p>First you have to untar the apache source <pre><code>cd /opt/apache2/sources/\ntar xf httpd-2.4.23.tar.gz\n</code></pre></p> </li> <li> <p>Untar apr, apr-util <pre><code>cd /opt/apache2/sources/httpd-2.4.23/srclib\ntar xf /opt/apache2/sources/apr-1.5.2.tar.gz\ntar xf /opt/apache2/sources/apr-util-1.5.4.tar.gz\n</code></pre></p> </li> <li> <p>Rename APR directories <pre><code>mv apr-1.5.2 apr\nmv apr-util-1.5.4 apr-util\n</code></pre></p> </li> <li> <p>Compile Apache <pre><code>cd /opt/apache2/sources/httpd-2.4.23\n./configure  --enable-mpms-shared=all --enable-ssl --prefix=/opt/apache2/httpd-2.4.23\nmake\nmake install\n</code></pre></p> </li> </ul> <p>Apache will be installed to <code>/opt/apache2/httpd-2.4.23</code>.  After the make install command you can start you newly installed apache httpd server. But if you try to start apache you will be get this error:</p> <pre><code>apache2@vps10:/opt/apache2/httpd-2.4.23/bin$ ./apachectl start\n(13)Permission denied: AH00072: make_sock: could not bind to address [::]:80\n(13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:80\nno listening sockets available, shutting down\nAH00015: Unable to open logs\n</code></pre> <p>Yes, only root user can bind ports below 1025. So we have to modify listen port in <code>/opt/apache2/httpd-2.4.23/conf/httpd.conf</code> file. From: <code>Listen 80</code> To: <code>Listen 8080</code> After that little modification apache can be started. Check the log file:</p> <pre><code>cat /opt/apache2/httpd-2.4.23/logs/error_log\n[Sat Aug 27 12:02:53.021908 2016] [mpm_event:notice] [pid 30164:tid 140121349551872] AH00489: Apache/2.4.23 (Unix) PHP/5.6.25 configured -- resuming normal operations\n[Sat Aug 27 12:02:53.021972 2016] [core:notice] [pid 30164:tid 140121349551872] AH00094: Command line: '/opt/apache2/httpd-2.4.23/bin/httpd'\n</code></pre> <p>Now your apache web server is accessible on port 8080, you can call it from your browser.</p>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/#install-php","title":"Install PHP","text":"<ul> <li> <p>Previously we downloaded the php source, now untar it: <pre><code>cd /opt/apache2/sources\ntar xf php-5.6.25.tar.gz\n</code></pre></p> </li> <li> <p>Compile <pre><code>./configure --with-apxs2=/opt/apache2/httpd-2.4.23/bin/apxs --with-mysql --prefix=/opt/apache2/php-5.6.25\nmake \nmake install\n</code></pre></p> </li> </ul> <p>This will create a new directory: <code>/opt/apache2/php-5.6.25</code> This is the install path of php.  During the installation php will modify your apache configuration to load php module: <code>LoadModule php5_module        modules/libphp5.so</code> And you can find the php module in apache modules directory: <pre><code>apache2@vps10:/opt/apache2/httpd-2.4.23/modules$ ls -al | grep php\n-rwxr-xr-x  1 apache2 apache2 28575008 Aug 27 09:09 libphp5.so\n</code></pre></p> <p>Before you restart your apache2 instance place your php.ini to the appropriate place: <code>apache2@vps10:/opt/apache2/php-5.6.25/lib$ cp /opt/apache2/sources/php-5.6.25/php.ini-production php.ini</code></p> <p>To check if php is working place <code>info.php</code> file to your htdocs directory with this content: <pre><code>apache2@vps10:/opt/apache2/httpd-2.4.23/htdocs$ cat info.php\n&lt;?php\nphpinfo();\n?&gt;\n</code></pre></p> <p>You can call this file using lynx: <pre><code>lynx --dump http://localhost:8080/info.php | grep php.ini\n   Configuration File (php.ini) Path /opt/apache2/php-5.6.25/lib\n   Loaded Configuration File /opt/apache2/php-5.6.25/lib/php.ini\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/Compile_Apache_Httpd_2.4.x_Php/#summary","title":"Summary","text":"<p>As you can see \"compiling apache from source\" consist of a few steps, but after that if you need any additional php module you will have to install them manually by:</p> <ul> <li>Using /opt/apache2/php-5.6.25/bin/pear</li> <li>Using /opt/apache2/php-5.6.25/bin/pecl</li> <li>Compile from source (phpize, configure, make, make install)</li> </ul> <p>If you can afford, consider using your package manager to install apache &amp; php, because it is much simpler and in most cases you won't need to compile from source. </p>"},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/","title":"Create Self Signed Certificate For Apache Webserver","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#create-self-singet-certificate","title":"Create Self Singet Certificate","text":""},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#easyest-way","title":"Easyest Way","text":"<p>You can create Self Signed Certificate for you web server with just one command:</p> <p><code>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mysitename.key -out mysitename.crt</code></p> <p>References:</p> <ul> <li>https://www.sslshopper.com/article-how-to-create-and-install-an-apache-self-signed-certificate.html</li> <li>https://httpd.apache.org/docs/2.4/ssl/ssl_faq.html</li> </ul>"},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#with-csr-certificate-signing-request-des3","title":"With CSR (Certificate Signing Request) - DES3","text":"<p>Honestly there is no real difference between this and the previous method, if you use a self signed certificate.  But if you create CSR you can send it to Certifying Authority (CA) to be signed. And this method is useful when you want to use the same key with different certs.</p> <ul> <li>Generate Private Key <code>openssl genrsa -des3 -out example.key 2048</code> I recommend that create at lease 2048 bit key. <pre><code>openssl genrsa -des3 -out example.key 1024\nGenerating RSA private key, 1024 bit long modulus\n....++++++\n............++++++\ne is 65537 (0x10001)\nEnter pass phrase for example.key:\nVerifying - Enter pass phrase for example.key:\n</code></pre></li> <li> <p>Generate a CSR <code>openssl req -new -key example.key -out example.csr</code> Output: <pre><code>openssl req -new -key example.key -out example.csr\nEnter pass phrase for example.key:\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n-----\nCountry Name (2 letter code) [AU]:HU\nState or Province Name (full name) [Some-State]:SomeState\nLocality Name (eg, city) []:City\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:SomeState's Company\nOrganizational Unit Name (eg, section) []:Technology\nCommon Name (e.g. server FQDN or YOUR name) []:example.com\nEmail Address []:no-spam@realmail.com\n\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:12345678\nAn optional company name []:\n</code></pre> At this point you can send your CSR file to a CA, if you need a \"real\", trusted cert.</p> </li> <li> <p>Remove Passphrase from Key If you skip these steps apache will ask for the passphrase at each startup.  <code>cp example.key example.key.org</code> <code>openssl rsa -in example.key.org -out example.key</code></p> </li> <li> <p>Generating Self-Signed Certificate <code>openssl x509 -req -days 365 -in example.csr -signkey example.key -out example.crt</code></p> </li> </ul> <p>Now you have some new files: <pre><code>ls -lrt\ntotal 12\n-rw-r--r-- 1 janos.vincze bio 761 Aug 15 12:53 example.csr\n-rw-r--r-- 1 janos.vincze bio 963 Aug 15 12:59 example.key.org\n-rw-r--r-- 1 janos.vincze bio 887 Aug 15 12:59 example.key\n-rw-r--r-- 1 janos.vincze bio 1001 Aug 15 13:03 example.crt\n</code></pre> But you only need the <code>.key</code> and <code>.crt</code> file to configure apache.</p>"},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#with-root-key-ca","title":"With root key CA","text":"<p>I don't know if there is anybody who wants to use a root CA key on its own webpage(s). I can imagine one scenario when it can be useful. Inside an organization you can create a root CA key and sign all your certificate with it, then import the CA to all clients. For example, you have many web servers inside your intranet and sign all its certificate with your own CA. Clients inside your network can use these webpages as \"trusted\" provider if the root CA pub key is imported to the browser or to the system. I will show you how to install root CA cert into Firefox and Internet Explorer, but first we need to follow these steps to create the necessary files.</p> <ul> <li>Generate ROOT CA <code>openssl genrsa -des3 -out rootCA.key 2048</code> <code>openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem -config rootCA.conf</code> As you can see we are using a configuration file: <code>rootCA.conf</code>  So you first need to create something like this: <pre><code>[req]\ndistinguished_name = req_distinguished_name\n\n[req_distinguished_name]\ncountryName = HU\ncountryName_default = HU\nstateOrProvinceName = Budapest\nstateOrProvinceName_default = Budapest\nlocalityName = Budapest\nlocalityName_default = Budapest\norganizationalUnitName  = Technology\norganizationalUnitName_default  = Technology\ncommonName = VinczeJanosRootCA\ncommonName_default = VinczeJanosRootCA\norganizationName = Some Ltd\norganizationName_default = Some Ltd.\nE=jvincze84@gmail.com\ncommonName_max  = 64\n</code></pre></li> <li>Generate web server key(s) <code>openssl genrsa -out server1.key 2048</code> You should generate one key per sites. </li> <li>Generate CSR for the key This step is very similar to the previously mentioned.</li> <li>Generate the CSR: <code>openssl req -sha256 -new -out server1.csr -key server1.key -config config.cnf</code></li> <li>Backup the original server key: <code>cp server1.key server1.key.org</code></li> <li> <p>Remove the Passphrase <code>openssl rsa -in server1.key.org -out server1.key</code> You will use this key on the server. NOTE: You can see another config file: <code>config.cnf</code> This is necessary for the server key/crt. And please note that you can use <code>alt.names</code> in the configuration files. This is very useful if you have multiple domain names for one server or virtualhost. For example, you have two domain name: www.server.com and login.server.com. And these names are associated to one apache virtualhost: www.server.com -&gt; ServerName and login.server.com -&gt; ServerAlias.  Example Config File: <pre><code>[req]\ndistinguished_name = req_distinguished_name\nreq_extensions = v3_req\n\n[req_distinguished_name]\ncountryName = HU\ncountryName_default = HU\nstateOrProvinceName = Budapest\nstateOrProvinceName_default = Budapest\nlocalityName = Budapest\nlocalityName_default = Budapest\norganizationalUnitName  = Technology\norganizationalUnitName_default  = Technology\ncommonName = server1.company.com\ncommonName_default = server1.company.com\norganizationName = Company Ltd.\norganizationName_default = Company Ltd.\nE=boss@company.com\ncommonName_max  = 64\n\n[ v3_req ]\n# Extensions to add to a certificate request\nsubjectAltName = @alt_names\n\n[alt_names]\nDNS.1 = server1.company.com\nDNS.2 = server2.company.com\n</code></pre></p> </li> <li> <p>Sign your csr with the root CA key <code>openssl x509 -req -in server1.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server1.crt -days 3650 -extensions v3_req  -extfile config.cnf</code> This command will create the <code>server1.crt</code> which is to be used on Apache webserver.</p> </li> </ul> <p>Ok now we have the .key and .crt files. Check the cert: <code>openssl x509 -in server1.crt -text -noout</code></p> <p>Output: <pre><code>Certificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            87:8b:67:2d:2d:60:2c:48\n    Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=HU, ST=Budapest, L=Budapest, OU=Technology, CN=VinczeJanosRootCA, O=Some Ltd.\n        Validity\n            Not Before: Aug 16 09:59:59 2016 GMT\n            Not After : Aug 14 09:59:59 2026 GMT\n        Subject: C=HU, ST=Budapest, L=Budapest, OU=Technology, CN=server1.company.com, O=Company Ltd.\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:9c:ed:ec:7d:b4:bf:4e:ff:3a:ab:ef:d5:a3:fd:\n                    a1:a7:96:d0:30:c5:69:f7:a7:6c:91:ef:78:7f:03:\n                    e9:48:f3:11:45:12:39:f6:4e:ed:79:60:df:f0:6b:\n                    9a:59:16:7a:22:31:34:c7:10:df:a0:ca:c6:fb:6a:\n                    ee:77:a3:6d:89:d2:b3:db:7f:f2:f9:d0:b5:5b:f2:\n                    ed:0c:8e:03:85:5d:75:8a:de:29:dd:cd:d6:a8:7b:\n                    8f:2c:5b:77:95:19:b9:da:42:d0:15:d5:c5:20:08:\n                    61:83:2a:18:78:c9:1a:7c:55:df:25:ff:6a:69:53:\n                    09:1a:22:a0:b6:98:63:09:ef:a9:3f:54:56:4d:78:\n                    ea:2f:d7:cd:e8:58:8e:08:64:45:59:a5:c4:93:d7:\n                    ac:b5:99:1d:5c:7a:3b:6b:85:c7:cb:33:8c:e4:b0:\n                    bf:80:f1:cd:d7:68:70:dc:a0:ba:bd:fd:02:d3:36:\n                    3d:11:c9:f9:71:c8:dd:2f:3f:b5:5d:8a:66:2e:34:\n                    33:32:44:b3:49:78:5b:13:f9:8f:6f:42:d1:1f:f5:\n                    bb:4d:6f:b1:81:42:c2:93:3c:f2:81:5d:1d:1d:19:\n                    a4:40:e2:d1:2c:a5:2e:6d:fa:ad:ff:31:c3:65:58:\n                    e3:ba:50:10:80:3e:53:86:ce:0e:43:df:cd:77:dd:\n                    f9:f3\n                Exponent: 65537 (0x10001)\n        X509v3 extensions:\n            X509v3 Subject Alternative Name:\n                DNS:server1.company.com, DNS:server2.company.com\n    Signature Algorithm: sha256WithRSAEncryption\n         af:80:32:53:42:9c:8f:9e:4f:4b:e5:05:cc:41:5b:2f:c8:68:\n         1d:eb:d8:8c:07:56:d3:ba:77:d4:f9:89:7e:ea:28:57:58:59:\n         9e:df:bd:84:eb:2a:48:06:8e:44:c6:35:52:79:4e:c7:c7:0d:\n         2d:4c:08:aa:5a:95:2a:10:65:7b:56:59:26:bb:fc:4e:5b:6c:\n         73:08:18:d0:2b:59:a2:90:78:7c:2f:1d:d7:41:4e:87:59:71:\n         78:87:59:8f:f9:67:33:ae:d6:77:f0:70:00:38:e5:e8:41:67:\n         a1:b5:1d:33:ff:8a:89:97:99:cd:6c:b2:77:01:57:03:35:a5:\n         25:0d:4b:19:dd:d3:ed:98:66:0a:c2:94:17:42:68:6f:2a:19:\n         e1:cb:d3:2e:e7:e5:3a:8b:6e:3d:86:51:e9:29:56:9e:7e:b0:\n         34:96:78:bf:60:8b:db:07:2a:3e:a3:2f:44:2a:70:8f:16:b2:\n         c8:97:31:a0:ea:53:87:48:9d:6d:e3:20:33:c3:68:2a:40:37:\n         06:cb:fe:4c:01:6f:a2:6a:f1:43:0f:ed:1c:84:4e:a7:4d:a7:\n         7d:44:21:56:46:94:2f:75:6d:cf:be:1b:46:cd:5c:ef:e6:f6:\n         6e:9a:53:b5:96:9a:a7:08:73:31:14:27:57:e3:66:63:cd:82:\n         3a:f3:e0:3c\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#minimal-apache-14-configuration","title":"Minimal Apache (1.4) configuration","text":"<p>Now we can create an apache self signed certificate with 3 different methods, but as result we have to have one <code>.crt</code> and one <code>.key</code> file. This VirtualHost example redirects all http request to https, and works as a transparent proxy: <pre><code>&lt;VirtualHost *:80&gt;\n        ServerName http://pve.server.com\n        RewriteEngine On\n        RewriteCond %{HTTPS} !=on\n        RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L]\n&lt;/VirtualHost&gt;\n\n&lt;VirtualHost *:443&gt;\n  ServerAdmin webmaster@localhost\n  SSLProxyEngine on\n  SSLProxyCheckPeerCN off\n  SSLProxyVerify none\n  SSLProxyCheckPeerName off\n  SSLProxyCheckPeerExpire off\n  SSLProxyProtocol all\n\n  DocumentRoot /var/www/html\n  ServerName https://pve.server.com\n\n  SSLEngine on\n\n  SSLCertificateFile    /etc/apache2/cert/pve.crt\n  SSLCertificateKeyFile /etc/apache2/cert/pve.key\n  BrowserMatch \"MSIE [17-9]\" ssl-unclean-shutdown\n\n        DocumentRoot /var/www\n        &lt;Directory /&gt;\n                Options FollowSymLinks\n                AllowOverride None\n        &lt;/Directory&gt;\n        &lt;Directory /var/www/&gt;\n                Options Indexes FollowSymLinks MultiViews Indexes\n                AllowOverride all\n                Order allow,deny\n                allow from all\n        &lt;/Directory&gt;\n\n\n        ErrorLog ${APACHE_LOG_DIR}/pve-error.log\n        CustomLog ${APACHE_LOG_DIR}/pve-access.log combined\n\n        ProxyRequests off\n        ProxyPreserveHost on\n\n\n        ProxyPass /   https://10.30.16.100:8006/\n        ProxyPassReverse /   https://10.30.16.100:8006/\n\n&lt;/VirtualHost&gt;\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/Create_Self_Signed_Certificate_For_Apache_Webserver/#import-you-root-ca-key-to-firefox","title":"Import you root CA key to Firefox","text":"<p>If you don't want to get a \"self Signed certificate\" warning in FF you can import you root ca public key to Firefox with a few easy steps.</p> <ul> <li>Go to <code>about:preferences</code>, Advanced, Certificate. And Click View Certificates.</li> <li>In the pop-up window Choose Authories  and click \"import\"  </li> <li>Import your <code>rootCA.pem</code> file.  </li> </ul> <p>Next time you visit your website FF will trust its certificate.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/","title":"Install (compile) & Configure Motion Av Tools","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#install-compile-configure-motion-av-tools","title":"Install (compile) &amp; Configure motion + AV tools","text":""},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#0-update-running-system-install-necessary-pacgakes","title":"0. Update running system &amp; Install necessary pacgakes","text":"<ul> <li>As always, start with updating you linux system.</li> </ul> <pre><code>apt-get update\napt-get upgrade\n</code></pre> <ul> <li>Install packages</li> </ul> <pre><code>apt-get install libjpeg-dev libjpeg62-turbo-dev autoconf automake build-essential libzip-dev git yasm nasm pkg-config libavutil-dev libavformat-dev libavcodec-dev libswscale-dev autoconf automake build-essential pkgconf libtool libzip-dev libjpeg62 git libavformat-dev libavcodec-dev libavutil-dev libswscale-dev libavdevice-dev ca-certificates webp libwebp-dev curl lynx zip apache2 libx264-dev x264 libav-tools mpv\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#1-install-libav12","title":"1. Install libav12","text":"<p>Note</p> <p>This step is optional, because at the 0. step we installed libav-tools package which contains avconv. But with compiling from source we get a newer version:</p> <ul> <li>Repository: avconv version 11.8-6:11.8-1~deb8u1+rpi1</li> <li>Compiled: avconv version 12, Copyright (c) 2000-2016 the Libav developers</li> </ul>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#11-download-source-code-from-the-git-repository","title":"1.1. Download source code from the git repository","text":"<pre><code>cd /usr/src\ngit clone https://github.com/todostreaming/libav12\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#12-configure-make-and-make-install","title":"1.2. Configure &amp; make and make install","text":""},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#121-configure","title":"1.2.1. Configure","text":"<pre><code>cd /usr/src/libav12\n./configure --enable-libwebp --enable-libx264 --logfile=/root/libav-conf-$(date +%s).log --enable-gpl --prefix=/opt/libav12 | tee -a /root/libav-conf-$(date +%s).out\n</code></pre> <p>After the configure is done you can check two log files:</p> <ul> <li><code>/root/libav-conf-*.log</code></li> <li><code>/root/libav-conf-*.out</code></li> </ul> <p>The configure script must be done without any error. If you see errors in one of the log files please do not continue, and try to fix them. Usually the most error cause by a missing library and easy can be fixed with install the missing lib using <code>apt-get install</code> command.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#122-compile-the-source-code","title":"1.2.2. Compile the source code:","text":"<p>To speed up this process lets use <code>-j2</code> option. (Or if you want -j3 or -j4)</p> <pre><code>make -j2\n</code></pre> <p>During this process maybe you will some warning messages, but no errors. So you can skip them.</p> <p><code>-j [jobs], --jobs[=jobs]</code> Specifies  the  number  of  jobs (commands) to run simultaneously.  If there is more than one -j option, the last one is effective.  If the -j option is given without an argument, make will not limit the number of jobs that can run simultaneously.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#123-install-libav","title":"1.2.3. Install libav","text":"<p>To do this simply run this command:</p> <pre><code>make install\n</code></pre> <p>The binaries will be available in the directory specified with \"--prefix\" option when we run configure script. In our case: <code>/opt/libav12</code></p> <p>References: </p> <ul> <li>https://wiki.libav.org</li> <li>https://libav.org</li> <li>https://github.com/todostreaming/libav12</li> </ul>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#2-install-compile-ffmpeg","title":"2. Install / Compile FFMpeg","text":"<p>FFMpeg is no longer available in Debian repository, so if you want to use it you have to compile on your own. </p> <pre><code>cd /usr/src\ngit clone https://github.com/FFmpeg/FFmpeg\nroot@rpi2camsrv01:/usr/src/FFmpeg# ./configure --prefix=/opt/ffmpeg --enable-libx264 --enable-libwebp --enable-gpl --enable-nonfree\nmake\nmake install\n</code></pre> <p>Note: The make command runs very long time. Please be patient, and / or try run with -j[234] option.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#3-install-compile-motion","title":"3. Install / Compile Motion","text":"<p>Motion can be easily installed with <code>apt-get install motion</code>, so you my think this post is absolutely useless and has no sense. But if you want to use RTSP stream, you have to have the latest (or at lease 4.x) version of motion. The version of apt-get repository is too old and has not this feature:</p> <pre><code>root@rpi2camsrv01:/opt/motion-3.4.1-316/bin# apt-cache show motion\nPackage: motion\nSource: motion (3.2.12+git20140228-4)\nVersion: 3.2.12+git20140228-4+b2\n</code></pre> <p>By using the git source code we can install the latest version: <code>motion Version 4.0.1+git8a1b9a97</code></p> <p>If you want to use motion with mysql support in the future you need some additional packages: <code>apt-get install libmysql++-dev libmysqlclient-dev</code></p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#31-clone-from-git","title":"3.1. Clone from git","text":"<pre><code>/usr/src\ngit clone https://github.com/Motion-Project/motion.git\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#32-configure","title":"3.2. Configure","text":"<p>Before you start you may want to read the INSTALL guide: <code>less /usr/src/motion/INSTALL</code></p> <p>You can choose which library do you want to use ffmpeg / libav:</p> <pre><code>cd /usr/src/motion\nautoreconf -fiv\n./configure --prefix=/opt/motion-3.4.1-316 --with-ffmpeg=/opt/ffmpeg\n#OR\n./configure --prefix=/opt/motion-3.4.1-316 --with-ffmpeg=/opt/libav12\n</code></pre> <p>You can configure without \"with-ffmpeg\" option, it this case configure script will use the already installed libraries (libav-tool)</p> <p>After the configure script done you will see something like this:</p> <pre><code>   **************************\n      Configure status       \n      motion 4.0.1+git8a1b9a9\n   **************************\n\nOS             :     Linux\npthread support:     Yes\njpeg support:        Yes\nwebp support:        No\nV4L2 support:        Yes\nBKTR support:        No\nMMAL support:        Yes\n ... MMAL_CFLAGS: -std=gnu99 -DHAVE_MMAL -Irasppicam -I/opt/vc/include\n ... MMAL_OBJ: mmalcam.o raspicam/RaspiCamControl.o raspicam/RaspiCLI.o\n ... MMAL_LIBS: -L/opt/vc/lib -lmmal_core -lmmal_util -lmmal_vc_client -lvcos -lvchostif -lvchiq_arm\nFFmpeg support:      Yes\n ... FFMPEG_CFLAGS: -I/opt/libav12/include  \n ... FFMPEG_LIBS: -lswscale -lavdevice -lavformat -lavcodec -lx264 -lwebp -lz -pthread -lavresample -L/opt/libav12/lib -lavutil -lm  \nSQLite3 support:     No\nMYSQL support:       Yes\nPostgreSQL support:  No\n\nCFLAGS: -g -O2 -I/usr/local/include -g -O2 -D_THREAD_SAFE \nLIBS: -lm -L/usr/local/lib -pthread -ljpeg -lmysqlclient -lz\nLDFLAGS:  -L/usr/local/lib \n\nInstall prefix:       /opt/motion-3.4.1-316\n</code></pre> <p>If you need webp support configure with <code>--with-webp</code> option. The necessary packages are already installed in the 0. step (webp libwebp-dev). <code>/configure --prefix=/opt/motion-3.4.1-316 --with-ffmpeg=/opt/libav12 --with-webp</code></p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#32-make-make-install","title":"3.2. Make &amp; Make install","text":"<p>Now we have only to easy steps left, run make and make install.</p> <pre><code>make\nmake install\n</code></pre> <p>Your motion instance is now ready to use, and can be located in <code>/opt/motion-3.4.1-316</code>.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#update-20170509","title":"Update - 2017.05.09","text":"<p>If you want to use fmpeg to encode/decode .webm files you need to install these extra packages:</p> <pre><code>apt-get install libvorbis-dev libvpx-dev mjpegtools\n(optional) apt-get instell vpx-tools imagemagick\n</code></pre> <p>And configure ffmpeg with these parameters: <pre><code>./configure --prefix=/opt/ffmpeg-webm --enable-libx264 --enable-libwebp --enable-gpl --enable-nonfree --enable-libvpx --enable-libvorbis\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#4-configure-motion-to-start-at-boot","title":"4. Configure Motion to start at boot","text":""},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#41-create-user-for-motion-service","title":"4.1. Create user for motion service","text":"<pre><code>useradd motion\nroot@rpi2camsrv01:/lib/systemd/system# id motion\nuid=1002(motion) gid=1002(motion) groups=1002(motion)\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#42-edit-motionservice-systemd-config-file","title":"4.2. Edit motion.service systemd config file","text":"<p>An example service file can be found in the motion install directory: <code>/opt/motion-3.4.1-316/share/motion/examples/motion.service</code></p> <ul> <li>Copy this file to systemd directory:</li> </ul> <pre><code>cd /lib/systemd/system\ncp /opt/motion-3.4.1-316/share/motion/examples/motion.service .\n</code></pre> <ul> <li>Create dir for PID file</li> </ul> <pre><code>mkdir /opt/motion-3.4.1-316/var\n</code></pre> <ul> <li>Edit motion.sevice file</li> </ul> <pre><code>[Unit]\nDescription=Motion daemon\nAfter=local-fs.target network.target\n\n[Service]\nUser=motion\nGroup=motion\nPIDFile=/opt/motion-3.4.1-316/var/motion.pid\nExecStart=/opt/motion-3.4.1-316/bin/motion -n\nType=simple\nStandardError=null\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <ul> <li>Change Owner of motion</li> </ul> <pre><code>chown -R motion:motion /opt/motion-3.4.1-316/\n</code></pre> <ul> <li>Try to start motion.service</li> </ul> <pre><code>root@rpi2camsrv01:/lib/systemd/system# systemctl start motion.service \n</code></pre> <ul> <li>Check the service</li> </ul> <pre><code>root@rpi2camsrv01:/lib/systemd/system# systemctl status motion.service \n\u00e2\u2014\u008f motion.service - Motion daemon\n   Loaded: loaded (/lib/systemd/system/motion.service; disabled)\n   Active: active (running) since Fri 2017-03-24 09:36:46 UTC; 5s ago\n Main PID: 12812 (motion)\n   CGroup: /system.slice/motion.service\n           \u00e2\u201d\u201d\u00e2\u201d\u20ac12812 /opt/motion-3.4.1-316/bin/motion -n\n</code></pre> <ul> <li>Stop service and install service file</li> </ul> <pre><code>root@rpi2camsrv01:/lib/systemd/system# systemctl stop  motion.service \nroot@rpi2camsrv01:/lib/systemd/system# systemctl enable  motion.service \nCreated symlink from /etc/systemd/system/multi-user.target.wants/motion.service to /lib/systemd/system/motion.service.\n</code></pre> <p>Now you should use 'motion' user to configure motion, otherwise the owner of files may be changed causing permission denied. </p> <pre><code>root@rpi2camsrv01:/lib/systemd/system# sudo su - motion\nNo directory, logging in with HOME=/\nmotion@rpi2camsrv01:/opt/motion-3.4.1-316$ \n</code></pre> <p>You can create home directory to motion user: <pre><code>mkdir /home/motion\nchown -R motion:motion /home/motion/\n</code></pre></p> <p>You motion installation is ready for use now. </p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#5-configure-motion-to-use-an-onvif-ip-camera-via-rtsp-stream","title":"5. Configure Motion to use an ONVIF IP camera via RTSP stream","text":"<p>You can buy low budget IP cameras from eBay, Aliexpress and so on. Nowadays almost all cheap cameras are using ONVIF interface. I have 3 IP cameras and I have to say that the web interfaces of them are really poor. My main problem is that two of them have web interface which can be accessed only with IE, because of ActiveX. :( I hate it because I usually use Linux. Another option to configure you camera is to use some ONVIF manager software. Usually the provider of the camera send a software, as well, but its also need Windows. :( I tried to find some ONVIF manager for Linux, but could not find a really good one. I don't care too much because normally a camera have to be set up once, and then can be used.  To use these cameras with MOTION you have to determine the stream URL. If you are lucky you can find it somewhere in the web interface.  The following steps are valid only if you have already configured you camera. This means  that the cam is connected to your network at least, and you know it IP address. This post doesn't aim to describe 'how to configure your cam', so you have to do it on your own.  It is more exciting to find you ONVIF camera's stream URLs.  If you don't like my method you can do some googleing to find the URLs of your camera.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#51-what-you-need","title":"5.1. What you need?","text":"<ul> <li>Installed SOAPUI.</li> <li>Your camera WebService URL. Unfortunately this can be different from mine. Here is two examples:</li> <li>http://172.19.0.3:8080</li> <li>http://172.19.0.2/onvif/device_service</li> </ul> <p>At this step the main problem is that I don't know exact method to find your camera web service URL. If the two example above aren't working you have to do some searching on Google. </p> <p>According to the Official Documentation the URL should be this:</p> <p>The entry point for the device management service is fixed to: <code>http://onvif_host/onvif/device_service</code></p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#52-load-onvif-device-wsdl-to-soapui","title":"5.2. Load ONVIF device WSDL to soapUI","text":"<p>URL: https://www.onvif.org/ver10/device/wsdl/devicemgmt.wsdl</p> <p></p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#53-modify-endpoint","title":"5.3. Modify Endpoint","text":"<p>You can specify username password if your camera is using authentication</p> <p></p> <p>Assign the newly added endpoint to the requests, without this you can specify the endpoint for all requests. </p> <p></p> <p>Now you can close this window.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#54-getservices","title":"5.4. GetServices","text":"<p>Find 'GetServices' in the left panel.</p> <p></p> <p>Modify the request to:</p> <pre><code>&lt;wsdl:IncludeCapability&gt;true&lt;/wsdl:IncludeCapability&gt;\n</code></pre> <p>You will get all web services which supported by your camera. I paste only the relavant information from the request here. <pre><code>&lt;tds:Namespace&gt;http://www.onvif.org/ver10/device/wsdl&lt;/tds:Namespace&gt;\n&lt;tds:XAddr&gt;http://172.19.0.3:8080/onvif/devices&lt;/tds:XAddr&gt;\n&lt;tds:Namespace&gt;http://www.onvif.org/ver10/media/wsdl&lt;/tds:Namespace&gt;\n&lt;tds:XAddr&gt;http://172.19.0.3:8080/onvif/media&lt;/tds:XAddr&gt;\n&lt;tds:Namespace&gt;http://www.onvif.org/ver10/events/wsdl&lt;/tds:Namespace&gt;\n&lt;tds:XAddr&gt;http://172.19.0.3:8080/onvif/events&lt;/tds:XAddr&gt;\n&lt;tds:Namespace&gt;http://www.onvif.org/ver20/analytics/wsdl&lt;/tds:Namespace&gt;\n&lt;tds:XAddr&gt;http://172.19.0.3:8080/onvif/analytics&lt;/tds:XAddr&gt;\n</code></pre></p> <p>We need the media one: <code>http://www.onvif.org/ver10/media/wsdl</code> Copy paste this URL to your browser to get the WSDL location. You will be redirected to this URL: <code>https://www.onvif.org/ver10/media/wsdl/media.wsdl7</code></p> <p>Add this WSDL to your soapUI project. For reference please see chapter 5.2.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#55-mediabinding-getprofiles","title":"5.5. MediaBinding / GetProfiles","text":"<p>With this you get the profiles of you camera. I don't copy paste the whole response here just some important parts.</p> <ul> <li>I have two profiles:</li> </ul> <pre><code>&lt;trt:Profiles fixed=\"true\" token=\"MainProfileToken\"&gt;\n&lt;trt:Profiles fixed=\"true\" token=\"SubProfileToken\"&gt;\n</code></pre> <ul> <li>You can find everything about the profiles for example resolution:</li> </ul> <pre><code>&lt;tt:VideoEncoderConfiguration token=\"main_video_encoder_cfg_token\"&gt;\n   &lt;tt:Name&gt;main_video_encoder_cfg&lt;/tt:Name&gt;\n   &lt;tt:UseCount&gt;1&lt;/tt:UseCount&gt;\n   &lt;tt:Encoding&gt;H264&lt;/tt:Encoding&gt;\n   &lt;tt:Resolution&gt;\n      &lt;tt:Width&gt;1920&lt;/tt:Width&gt;\n      &lt;tt:Height&gt;1080&lt;/tt:Height&gt;\n   &lt;/tt:Resolution&gt;\n</code></pre> <p></p> <p>I want to use this profile in Motion (MainProfileToken).</p> <p>These profiles can be found on the web interface, but maybe the name is different.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#56-get-urls-mediabinding-getstreamuri","title":"5.6. Get URLS (MediaBinding / GetStreamUri)","text":"<p>Now you have to edit this XML before start the request.</p> <p>Open this URL in you browser: https://www.onvif.org/ver10/media/wsdl/media.wsdl And find 'GetStreamUri' operation.</p> <p>You will see how to configure this request.</p> <p></p> <p>Example:</p> <pre><code>&lt;soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:wsdl=\"http://www.onvif.org/ver10/media/wsdl\" xmlns:sch=\"http://www.onvif.org/ver10/schema\"&gt;\n   &lt;soap:Header/&gt;\n   &lt;soap:Body&gt;\n      &lt;wsdl:GetStreamUri&gt;\n         &lt;wsdl:StreamSetup&gt;\n            &lt;sch:Stream&gt;RTP-Unicast&lt;/sch:Stream&gt;\n            &lt;sch:Transport&gt;\n               &lt;sch:Protocol&gt;RTSP&lt;/sch:Protocol&gt;\n               &lt;!--Optional:--&gt;\n               &lt;sch:Tunnel/&gt;\n            &lt;/sch:Transport&gt;\n            &lt;!--You may enter ANY elements at this point--&gt;\n         &lt;/wsdl:StreamSetup&gt;\n         &lt;wsdl:ProfileToken&gt;MainProfileToken&lt;/wsdl:ProfileToken&gt;\n      &lt;/wsdl:GetStreamUri&gt;\n   &lt;/soap:Body&gt;\n&lt;/soap:Envelope&gt;\n</code></pre> <p>The most important things to note the ProfileToken:</p> <pre><code>&lt;wsdl:ProfileToken&gt;MainProfileToken&lt;/wsdl:ProfileToken&gt;\n</code></pre> <p></p> <p>Finally we have the stream URL: <pre><code>&lt;tt:Uri&gt;rtsp://172.19.0.3:554/11&lt;/tt:Uri&gt;\n</code></pre></p> <p>You can try this url in VLC media player. </p> <p>Just for demonstration my <code>&lt;wsdl:ProfileToken&gt;SubProfileToken&lt;/wsdl:ProfileToken&gt;</code> address is:</p> <pre><code>&lt;tt:Uri&gt;rtsp://172.19.0.3:554/12&lt;/tt:Uri&gt;\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#57-bonus-mediabinding-getsnapshoturi","title":"5.7. (Bonus) MediaBinding / GetSnapshotUri","text":"<p>You can get snapshot images from ONVIF cameras. To get the URL use GetSnapshotUri request:</p> <p></p> <pre><code>&lt;tt:Uri&gt;http://172.19.0.3:80/web/auto.jpg?-usr=admin&amp;amp;-pwd=admin&amp;amp;&lt;/tt:Uri&gt;\n</code></pre>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#58-bonus-devicebinding-getdeviceinformation","title":"5.8. (Bonus) DeviceBinding / GetDeviceInformation","text":"<p>Just for fun my last sample request is the 'GetDeviceInformation'. Request: <pre><code>&lt;soap:Envelope xmlns:soap=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:wsdl=\"http://www.onvif.org/ver10/device/wsdl\"&gt;\n   &lt;soap:Header/&gt;\n   &lt;soap:Body&gt;\n      &lt;wsdl:GetDeviceInformation/&gt;\n   &lt;/soap:Body&gt;\n&lt;/soap:Envelope&gt;\n</code></pre></p> <p>Response:</p> <pre><code>...\n   &lt;SOAP-ENV:Header/&gt;\n   &lt;SOAP-ENV:Body&gt;\n      &lt;tds:GetDeviceInformationResponse&gt;\n         &lt;tds:Manufacturer&gt;IPCAM&lt;/tds:Manufacturer&gt;\n         &lt;tds:Model&gt;C6F0SiZ3N0P0L0&lt;/tds:Model&gt;\n         &lt;tds:FirmwareVersion&gt;V6.1.10.2.1-20150624&lt;/tds:FirmwareVersion&gt;\n         &lt;tds:SerialNumber&gt;00E0F8218509&lt;/tds:SerialNumber&gt;\n         &lt;tds:HardwareId&gt;V6.1.10.2.1-20150624&lt;/tds:HardwareId&gt;\n      &lt;/tds:GetDeviceInformationResponse&gt;\n   &lt;/SOAP-ENV:Body&gt;\n&lt;/SOAP-ENV:Envelope&gt;\n</code></pre> <p></p> <p>Note: </p> <p>Not all requests are support by all cameras, but the main features are implemented (, I hope). Maybe your camera has different Profile names, but the xml tags must be the same.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#59-motion-configuration-changes","title":"5.9. Motion Configuration changes","text":"<ul> <li> <p>Change directory to motion conf <code>cd /opt/motion-3.4.1-316/etc/motion</code></p> </li> <li> <p>Rename motion-dist.conf to motion.conf <code>mv motion-dist.conf motion.conf</code></p> </li> <li> <p>Turn on daemon mode <code>daemon on</code></p> </li> <li> <p>Configure Logs</p> </li> <li>Run: <code>mkdir  -p /opt/motion-3.4.1-316/var/log/motion</code></li> <li> <p>Chage conf to: <code>logfile /opt/motion-3.4.1-316/var/log/motion/motion.log</code></p> </li> <li> <p>Increase log level <code>log_level 7</code></p> </li> <li> <p>Comment out videodevice line, since we will use netcam. <code>#videodevice /dev/video0</code></p> </li> <li> <p>Change resolution <pre><code># Image width (pixels). Valid range: Camera dependent, default: 352\nwidth 1920\n\n# Image height (pixels). Valid range: Camera dependent, default: 288\nheight 1080\n</code></pre></p> </li> <li> <p>Change frame rate <code>framerate 7</code></p> </li> <li> <p>Specify netcam_url <code>netcam_url rtsp://172.19.0.4:554/11</code></p> </li> <li> <p>Switch keepalive on <code>netcam_keepalive on</code></p> </li> <li> <p>Motion Detection Settings</p> </li> <li>Change treshold: <code>threshold 2500</code></li> <li>Increase minimum motion frame: <code>minimum_motion_frames 3</code></li> <li>Modify event_gap: <code>event_gap 10</code></li> <li>Set max movie time to 10 mins: <code>max_movie_time 600</code>   This is optional. I don't save any movies, but only images. It's your choice.</li> <li>Turn video save off: <code>ffmpeg_output_movies off</code> </li> <li>File save settings:</li> <li>Specify target dir to save images (and / or videos) <code>mkdir -p /opt/motion-3.4.1-316/var/spool/motion</code> <code>target_dir /opt/motion-3.4.1-316/var/spool/motion</code> </li> <li> <p>Set up the pictures name conversation: <code>picture_filename imgs/%Y-%m-%d/%H-%M-%S__%q-%D</code> <code>picture_filename imgs/%Y-%m-%d/%H/%H-%M-%S__%q-%D</code>   This will save images to [target_dir]/imgs/YEAR-MONTH-DAY/HOUR-MIN-SEC__FRAMENUMBER_MOTION.jpg format. </p> </li> <li> <p>(optional) Enable access to web control and Live View from anywhere</p> </li> </ul> <pre><code>stream_localhost off\nwebcontrol_localhost off\n</code></pre> <p>I do not save movies, because </p> <ul> <li>movies are taken real time, and uses CPU</li> <li>moves are captured real-time, I mean 10 minutes movies takes 10 minutes to watch about 10 minutes long motion detection, so instead of this I create timelapse movies based on saved images. This way I can check the movies faster than the event happened and if I find something, I can look for the saved images to investigate it deeper. </li> </ul> <p>Motion has a log of configuration it worth it to look through the motion.conf file I'm sure you will find exciting and useful options.  My setup is just enough to detect motion and save images.  I post process the images using shell scripts and ffmpeg (or libav) tools.</p>"},{"location":"Old_Blog_Contents/linux/Install_%28compile%29_%26_Configure_Motion_Av_Tools/#6-create-movie-from-still-images-jpg","title":"6. Create movie from still images (*.jpg)","text":"<p>Now we have a lot of images from the camera. My motion save one images in every 30 seconds: <pre><code># Make automated snapshot every N seconds (default: 0 = disabled)\nsnapshot_interval 30\n</code></pre> And save images when motion is detected: <pre><code># Threshold for number of changed pixels in an image that\n# triggers motion detection (default: 1500)\nthreshold 2500\n</code></pre></p> <p>My motion saves images in the following directory structure: <code>/storage/motion/output/cam1/imgs/[DATE]/[HOUR]/*.jpg</code></p> <p>Setup:</p> <pre><code>picture_filename imgs/%Y-%m-%d/%H/%H-%M-%S__%q-%D\ntarget_dir /storage/motion/output/cam1\n...\ntarget_dir /storage/motion/output/cam2\n</code></pre> <p>Each camera uses its own directory. </p> <p>I'm using this shell script to create movie from the images:</p> <pre><code>#!/bin/bash\n\nROOT_DIR=\"/storage/motion/output\"\n\nfunction LOG() {\n  echo \"[ $( date +%F\\ %T ) ] - $1 \"\n}\n\n\n###\n## VARIABLES\n###\nTMP_DIR=\"/storage/motion/tmp\"\nCAM_IMG_DIRS=$( mktemp $TMP_DIR/dovid_XXXXXXXXXXX.txt )\nNOW_DATE=$( date  +%Y-%m-%d-%H )\n\nLOG \"###\"\nLOG \"## Starting SCRITP\"\nLOG \"###\"\nLOG \"Prepare temorary file: $CAM_IMG_DIRS\"\nLOG \"Running first FOR loop: Searching for camera dirs (cam[X]). \"\nfor DIR in $( find $ROOT_DIR -mindepth 1 -maxdepth 1 -type d | egrep 'cam[0-9]{1,1}' | sort -n )\ndo\n  CAM_NUM=$( echo $DIR | egrep -o 'cam[0-9]{1,1}' ) # Extract camera number, example: cam1, cam2 ...\n  LOG \"Runninng second FOR loop: Searching for dates in camera[X] dir\"\n  for DATE_IN_DIR in $( find $DIR/imgs -mindepth 1 -maxdepth 1 -type d | egrep '[0-9]{4,4}-[0-9]{2,2}-[0-9]{2,2}' | sort -n )\n  do\n    LOG \"Running third FOR loop: Searching HOURS in camX/DATE/* ($DATE_IN_DIR) \"\n    for HOUR_IN_DATE_IN_DIR in $( find $DATE_IN_DIR -mindepth 1 -maxdepth 1 -type d | egrep '[0-9]{2,2}' | sort -n )\n    do\n      DATE_FROM_DIR=$( basename $DATE_IN_DIR ) # Extract Date From Directory Name\n      HOUR_FROM_DIR=$( basename $HOUR_IN_DATE_IN_DIR ) # Extract HOUR number\n      VID_DIR=\"$ROOT_DIR/$CAM_NUM/vids/$DATE_FROM_DIR\" # Preapre VIDEO DIR\n      VID_FILE=\"$VID_DIR/$DATE_FROM_DIR-$HOUR_FROM_DIR.avi\" # SET Video FILE\n      [ ! -d $VID_DIR  ] &amp;&amp; mkdir -p $VID_DIR\n      if [ ! -f $VID_FILE ]\n      then\n        LOG \"Adding this line to $CAM_IMG_DIRS\"\n        echo \"$HOUR_IN_DATE_IN_DIR|$VID_FILE\" | tee -a $CAM_IMG_DIRS\n      fi\n    done\n  done\ndone\nLOG \"Preparing temporary file is done.\"\n\nLOG \"################ PROCESSING temporary file ################\"\nwhile IFS='|' read -r IMG_DIR VID_FILE\ndo\n  LOG \"+++++ Processing: +++++\"\n  LOG \"-- Image Dir: $IMG_DIR\"\n  LOG \"-- Video File: $VID_FILE\"\n  VID_FILE_BN=$( basename $VID_FILE ) # Video File Base Name\n  VID_FILE_BN_WO_EXT=\"${VID_FILE_BN%.*}\" # Video File WO Base Name\n  LOG \"-- Checking the the hour...\"\n  if [ \"$VID_FILE_BN_WO_EXT\" != \"$NOW_DATE\" ]\n  then\n    LOG \"!!!!!!!!!!!!!!!!!! Encoding: $VID_FILE !!!!!!!!!!!!!!!!!!\"\n    START_ENC_DATE=$( date +%s )\n    for FILE in $( find $IMG_DIR -type f -name '*.jpg' | sort )\n    do\n      cat $FILE\n    done | /opt/ffmpeg/bin/ffmpeg -loglevel warning -r 25 -f image2pipe -i - -c:v mpeg4 -vtag xvid -qscale:v 14 $VID_FILE\n    RETVAL=$?\n    STOP_ENC_DATE=$( date +%s )\n    LOG \"Encoding DONE in $(( $STOP_ENC_DATE - $START_ENC_DATE  )) seconds, and finsihed with $RETVAL error code.\"\n  else\n    LOG \"$NOW_DATE is the current date (hour), so we can not process this folder.  Skipping... ($VID_FILE) \"\n  fi\ndone &lt;$CAM_IMG_DIRS\necho\necho\n</code></pre> <p>This script creates separate movie files per hours. Example:</p> <pre><code>motion@camsrv01:/opt/motion-3.4.1-316/etc/motion$ ls -al /storage/motion/output/cam1/vids/2017-05-13\ntotal 680340\ndrwxr-xr-x 2 motion motion     4096 May 14 00:10 .\ndrwxr-xr-x 9 motion motion     4096 May 14 00:10 ..\n-rw-r--r-- 1 motion motion 11216416 May 13 01:10 2017-05-13-00.avi\n-rw-r--r-- 1 motion motion  4704858 May 13 02:10 2017-05-13-01.avi\n-rw-r--r-- 1 motion motion  4349738 May 13 03:10 2017-05-13-02.avi\n-rw-r--r-- 1 motion motion  6069618 May 13 04:10 2017-05-13-03.avi\n-rw-r--r-- 1 motion motion 10084490 May 13 05:10 2017-05-13-04.avi\n-rw-r--r-- 1 motion motion 16269868 May 13 06:10 2017-05-13-05.avi\n-rw-r--r-- 1 motion motion 29998296 May 13 07:11 2017-05-13-06.avi\n-rw-r--r-- 1 motion motion 42320628 May 13 08:11 2017-05-13-07.avi\n-rw-r--r-- 1 motion motion 57703934 May 13 09:12 2017-05-13-08.avi\n-rw-r--r-- 1 motion motion 46332720 May 13 10:11 2017-05-13-09.avi\n-rw-r--r-- 1 motion motion 48425176 May 13 11:11 2017-05-13-10.avi\n-rw-r--r-- 1 motion motion 47206092 May 13 12:11 2017-05-13-11.avi\n-rw-r--r-- 1 motion motion 43665386 May 13 13:11 2017-05-13-12.avi\n-rw-r--r-- 1 motion motion 36392852 May 13 14:11 2017-05-13-13.avi\n-rw-r--r-- 1 motion motion 64864312 May 13 15:11 2017-05-13-14.avi\n-rw-r--r-- 1 motion motion 44121274 May 13 16:11 2017-05-13-15.avi\n-rw-r--r-- 1 motion motion 43531740 May 13 17:11 2017-05-13-16.avi\n-rw-r--r-- 1 motion motion 30732256 May 13 18:11 2017-05-13-17.avi\n-rw-r--r-- 1 motion motion 25023926 May 13 19:10 2017-05-13-18.avi\n-rw-r--r-- 1 motion motion 22104216 May 13 20:10 2017-05-13-19.avi\n-rw-r--r-- 1 motion motion 25939654 May 13 21:11 2017-05-13-20.avi\n-rw-r--r-- 1 motion motion 15746894 May 13 22:11 2017-05-13-21.avi\n-rw-r--r-- 1 motion motion 11119838 May 13 23:10 2017-05-13-22.avi\n-rw-r--r-- 1 motion motion  8694202 May 14 00:10 2017-05-13-23.avi\n</code></pre> <p>Crontab: <pre><code>10      *       *       *       *       /storage/motion/scripts/do_vids_v2.sh &gt;&gt;/storage/motion/logs/cam1_vid_create.log 2&gt;&amp;1\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/Install_Mps-youtube_Console_Based_Youtube_Player/","title":"Install Mps youtube Console Based Youtube Player","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/linux/Install_Mps-youtube_Console_Based_Youtube_Player/#install-mps-youtube-console-based-youtube-player","title":"Install mps-youtube console based youtube player","text":"<p>Why do I need console based youtube player? The answer is very simple. When I'm working in my Workshop I want to listen music from youtube. OK. I know that there are uncountable way to listen music online.  I have some old desktop PC which  have very limited resources, and can't able to play youtube clip in browser. This old PC is connected to an as old HIFI system as the PC itself.  So I needed a lightweight youtube player which can be run on an old PC. After some gooleing I found mps-youtube.  It is very simple to install and use it. I love it.  :) This is exactly what I wanted.</p> <p>Here is the steps to install mps-youtube: Install python pip3: <code>apt-get install python3-pip</code></p> <p>Install dependencies: <code>apt-get install mplayer2 mpv</code></p> <p>Install mps-youtube: <code>pip3 install mps-youtube</code></p> <p>Install youtube-dl: <code>pip3 install youtube_dl</code></p> <p>Set player to mpv Run <code>mpsyt</code> Type: <code>set player /usr/bin/mpv</code></p> <p>Reference:</p> <ul> <li>https://github.com/mps-youtube/mps-youtube</li> </ul> <p>Bonus: Some youdube-dl example.</p> <ul> <li>Download youtube video in MP3 format:</li> </ul> <pre><code>youtube-dl --restrict-filenames --no-mtime --extract-audio --audio-format mp3 --audio-quality 0 -o /path/to/dir/%\\(title\\)s.%\\(ext\\)s [LINK]\n</code></pre> <p>Where:   * <code>--restrict-filenames</code> Restrict filenames to only ASCII characters, and avoid \"&amp;\" and spaces in filenames   * <code>--no-mtime</code> Do not use the Last-modified header to set the file modification time   * <code>-x, --extract-audio</code> Convert video files to audio-only files (requires ffmpeg or avconv and ffprobe or avprobe)   * <code>--audio-format mp3</code> Save output as MP3.   * <code>audio-quality</code> Specify ffmpeg/avconv audio quality, insert a value between 0 (better) and 9 (worse) for VBR or a specific bitrate like 128K (default 5)   * <code>-o /path/to/dir/%\\(title\\)s.%\\(ext\\)s</code> Output directory and file pattern.</p> <ul> <li>Download Video File</li> </ul> <p>Simply run <code>youtube-dl [LINK]</code>, this will save the video in mp4 format.</p> <ul> <li>Process URLs from file:  </li> </ul> <p><code>youtube-dl --batch-file list.txt</code> </p>"},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/","title":"create Your Own Dyndns Service With Bind named","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#create-your-own-dyndns-service-with-bind-named","title":"Create Your Own DynDns Service with Bind (Named)","text":""},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#1-first-you-need-to-generate-the-private-and-public-key","title":"1. First you need to generate the private and public key","text":"<p>You can do that with one simple command: <pre><code>dnssec-keygen -a HMAC-MD5 -b 256 -n HOST dyn-key\n</code></pre> dnssec-keygen -a HMAC-MD5 -b 256 -n HOST dyn-key</p> <p>I chose <code>HMAC-MD5</code> hash algorithm, and I recommend to generate at least 256 bit keys. The <code>-n</code> option: <code>-n &lt;nametype&gt;: ZONE | HOST | ENTITY | USER | OTHER</code></p> <p>We will have these two files: <pre><code>Kdyn-key.+157+60890.key\nKdyn-key.+157+60890.private\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#2-modify-namedconf","title":"2 Modify named.conf","text":"<p>Add this line to <code>named.conf</code>:</p> <pre><code>include \"/etc/bind/dns.keys\";\n</code></pre>"},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#3-create-dnskeys-configuration-file","title":"3. Create dns.keys configuration file","text":"<p>It must look like something similar to this example: <pre><code>cat dns.keys \nkey dyn-key. {\n    algorithm HMAC-MD5;\n    secret \"fop39Dcbz9HZ9sQqzo64fHorSIJXnmGjJ980BwTg6O4=\";\n};\n</code></pre></p> <p>We have to stop here for some words. Where is the \"secret\" come from?  You can find this private key in <code>Kdyn-key.+157+60890.private</code>. In my case: </p> <pre><code>cat Kdyn-key.+157+60890.private \nPrivate-key-format: v1.3\nAlgorithm: 157 (HMAC_MD5)\nKey: fop39Dcbz9HZ9sQqzo64fHorSIJXnmGjJ980BwTg6O4=\nBits: AAA=\nCreated: 20161015122904\nPublish: 20161015122904\nActivate: 20161015122904\n</code></pre>"},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#4-allow-update-zone-with-these-keys","title":"4. Allow Update Zone with these keys","text":"<p>Example: <pre><code>zone \"dyn.vinczejanos.info\" {\n        type master;\n        file \"/etc/bind/db.dyn.vinczejanos.info\";\n        allow-query { any; };\n        allow-update { key \"dyn-key.\"; };\n};\n</code></pre></p> <p>After the configuration is done, do not forget to restart bind. <pre><code>/etc/init.d/bind9 restart\n</code></pre></p>"},{"location":"Old_Blog_Contents/linux/create_Your_Own_Dyndns_Service_With_Bind_named/#5-check-update","title":"5. Check Update","text":"<pre><code>cat update.sh \ncat &lt;&lt; EOF | nsupdate -k \"Kdyn-key.+157+60890.key\"\nserver ns20.vinczejanos.info\nzone dyn.vinczejanos.info.\nupdate delete test-dyn.dyn.vinczejanos.info\nupdate add test-dyn.dyn.vinczejanos.info 60 A 192.168.0.1\nshow\nsend\nEOF\n</code></pre>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/","title":"How To Compile Nodemcu Firmware","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#how-to-compile-nodemcu-firmware","title":"How To Compile Nodemcu Firmware","text":"<p>In this post I will assist you through some easy steps on how to build NodeMCU firmware on your own.  To do this firstly I created a vanilla Debian 8 OpenVZ container.</p> <p>If you don't want to bother to compile NodeMCU firmware on your own, you have another option: you can make it online. Here is the link.</p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#0-step","title":"0. Step","text":"<p>Every time you install a new package you should start with updating your Linux system. <pre><code>apt-get update\napt-get upgrade\n</code></pre></p> <pre><code>root@nodemcu:~# uname -a\nLinux nodemcu 2.6.32-39-pve #1 SMP Fri May 8 11:27:35 CEST 2015 x86_64 GNU/Linux\nroot@nodemcu:~# cat /etc/issue\nDebian GNU/Linux 8 \\n \\l\n</code></pre>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#1-install-the-necessary-packages","title":"1. Install the necessary packages","text":"<pre><code>apt-get install git gcc make libtool-bin gperf bison flex build-essential texinfo automake libtool cvs autoconf libncurses5-dev help2man wget bzip2 python-dev python-serial python3-serial\n</code></pre> <p>You may need to install: <pre><code>apt-get install gawk\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#2-clone-packages-from-git","title":"2. Clone packages from git","text":"<ul> <li>First we have to create a new user, because <code>esp-open-sdk</code> can not be compiled with root user. <code>useradd nodemcu</code> </li> <li>Next create a working directory for nodemcu user: <code>mkdir /opt/nodemcu</code> </li> <li>Give all permission: <pre><code>chown -R nodemcu:nodemcu /opt/nodemcu/\nchmod u+rwx /opt/nodemcu/\n</code></pre></li> <li>Change to nodemcu user <code>sudo su - nodemcu</code></li> <li>Clone the neccessary packages form Git. <pre><code>cd /opt/nodemcu\ngit clone https://github.com/nodemcu/nodemcu-firmware\ngit clone --recursive https://github.com/pfalcon/esp-open-sdk\n</code></pre></li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#3-compile-esp-open-sdk","title":"3. Compile <code>esp-open-sdk</code>","text":"<p>This package is needed to compile the NodeMCU firmware. Esp-open-sdk contains some tools which may will be useful in the future, for example tools to flash you ESP8266 board.</p> <p>Steps:</p> <ul> <li>Change directory to /opt/nodemcu/esp-open-sdk  <code>cd /opt/nodemcu/esp-open-sdk</code></li> <li>Run make command <code>make STANDALONE=y</code></li> </ul> <p>If the compilation is successfully finished you should get something like that: <pre><code>make[1]: Leaving directory '/opt/nodemcu/esp-open-sdk/esp-open-lwip'\ncp -a esp-open-lwip/include/arch esp-open-lwip/include/lwip esp-open-lwip/include/netif \\\n    esp-open-lwip/include/lwipopts.h \\\n    /opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/xtensa-lx106-elf/sysroot/usr/include/\n\nXtensa toolchain is built, to use it:\n\nexport PATH=/opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/bin:$PATH\n\nEspressif ESP8266 SDK is installed, its libraries and headers are merged with the toolchain\n</code></pre> The point is the <code>export</code> line. Each time you want to compile NodeMCU firmware you have to run this export.</p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#4-configure-and-compile-nodemcu-firmware","title":"4. Configure and Compile NodeMCU firmware","text":""},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#41-configuration","title":"4.1. Configuration","text":"<p>Before you run make command there are some configuration to do. Configuration files:</p> <pre><code>/opt/nodemcu/nodemcu-firmware/app/include/user_config.h\n/opt/nodemcu/nodemcu-firmware/app/include/user_modules.h\n/opt/nodemcu/nodemcu-firmware/app/include/user_version.h\n</code></pre> <ul> <li>user_version.h In this file you can configure version related properties. Example: <pre><code>#define NODE_VERSION    \"NodeMCU 1.5.4.1 - custom bild by jvincze\"\n#ifndef BUILD_DATE  \n#define BUILD_DATE        \"2016-09-23\"\n</code></pre></li> <li>user_modules.h Here you can configure that which modules will be included in the firmware. <pre><code>//#define LUA_USE_MODULES_AM2320 --&gt; commented out, won't be compiled\n//#define LUA_USE_MODULES_APA102\n#define LUA_USE_MODULES_BIT --&gt; will be compiled\n//#define LUA_USE_MODULES_BMP085\n</code></pre></li> <li>user_config.h  In this file there are some board related configuration, for example: memory size.</li> </ul> <p>NOTE: If you can not connect to your ESP after flashing it try to modify this value in <code>user_config.h</code>: From: <code>#define BIT_RATE_DEFAULT BIT_RATE_115200</code> To: <code>#define BIT_RATE_DEFAULT BIT_RATE_9600</code> </p> <p>And re-flash your ESP.</p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#42-compilation","title":"4.2. Compilation","text":"<p>Now everything is ready to build our first NodeMCU firmware: <pre><code>export PATH=/opt/nodemcu/esp-open-sdk/xtensa-lx106-elf/bin:$PATH\ncd /opt/nodemcu/nodemcu-firmware\nmake\n</code></pre> Our brand now firmware can be found here: <pre><code>nodemcu@nodemcu:/opt/nodemcu/nodemcu-firmware/bin$ ls -al\ntotal 392\ndrwxr-xr-x  2 nodemcu nodemcu   4096 Sep 23 14:02 .\ndrwxr-xr-x 15 nodemcu nodemcu   4096 Sep 23 13:58 ..\n-rw-r--r--  1 nodemcu nodemcu     79 Sep 23 09:54 .gitignore\n-rw-r--r--  1 nodemcu nodemcu  27808 Sep 23 14:02 0x00000.bin\n-rw-r--r--  1 nodemcu nodemcu 354899 Sep 23 14:02 0x10000.bin\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Compile_Nodemcu_Firmware/#references","title":"References","text":"<ul> <li>https://github.com/nodemcu/nodemcu-firmware</li> <li>http://nodemcu.readthedocs.io/en/master/en/build/#linux-build-environment</li> <li>https://nodemcu-build.com/</li> <li>http://www.esp8266.com/wiki/doku.php?id=toolchain</li> <li>https://github.com/pfalcon/esp-open-sdk</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/","title":"How To Unbrick Esp8266 blinking Blue Led","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#how-to-unbrick-esp8266-blinking-blue-led","title":"How To Unbrick ESP8266 (Blinking Blue Led)","text":""},{"location":"Old_Blog_Contents/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#tldr","title":"TL;DR","text":"<p>I have some ESP8266 (NodeMCU DEV kit, ESP-01 and ESP07). Two of them bricked during firmware upgrade (Flashing). The first was my mistake because I mistyped the memory address, and the other was a power outage. So I had two bricked ESP.  After I powered up the board, the blue led was blinking continuously and rapidly. I tried to update the firmware with custom builds and with builds from https://nodemcu-build.com/, but all my tries was unsuccessful.  I was a bit upset and was thinking about getting rid of them, but I never give up anything so easily. After doing some google search I found some articles about the memory map of the ESP, and some article on \"how to update boot loader\". Honestly, I'm not a developer, and I couldn't say everything I read was clear to me, but finally I successfully brought two ESP back from the death. :)  I'm writing this post just because I think it will be useful for someone, some time. I absolutely do not guarantee that this method will work in any cases, the only think I can suggest: do not give up trying, and googleing. :) And note that maybe your ESP has a different memory layout, so first do some search to find as many details about your ESP as you can.</p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#1-check-your-eps8266-symptoms","title":"1. Check your EPS8266 Symptoms","text":"<p>As I mentioned above the blue led on my esp was continuously blinking, when connected it to my computer. I used an USB to serial converter, and saw this message repeating endlessly:</p> <p><pre><code> ets Jan  8 2013,rst cause:2, boot mode:(3,6)\n\nload 0x40100000, len 25952, room 16 \ntail 0\nchksum 0x9a\nload 0x3ffe8000, len 2276, room 8 \ntail 12\nchksum 0x03\nho 0 tail 12 room 4\nload 0x3ffe88e4, len 8, room 12 \ntail 8\nchksum 0x3f\ncsum 0x3f\nrf cal sector: 251\nrf[112] \n</code></pre> I think this means that the ESP was restarting continuously, maybe because it couldn't find the boot loader or any of the necessary files. I could upload the firmware, so I think one of the following files must have been missing, corrupt or overwritten:</p> <ul> <li>blank.bin</li> <li>boot_v1.5.bin</li> <li>esp_init_data_default.bin</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#2-collect-the-necessary-files","title":"2. Collect The Necessary Files","text":"<p>The easiest way to do is download from this link. Or you can download the SDK from Gitub as well.</p> <p>If the links become broken or unavailable, you have two options: 1. Do some search (Google is your best friend. :)) 2. Compile the Nodemcu firmware (with SDK). To do this you can follow my previous post.</p> <p>I don't want to write the whole process again, so after you successfully compiled NodeMCU firmware on your own, you should have the necessary files.</p> <p>Actually you only have to compile \"esp-open-sdk\", the NodeMCU firmware isn't definitely needed to unbrick, if you have a pre-compiled firmware, you can use it.  NOTE: If you use some downloaded firmware instead of compiling one, please check which SDK was used for compiling!</p> <p>The files you will need can be located here: <pre><code>nodemcu@openhab:~/esp-open-sdk/ESP8266_NONOS_SDK_V1.5.4_16_05_20/bin$ find\n.\n./boot_v1.2.bin\n./boot_v1.5.bin\n./upgrade\n./esp_init_data_default.bin\n./at\n./at/512+512\n./at/512+512/user1.1024.new.2.bin\n./at/512+512/user2.1024.new.2.bin\n./at/README.md\n./at/1024+1024\n./at/1024+1024/user2.2048.new.5.bin\n./at/1024+1024/user1.2048.new.5.bin\n./at/noboot\n./at/noboot/eagle.irom0text.bin\n./at/noboot/eagle.flash.bin\n./blank.bin\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/How_To_Unbrick_Esp8266_blinking_Blue_Led/#3-flashing-the-esp","title":"3. Flashing the ESP","text":"<p>Before you flash the files to the ESP, double-check the size of its flash.  Based on the following tables [^1] upload the files to the appropriate  memory address.</p> <p></p> <p></p> <p>I used nodemcu-flasher to flash my ESP with these settings: </p> <ul> <li>blank.bin --&gt; 0x7E000</li> <li>esp_init_data_default.bin --&gt; 0x3FC000</li> <li>user1.1024.new.2.bin --&gt; 0x01000</li> <li>boot_v1.5.bin --&gt; 0x00000</li> </ul> <p>The last step is to flash your firmware, for example with a custom build one: <pre><code>nodemcu@openhab:~/nodemcu-firmware/bin$ ls -al\ntotal 444\ndrwxr-xr-x  2 nodemcu nodemcu   4096 Oct 24 18:26 .\ndrwxr-xr-x 15 nodemcu nodemcu   4096 Nov  1 11:28 ..\n-rw-r--r--  1 nodemcu nodemcu  28160 Nov  1 11:29 0x00000.bin\n-rw-r--r--  1 nodemcu nodemcu 413495 Nov  1 11:29 0x10000.bin\n-rw-r--r--  1 nodemcu nodemcu     79 Oct 24 18:20 .gitignore\n</code></pre></p> <p>I hope this post will be useful, and you will be able to unbrick your ESPs.</p> <p>REFERENCES:</p> <ul> <li>http://jasiek.me/2015/04/28/unbricking-an-esp8266-with-flashing-led.html</li> <li>http://www.electrodragon.com/w/ESP8266_AT_Commands</li> <li>https://nodemcu.readthedocs.io/en/master/en/flash/ (Upgrading Firmware)</li> <li>https://github.com/esp8266/esp8266-wiki/tree/master/sdk</li> <li>https://github.com/esp8266/esp8266-wiki</li> </ul> <p>[^1]: * https://espressif.com/sites/default/files/documentation/2a-esp8266-sdk_getting_started_guide_en.pdf</p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/","title":"Install OpenALPR on Raspberry PI 3","text":"<p>Caution</p> <p>This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>Warning</p> <p>It seems there  are some problems with using the latest Tesseract code base. For mor details please see the comments!</p> <p>Info</p> <p>I wrote a now post about this topic. \"Install OpenALPR on Raspberry PI 3 (Part 2)\"</p> <p>Before you start installing OpenALPR I suggest you to go through this and the mentioned post first. They may contain a lot of useful information. </p> <p>In this tutorial I will show how can you install OpenALPR on you Raspberry PI 3. From its home page:</p> <p>OpenALPR is an open source Automatic License Plate Recognition library written in C++ with bindings in C#, Java, Node.js, Go, and Python. The library analyzes images and video streams to identify license plates. The output is the text representation of any license plate characters.</p> <p>So after successfully installation of OpenALPR you Raspberry will be able to recognize License Plates from a single photo or from live stream.  Please note that in your country maybe illegal to use this tool on public or even for private use, therefore I use it only for my entertainment.</p> <p>OK. Lets Begin. :)</p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#what-is-needed","title":"What is needed?","text":"<ul> <li>Raspberry Image: 2016-05-27-raspbian-jessie.img https://www.raspberrypi.org/downloads/</li> <li>At least 8GB 16GB microSD card to flash the image.</li> <li>Raspberry PI 2 or 3 (I do not advise RPI 1 because I think it is too slow, and image processing will also be slow, and the compiling process will take much longer)</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#update-upgrade","title":"Update &amp; Upgrade","text":"<p>Before you start installing alpr and its dependencies update your Raspbian. I use a vanilla image so it is must to update.</p> <p>Comment</p> <p>I will do evry step with root access. (Without root access you are lost, or at least the install process will be much harder.) This Raspberry is only for this project thus I don't have to care about loosing anything, or installing packages which overrides other projects. </p> <p>Run the following commands: <pre><code>apt-get update \napt-get upgrade\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#install-the-necessary-packages","title":"Install the necessary packages","text":"<p>Previously I gathered all of the packages are needed to compile alpr and all of its dependencies. </p> <pre><code>apt-get install autoconf automake libtool\napt-get install libleptonica-dev\napt-get install libicu-dev libpango1.0-dev libcairo2-dev\napt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev\napt-get install python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev\napt-get install virtualenvwrapper\napt-get install liblog4cplus-dev\napt-get install libcurl4-openssl-dev\n</code></pre> <p>In one line:  </p> <pre><code>apt-get install autoconf automake libtool libleptonica-dev libicu-dev libpango1.0-dev \\\nlibcairo2-dev cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev \\\nlibswscale-dev python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev \\\nlibjasper-dev libdc1394-22-dev virtualenvwrapper liblog4cplus-dev libcurl4-openssl-dev\n</code></pre> <p>Based on another tutorial I know It has a chance that <code>apt-get isntall</code> will fail with \"no package found\". In this case you have to manually find the missing package using <code>apt-cache search ....</code>.  It may happen that in the meanwhile the package name or version has been changed therefore apt won't find it.</p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#dependencies","title":"Dependencies","text":"<p>I think this chapter will be the hardest to be done. Offical github Install documentation: https://github.com/openalpr/openalpr/wiki/Compilation-instructions-(Ubuntu-Linux) https://github.com/openalpr/openalpr</p> <p>OpenALPR requires the following additional libraries:</p> <ul> <li>Tesseract OCR v3.0.4</li> <li>OpenCV v2.4.8+</li> </ul> <p>And these have them own dependencies. :(</p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#tesseract-ocr","title":"Tesseract OCR","text":"<ul> <li>Download (clone) the package from git. Reference:  https://github.com/tesseract-ocr/tesseract/wiki/Compiling</li> </ul> <pre><code>cd /usr/local/src/\ngit clone https://github.com/tesseract-ocr/tesseract\n</code></pre> <p>If you want to use the exactly same version I used please checkout <code>3.05.00dev-380-g2660647</code>. Currently this is the master.</p> <ul> <li> <p>Follow these steps: <pre><code>cd /usr/local/src/tesseract\n./autogen.sh \n</code></pre></p> </li> <li> <p>If you are lucky you will get this message: <pre><code>All done.\nTo build the software now, do something like:\n\n$ ./configure [--enable-debug] [...other options]\n</code></pre></p> </li> <li> <p>Next step is configure the package, as suggested in the message above run <code>./configure</code>. :) If you want to know what \"other options\" are available first run <code>./configure --help</code>. Now I don't want to override the default configuration. If you compile without root access or you want to specify the install location please use this option:</p> </li> </ul> <pre><code>Installation directories:\n  --prefix=PREFIX         install architecture-independent files in PREFIX\n                          [/usr/local]\n</code></pre> <p>As you can see by default Tesseract will be installed in <code>/usr/local/</code>. It is OK for me.</p> <p>After configuring run: <code>make</code> - It will take long time.  If you want make it faster use <code>-j2</code> option or if you are brave enough <code>-j4</code>. :)  <pre><code>-j [jobs], --jobs[=jobs]\n            Specifies the number of jobs (commands) to run simultaneously.  If there is more than one -j option, the last one is effective.  If the -j option is given without  an  argu\u00e2\u20ac\u0090\n            ment, make will not limit the number of jobs that can run simultaneously.\n</code></pre></p> <p>I have tried with <code>-j4</code> but it leads to \"segmentation fault\". So I advise you to run max 2 jobs simultaneously.</p> <p>Finish the install with <code>make install</code> command.</p> <p>You can check if the compilation was successfully or not by:</p> <p><pre><code>root@raspberrypi:/usr/local/src/tesseract# tesseract \ntesseract: error while loading shared libraries: libtesseract.so.3: cannot open shared object file: No such file or directory\n</code></pre> If you get the error below try to run <code>ldconfig</code>.</p> <p>Now you can check again: <pre><code>root@raspberrypi:/usr/local/src/tesseract# tesseract -v\ntesseract 3.05.00dev\n leptonica-1.71\n  libgif 4.1.6(?) : libjpeg 6b : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8 : libwebp 0.4.1 : libopenjp2 2.1.0\n</code></pre></p> <p>Optionally you can install tesseract training: <pre><code>make training\nsudo make training-install\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#opencv","title":"OpenCV","text":"<ul> <li>Home page</li> <li>Download page</li> <li>Current latest version</li> <li>Install Documentation</li> </ul> <p>Pre-Steps:</p> <ul> <li>Download the latest version.</li> </ul> <pre><code>cd /usr/local/src\nwget https://github.com/Itseez/opencv/archive/2.4.13.zip\nmv 2.4.13.zip OpenCV-2.4.13.zip\n</code></pre> <ul> <li>Unzip it</li> </ul> <p><code>unzip -q  OpenCV-2.4.13.zip</code> This will create a directory: <code>/usr/local/src/opencv-2.4.13</code></p> <ul> <li>Run cmake </li> </ul> <pre><code>cd /usr/local/src/opencv-2.4.13\nmkdir release\ncd release\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D BUILD_NEW_PYTHON_SUPPORT=ON -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON  -D BUILD_EXAMPLES=ON ..\n</code></pre> <p>This comamnd will configure the project. If you are lucky again you will get this message:</p> <pre><code>-- Configuring done\n-- Generating done\n-- Build files have been written to: /usr/local/src/opencv-2.4.13/release\n</code></pre> <ul> <li>Run make</li> </ul> <p>I strongly recommend to use <code>-j2</code> option, because this step takes the most of time: <code>root@raspberrypi:/usr/local/src/opencv-2.4.13/release# make -j2</code></p> <p>Unfortunately at the first time my <code>make</code> command died with this message:</p> <pre><code>[ 47%] Building CXX object modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o\nc++: internal compiler error: Segmentation fault (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee &lt;file:///usr/share/doc/gcc-4.9/README.Bugs&gt; for instructions.\nmodules/ocl/CMakeFiles/opencv_ocl.dir/build.make:719: recipe for target 'modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o' failed\nmake[2]: *** [modules/ocl/CMakeFiles/opencv_ocl.dir/src/cl_runtime/clamdfft_runtime.cpp.o] Error 4\nCMakeFiles/Makefile2:4734: recipe for target 'modules/ocl/CMakeFiles/opencv_ocl.dir/all' failed\nmake[1]: *** [modules/ocl/CMakeFiles/opencv_ocl.dir/all] Error 2\nmake[1]: *** Waiting for unfinished jobs....\n</code></pre> <p>As I wrote above you may get this error. It may be caused by because you run out of memory. I don't know exact solution for this issue on Raspberry, but I have some suggestion:</p> <ul> <li>Reboot you RPI</li> <li>Try not to specify <code>-j</code> option or use <code>-j1</code></li> <li>Run <code>make clean</code> and <code>make</code> again</li> <li>Increase swap space.</li> </ul> <pre><code>fallocate --length 2GiB /root/2G.swap\nchmod 0600 /root/2G.swap\nmkswap /root/2G.swap \nswapon /root/2G.swap \n</code></pre> <p>After 'n' (re)tries <code>make -j2</code> command finished successfully. The next (and final) step with OpenCV is run: <code>make install</code></p>"},{"location":"Old_Blog_Contents/nodemcu/Install_Openalpr_On_Raspberry_Pi_3/#install-openalpr","title":"Install OpenALPR","text":"<p>Finally we can continue with OpenALPR. :)</p> <ul> <li>Clone the git repository:</li> </ul> <pre><code>cd /usr/local/src\ngit clone https://github.com/openalpr/openalpr.git\n</code></pre> <ul> <li>(optional) Check version</li> </ul> <p><pre><code>cd openalpr\ngit describe --tags\nv2.1.0-513-gcd2aab0\n</code></pre> Now this is the master branch. If the master branch is not this version, you can checkout v2.1.0 if you want to use exact same version. <code>git checkout v2.1.0</code></p> <ul> <li>Run cmake</li> </ul> <pre><code>root@raspberrypi:/usr/local/src/openalpr# cd src/\nroot@raspberrypi:/usr/local/src/openalpr/src# cmake ./\n</code></pre> <ul> <li>run make</li> <li><code>root@raspberrypi:/usr/local/src/openalpr/src# make</code> The situation is the same, if <code>make</code> fails try to follow the steps are described earlier.</li> </ul> <p>Please remember to run <code>ldconfig</code>.</p> <p>Now you can see that compiling OpenALPR is much easier than installing its dependencies. And please note that I have gathered the necessary packages. At the very first time I installed these packages It took 6 or more hours. Despite the lot of good articles there were a lot of dependencies I had to find manually. And first time I tried to install OpenCV and Tesseract into a custom directories for example <code>/opt/OpenCV</code> and <code>/opt/Tesseract</code>. If you try this you have to manually define these libraries to OpenALPR in CMakeLists.txt.  Just for demonstration I tried these settings:</p> <pre><code>SET(OpenCV_DIR \"/opt/opencv-2.4.13/share/OpenCV/\")\nSET(Tesseract_DIR \"/usr/src/tesseract\")\nSET(Tesseract_LIB \"/opt/tesseract/lib/\")\nSET(Tesseract_INCLUDE_DIRS \"/opt/tesseract/include/\")\nSET(Tesseract_INCLUDE_BASEAPI_DIR \"/opt/tesseract/include\")\nSET(Tesseract_PKGCONF_INCLUDE_DIRS \"/opt/tesseract/include/tesseract\")\n</code></pre> <p>But for some reason <code>make</code> always failed because of tesseract. After some hours I was fed up with it and installed tesseract to its default location. I had to recompile only tesseract to successfully compile OpenALPR, OpenCV remained in <code>/opt/</code>. In this case this line is must inserted to CMakeLists.txt: <code>SET(OpenCV_DIR \"/opt/opencv-2.4.13/share/OpenCV/\")</code>.</p> <p>Ok. Test the newly install alpr system. <pre><code>cd /usr/local/src/openalpr/src\nroot@raspberrypi:/usr/local/src# alpr ea7the.jpg \nplate0: 10 results\n    - EA7THE     confidence: 91.0578\n    - EA7TBE     confidence: 84.133\n    - EA7T8E     confidence: 83.0083\n    - EA7TRE     confidence: 82.7869\n    - EA7TE  confidence: 82.5961\n    - EA7TME     confidence: 80.2908\n    - EA7TH6     confidence: 77.0045\n    - EA7THB     confidence: 75.5779\n    - EA7TH  confidence: 74.6576\n    - EA7TB6     confidence: 70.0797\n</code></pre></p> <p>Wow. It is working. :) You can find configuration examples in <code>/usr/local/share/openalpr/config</code> directory.</p> <p>REFERENCES: </p> <ul> <li>http://www.pyimagesearch.com/2015/02/23/install-opencv-and-python-on-your-raspberry-pi-2-and-b/</li> <li>http://docs.opencv.org/2.4/doc/tutorials/introduction/linux_install/linux_install.html#linux-installation</li> <li>https://virtualenv.pypa.io/en/stable/</li> <li>https://gist.github.com/amstanley/9da7febc9a3e3c2228ee</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/","title":"Logging Mqtt Data (subscription) To Mysql With Shell Script","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#logging-mqtt-data-subscription-to-mysql-with-shell-script","title":"Logging MQTT data (subscription) to MySQL with Shell Script","text":"<p>I have a several ESPs which are continuously logging humidity and temperature values. I've decided to save all data to a Mysql database for later use or analysis.  I found two different way to do this:</p> <ul> <li>OpenHAB persistence</li> <li>Shell script</li> </ul> <p>Both have advantages and disadvantages as well, but you can use both at the same time, and after a while you can choose he best for you. Or you can use NodeRed  to logging data to DB if you don't like my solutions.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#0-my-data-model","title":"0. My Data Model","text":"<p>All of my ESPs are sending data to a mqtt broker (mosquitto) using this topic format: <code>\"NodeMCU/[NODEID]/status/[MEASSURE]\"</code> and the data (value).</p> <p>Here is an example about what kind of messages are sent by one of my ESP:</p> <pre><code>NodeMCU/585548/status/nodeid 585548\nNodeMCU/585548/status/contacts/5 0\nNodeMCU/585548/status/humidity 58\nNodeMCU/585548/status/sta_macaddr 60:01:94:08:ef:4c\nNodeMCU/585548/status/contacts/1 0\nNodeMCU/585548/status/temperature -1.5\nNodeMCU/585548/status/ap_macaddr 62:01:94:08:ef:4c\nNodeMCU/585548/status/reboot 168457\nNodeMCU/585548/status/uptime 1485351907\nNodeMCU/585548/status/heap 19376\nNodeMCU/585548/status/ipaddr 172.31.0.168\nNodeMCU/585548/status/epoch 1485520364\nNodeMCU/585548/status/rssi -57\nNodeMCU/585548/status/voltage 3.531\nNodeMCU/585548/status/publicip 46.1*7.*3.15*\n</code></pre> <p>How do I collect the data? <pre><code>    function module.collectData()\n        -- GENERAL STATUS UPDATE\n        local table_status = {\n            [\"nodeid\"] =  node.chipid(),\n            [\"sta_macaddr\"] = wifi.sta.getmac(),\n            [\"ap_macaddr\"] = wifi.ap.getmac(),\n            [\"ipaddr\"] = wifi.sta.getip(),\n            [\"rssi\"] = wifi.sta.getrssi(),\n            [\"epoch\"] = rtctime.get(),\n            [\"reboot\"] = tmr.time(),\n            [\"uptime\"] =  rtctime.get() - tmr.time(),\n            [\"publicip\"] = ipaddr,\n            [\"heap\"] = node.heap(),\n            [\"voltage\"] = adc.readvdd33(0)/1000, \n        } \n\n        -- ############# (optional) - DHT\n        local status, temperature, humidity, temp_dec, humi_dec = dht.read(config.dhtPins)\n        table_status[\"temperature\"]=temperature\n        table_status[\"humidity\"]=humidity\n        return table_status\n     end -- End of collectData \n</code></pre></p> <p>How do I publish these data? <pre><code>    function module.publishData(mqtt,toPublishTable)\n      -- PUBLISH DATA\n      for st,va in pairs(toPublishTable) \n      do \n          m:publish(config.mqtt.publishTopicStatus..st,va,0,1) \n      end \n    end\n</code></pre> I call this function (publishData()) by using a timer  in every 60 secs.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#1-openhab-persistence","title":"1. OpenHAB persistence","text":"<p>OpenHAB supports saving data to MySQL database by setting up the mysql persistence.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#11-turn-on-mysql-persistance","title":"1.1. Turn on MySQL persistance","text":"<ul> <li> <p>Unzip <code>org.openhab.persistence.mysql-1.8.3.jar</code> to your addons directory. In my case: <code>/opt/openhab/runtime/distribution-1.8.3-runtime/addons</code> You can download all addons from the OpenHAB official web page:  OpenHAB downloads</p> </li> <li> <p>Edit <code>openhab.cfg</code> file. Set the following properties:</p> </li> </ul> <p><pre><code>persistence:default=mysql\nmysql:url=jdbc:mysql://172.18.0.105:3306/openhab\nmysql:user=openhab\nmysql:password=openhab\nmysql:localtime=true\n</code></pre> OpenHAB will create all necessary table.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#12-configuration","title":"1.2. Configuration","text":"<p>Create configuration file for MySQL persistence: <code>configurations/persistence/mysql.persist</code></p> <p>Example: <pre><code>Strategies {\n        default = everyChange\n}\nItems {\n        Temperatures* -&gt; \"Temperatures\"\n        Humidities* -&gt; \"Humidities\"\n        prd_batt_349307_voltage -&gt; \"Battery Voltage\"\n}\n</code></pre></p> <p>For the better understand here is my Temperatures and Humidities group config and \"prd_batt_349307_voltage\" configuration:</p> <p>Temperature: <code>Number  prd_347920_temp              \"Shaft Temperature [%.1f \u00c3\u201a\u00c2\u00b0C]\"     &lt;temperature&gt;   (GroupShed,Temperatures) {mqtt=\"&lt;[banana:NodeMCU/347920/status/temperature:state:default\"], autoupdate=\"true\" }</code></p> <p>Humidity: <code>Number  prd_347920_humi              \"Shaft Humidity [%.1f %%]\"        &lt;humi&gt;          (GroupShed,Humidities)   {mqtt=\"&lt;[banana:NodeMCU/347920/status/humidity:state:default\"], autoupdate=\"true\" }</code></p> <p>prd_batt_349307_voltage: This is a battery powered DHT22 sensor with ESP01. <code>Number  prd_batt_349307_voltage           \"Batt Voltage: [%.1f mV]\"        &lt;info&gt;              (BattDHT_1)              {mqtt=\"&lt;[banana:NodeMCU/349307/status/voltage:state:default\"], autoupdate=\"true\" }</code> </p> <p>OpenHAB will log all temperature and humidity values to the MySQL database when the value has changed.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#13-the-database-and-tables","title":"1.3. The Database and Tables","text":"<p>OpenHAB creates a table for all logged items, and there is another table which contains the ID and the name of the items:</p> <p><pre><code>mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e  \"show tables;\" openhab\nItem1\nItem2\nItem3\n...\n...\nItems\n</code></pre> Items table: <pre><code>mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e  \"select * from Items;\" openhab\n1       prd_81425_humi\n2       prd_80100_humi\n3       test_384849_humi\n4       prd_batt_349307_temp\n...\n...\n...\n</code></pre></p> <p>For example the values of</p> <ul> <li><code>prd_batt_349307_temp</code> can be queried from the Item4 table.</li> <li><code>test_384849_humi</code> can be found in the Item3 table.</li> </ul> <p>You can query the minimum temperature since 2017.01.25: <pre><code>mysql -h 172.18.0.105 -u openhab -popenhab  -e  \"select min(value) from Item4 where Time &gt;= '2017-01-25';\" openhab\n+------------+\n| min(value) |\n+------------+\n|       18.1 |\n+------------+\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#14-remove-incorrect-data-from-the-database","title":"1.4. Remove incorrect data from the database","text":"<p>Unfortunately sometimes the temperature and humidity values are incorrect. This means a very low or very high values (&gt;100 ; &lt;-100), so I remove these entries from the database with a simple shell script which runs on every hour from crontab:</p> <pre><code>#!/bin/bash\nIFS='\n'\n\nLOG=\"/opt/openhab/custom_scripts/clean_logs/logfile.log\"\n\nexec &gt;&gt; $LOG 2&gt;&amp;1\n\necho \"#################### $( date +%F\\ %T ) ####################\"\nfor TABLE in $( mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e  'show tables;' openhab | grep -v Items )\ndo\n  ITEMID=$( echo $TABLE  | sed 's#[^0-9]##g' )\n  ITEM_NAME=$( mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e  \"select ItemName from Items where ItemId=$ITEMID;\" openhab )\n  echo \"---------------- $TABLE\"\n  echo \"Item: $ITEM_NAME\"\n  echo \"$ITEM_NAME\" | egrep -q '(temp|humi)'\n  TO_CLEAN=$?\n\n  if [ $TO_CLEAN -eq 0 ]\n  then\n    mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e  \"select * from $TABLE where abs(Value)  &gt; 100 ;\" openhab\n    mysql -h 172.18.0.105 -u openhab -popenhab -s -N -e  \"delete from $TABLE where abs(Value)  &gt; 100 ;\" openhab\n  else\n    echo \"NO Humi or temp\"\n  fi\n  echo\ndone\n</code></pre> <p>OpenHAB persistence can work with all bindings, not only for MQTT. So if you are already using OpneHAB this method maybe the most suitable for you.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#2-shell-bash-script","title":"2. Shell (bash) Script","text":"<p>My another solution to log mqtt data to MySQL data is writing a simple shell script which subscribe to one or more topics, and INSERT data to the DB right after the message is received from the MQTT broker.</p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#21-create-the-database","title":"2.1. Create The Database","text":"<p>Unfortunately nobody will create the database and tables for you, so you have to do this on your own. You are lucky because I share mine with you.</p> <p>Create database <pre><code>CREATE DATABASE IF NOT EXISTS `nodemcu` DEFAULT CHARACTER SET latin1 COLLATE latin1_swedish_ci;\n-- Optional:\nUSE `nodemcu`;\n</code></pre></p> <p>Create table <pre><code>DROP TABLE IF EXISTS `esps`;\nCREATE TABLE IF NOT EXISTS `esps` (\n`_id` int(11) NOT NULL,\n  `timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\n  `nodeid` int(11) NOT NULL,\n  `measure` text NOT NULL,\n  `value` float NOT NULL,\n  `comment` text NOT NULL\n) ENGINE=InnoDB AUTO_INCREMENT=49334 DEFAULT CHARSET=latin1;\n</code></pre></p> <p>Add indexes: <pre><code>ALTER TABLE `esps`\n ADD PRIMARY KEY (`_id`), ADD KEY `nodeid` (`nodeid`), ADD KEY `value` (`value`);\n</code></pre></p> <p>Create user and GRANT accees: <pre><code>CREATE USER 'nodemcu'@'%' IDENTIFIED BY 'nodemcu';\n\nGRANT USAGE ON *.* TO 'nodemcu'@'%' IDENTIFIED BY 'nodemcu' \n  WITH MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 \n  MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0;\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/Logging_Mqtt_Data_%28subscription%29_To_Mysql_With_Shell_Script/#22-shell-script","title":"2.2. Shell Script","text":"<pre><code>#!/bin/bash\nIFS='\n'\n\nmosquitto_sub -R -v -h 172.16.0.250 -u vinyo -P *****  -t 'NodeMCU/+/status/temperature'  -t 'NodeMCU/+/status/humidity' -t 'NodeMCU/+/status/voltage' | while read RAW_DATA\ndo\nNODEID=$( echo $RAW_DATA | cut -f 2 -d\"/\" )\nMEASURE=$( echo $RAW_DATA | cut -f 4 -d\"/\" | cut -f1 -d\" \" )\nVALUE=$( echo $RAW_DATA | cut -f 2 -d\" \" )\n\nLAST_VALUE=$( mysql -h 172.18.0.105 -u nodemcu -pnodemcu -N -s -e \"select value from esps where nodeid='$NODEID' and measure='$MEASURE' order by _id DESC LIMIT 1;\" nodemcu )\n\n[ -z $LAST_VALUE ] &amp;&amp; LAST_VALUE=0\n\nif [ $LAST_VALUE != $VALUE ]\nthen\necho \"INSERT (NodeId: $NODEID; MEASURE: $MEASURE ( $LAST_VALUE --&gt; $VALUE )\"\nmysql -h 172.18.0.105 -u nodemcu -pnodemcu -e \"insert into esps(nodeid,measure,value) VALUES('$NODEID','$MEASURE','$VALUE');\" nodemcu\nelse\necho \"Not Changed: (NodeId: $NODEID; MEASURE: $MEASURE ( $LAST_VALUE --&gt; $VALUE )\"\nfi\n\ndone\n</code></pre> <p>Explanation: </p> <ul> <li>The script inserts data only when it differs from the previous value ($LAST_VALUE). It is important because ESPs send messages very frequently, and without this the db would grow fast.</li> <li>At the first start (when there is no data in the db) 0 will be used as the <code>$LAST_VALUE</code>. <code>[ -z $LAST_VALUE ] &amp;&amp; LAST_VALUE=0</code> Without this, at the first start the \"if\" statement would run into an error.</li> <li>It logs \"only\" the temperature, humidity and voltage values by subscripting these topics: <code>mosquitto_sub -R -v -h 172.16.0.250 -u v*n*y*a -P *****  -t 'NodeMCU/+/status/temperature'  -t 'NodeMCU/+/status/humidity' -t 'NodeMCU/+/status/voltage'</code></li> <li>This scripts log to the standard output. Of course you can redirect all output to a log file by putting the following line to script (before the mosquitto_sub command). <code>exec &gt;&gt; /path/of/the/log/file 2&gt;&amp;1</code></li> </ul> <p>How to run in the background? Simply use the well-known method: nohup + command + $  <code>nohup ./script-name-sh &amp;</code></p>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/","title":"Reliable MQTT conenction with NodeMCU","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#reliable-mqtt-conenction-with-nodemcu","title":"Reliable MQTT conenction with NodeMCU","text":""},{"location":"Old_Blog_Contents/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#tldr","title":"TL;DR","text":"<p>I want to use ESP modules for my brand new project: **H**ome **A**utomatization **W**ith **O**penHAB (HAWO). :) To realise this I want to connect some ESP module to my OpenHAB server using MQTT broker (mosquitto). Building a complete Smart Home is not my goal (at least not now), I only want to control some lights / stuff with my phone (or tablet) and place some DHT22 (Temperature and Humidity) sensors in my house, workshop and garden. I started working with NodeMCU some months ago and I ran into several problems and bugs during my coding. I cannot write all my experience to this post, but I hope this will be helpful for you. :)</p> <p>Steps to be done before you can connect to mqtt broker:</p> <ul> <li>Install and configure mosquitto mqtt broker </li> <li>Connect ESP8266 module to your Wi-fi network  </li> </ul> <p>Maybe later I will write a posts about these steps, but now I want to give more details only about mqtt connection in this thread.  So I need a reliable connection to my mqtt broker which can handle network or other errors. Unfortunately for some reason mqtt module can not re-connect to broker for example if Wi-fi disconnect and reconnect.  To better understand here is a basic example to connect to the broker.</p> <p><pre><code>m = mqtt.Client(\"ClienID\", 60, \"test\", \"test123\")\nm:connect(\"192.168.10.10\", 1883, 0, \n  function(client) \n     print(\"connected\") \n   end, \n function(client, reason) \n  print(\"failed reason: \"..reason) \nend)\n</code></pre> When you specify <code>mqtt.client:connect()</code> you have on option to turn on/off auto-reconnect.  <code>mqtt:connect(host[, port[, secure[, autoreconnect]]][, function(client)[, function(client, reason)]])</code></p>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#what-kind-of-problem-can-occurr","title":"What kind of problem can occurr?","text":"<ul> <li>Wi-fi disconnection </li> <li>MQTT broker become unavailable or is being restarted.</li> <li>Other network issues (eg.: DNS error)</li> <li>Misconfiguration: bad username, password, etc.</li> </ul> <p>My biggest problem is that the connection between ESP and the broker cannot be tested in any way.  Based on my experiences I ran into these problems:</p> <ul> <li>Using autoreconnect true</li> <li>If I restart mosquitto the clients reconnect to it successfully. </li> <li>But if I disconnect and connect to Wi-fi, the clients can not reconnect. This is a big problem because you can not reconnect to the broker. If you try <code>mqtt.client:connect()</code> again ESP will give you \"Already Connected\" error message. If you try firstly <code>mqtt.client:close()</code> the ESP will be restarted. I do not know if this behavior is a bug or a feature but it is really annoying.</li> </ul> <p>I tried to check if the connection is established or not with this <code>if</code> condition: <pre><code>if  m:publish(config.mqttLwtTopic,\"Active\",0,1)\n  then\n    DO SOMETHING\n  else\n    DO SOMETHING ELSE\nend\n</code></pre></p> <p>But if you use autoreconnect=true it will return with true in any case, regardless whether  the message has been delivered or not. </p> <ul> <li>Using autoreconnect false It can be a good option but in this case we have to check the connection manually, and reconnect if something happens. </li> </ul> <p>My final solution is two functions and a timer combined with each other: <pre><code>local function checkLwt()\n    if  m:publish(config.mqttLwtTopic,\"Active\",0,1)\n        then\n            return true\n        else\n            return false\n    end\nend\n\nlocal function connectReconnect()\n    m:connect(config.mqttHost, config.mqttPort, 0, 0,\n    function(client) \n            print(\"MQTT connected to: \"..config.mqttHost) \n            subsribe()\n            onMessage()\n            tmr.start(config.mqttCheckTimerId)\n    end, \n    function(client, reason) \n            print(\"failed reason: \"..reason) \n            print(\"Sleep for 10 secs\")\n            tmr.alarm(6,10000,tmr.ALARM_SINGLE, function()\n                connectReconnect()\n            end)\n\n\n    end)\nend\ntmr.register(config.mqttCheckTimerId,config.mqttTmrDelay, tmr.ALARM_AUTO,\nfunction()\n        local status, err = pcall(checkLwt)\n        if status == true and err == true\n        then\n            print(\"LWT OK\")\n            updateStatus()\n        elseif status == true and err == false\n        then\n            print(\"LWT FAILED\")\n            connectReconnect()\n        end\n        if status == false and err ~= true\n        then\n                print(\"LWT faild with notconnected...\")\n                tmr.stop(config.mqttCheckTimerId)\n        end\n\n\nend)\n</code></pre></p>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#checklwt","title":"checkLwt()","text":"<p>There are three scenarios:</p> <ul> <li>Message is successfully delivered. Return true.</li> <li>Message is failed to deliver. Return false.</li> <li>And the last one is the worst. :( If this function is called while the client is trying to connect to the broker it will fail with \"Not Connected\" exception and ESP will be restarted. That's why I use <code>pcall</code> (Protected Call) in the timer.</li> </ul> <p><pre><code>local status, err = pcall(checkLwt)\nif status == true and err == true\n...\n</code></pre> The <code>status</code> is true when the function returns without exception. It prevents the ESP from restarting if the function throws an exception (Not Connected). Status can be true or false. The <code>err</code> can be <code>true</code> or the error message of the exception. Based on that 3 scenario exists:</p> <ul> <li><code>status == true and err == true</code>   The function successfully finished. Everything is OK.</li> <li><code>status == true and err == false</code> In this situation probably there is something with mqtt server. For example it stopped or is unreachable.</li> <li><code>status == false and err ~= true</code> Please not that I use \"not equal\" <code>~=</code> because err can be an error message as well.  So in this situation it is likely that we get \"not connected\" exception, and maybe something happened with the Wi-fi connection.</li> <li>bonus: <code>status == false and err == true</code> O.K. This is not possible. If status is false it means that we got an exception thus err could not be true.</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_MQTT_conenction_with_NodeMCU/#connectreconnect-tmr","title":"connectReconnect() &amp; tmr()","text":"<p>This is the main part of my code. What will happen when connectReconnect function is called?</p> <ol> <li>If the connection to the broker is successful</li> <li>the timer will be started</li> <li>and some other functions will be called. </li> <li>If the client failed to connect to the broker</li> <li>Wait 10 secs and the function will call itself. As you can see this is a recursive function which continuously call itself until the connection is established. The timer check whether the publication is working or not. Why  is it necessary to use 2 different error conditions? If the checkLwt fails, we have to know why, because if it fails due to the broker unavailability, callback events of <code>mqtt.client:connect()</code> won't work, neither the success nor the failed function will run.</li> </ol> <p>At first <code>status == true and err == false</code> condition will be always true, and connectReconnect() function will be called. From that point there are two error case:</p> <ul> <li>There is some network error, for example \"DNS failure!\". In this case the function will call itself. BUT! The timer is still running and will run into the <code>status == false and err ~= true</code> condition (because of \"Not Connected\" exception). We have to stop the timer because it is unnecessary.  The recursive function will call itself until the connection is not ready, and if we call <code>mqtt.client:connect()</code> multiple times, we will get \"Already Connected\" exception.</li> <li>The broker is shut down: <code>status == true and err == false</code> As I mentioned in this case the callback event of  <code>mqtt.client:connect()</code> won't work that's why we have to call connectReconnect() until the connection is not ready. </li> </ul> <p>I've created a table for the better understanding of <code>status</code> and <code>err</code>:</p> <p></p> <p>Two more very important things:</p> <ul> <li>Call connectReconnect() function after the network connection is successfully established. </li> <li>Register the \"offline\" callback event: <pre><code>m:on(\"offline\", function(con) \n    print (\"Offline\") \n    setup.resetRelay()\n    tmr.start(config.mqttCheckTimerId)\nend) \n</code></pre></li> </ul> <p>NOTE Always start the connection procedure by starting the timer, not by calling conenctReconnect() function. (<code>tmr.start(config.mqttCheckTimerId)</code>)!!!  Why? Because if you are connected to Wi-fi network and call connectReconnect() function, but MQTT broker is unavailable the timer never will be started, because <code>mqtt.client:connect()</code> will fail and neither the true nor the false function will be called (because of previously mentioned bug).</p> <p>Summary:</p> <p>Maybe this is not the best solution but I failed to find better one. I tried uncountable   variations of functions and timers and their combinations. The most important for me that the connection has to be reliable, and in case of any error ESP has to be reconnected to the broker. Maybe when the mentioned bugs will be fixed in the future, it will be enough to use only the reconnect=true option.</p> <p>You can download my full example from this link.</p> <p>References:</p> <ul> <li>https://docs.coronalabs.com/api/library/index.html</li> <li>https://nodemcu.readthedocs.io/en/master/</li> <li>https://www.lua.org/pil/8.5.html</li> </ul> <p>UPDATE: Reliable MQTT connection with NodeMCU (part 2)</p>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_Mqtt_Connection_With_Nodemcu_%28part_2%29/","title":"Reliable Mqtt Connection With Nodemcu (part 2)","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/nodemcu/Reliable_Mqtt_Connection_With_Nodemcu_%28part_2%29/#reliable-mqtt-connection-with-nodemcu-part-2","title":"Reliable MQTT connection with NodeMCU (part 2)","text":"<p>Thanks to \"Modestas\" post on my previous article  now I will show you another solution to this topic, which is much easier to understand and simpler.</p> <ul> <li>First we will create a config file like this: <pre><code>--[[\nFile Name: config.lua\n]]\nlocal module = {}\n    module.NodeID=node.chipid()\n\n    -- mqtt Related Config    \n    module.mqttHost=\"172.16.0.***\"\n    module.mqttPort=\"1883\"\n    module.mqttUserName=\"**************\"\n    module.mqttpassword=\"**************\"\n    module.mqttLwtTopic=\"NodeMCU/lwt/\"..module.NodeID\n\n    module.mqttSubscibeTopics={[\"NodeMCU/\"..module.NodeID..\"/command\"]=0,[\"NodeMCU/\"..module.NodeID..\"/relayCh/+\"]=0}\n    module.mqttPublishTopicStatus=\"NodeMCU/\"..module.NodeID..\"/status/\"\n    module.mqttUpdateStatusInterval=60000\n    module.mqttUpdateStatusTimerId=2\nreturn module\n</code></pre></li> </ul> <p>You can use <code>table</code> object to store these parameter/value pairs instead of this module. Maybe later I will write a post about 'how to do that'.</p> <ul> <li> <p>Create MQTT client and set LWT <pre><code>m=mqtt.Client(config.NodeID, 10, config.mqttUserName, config.mqttpassword)\nm:lwt(config.mqttLwtTopic, \"Inactive\", 0,1)\n</code></pre> And do not forget to set <code>isMqttAlive</code> value to <code>false</code>. We will use this variable to determine if mqtt connection is alive or not. The initial value must be false, because at the first run the ESP isn't connected to the broker.</p> </li> <li> <p>connectToMqtt() Function</p> </li> </ul> <pre><code>function connectToMqtt()\n    m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) \n        isMqttAlive = true \n        print(\"Successfully Conencted to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort)\n\n-- Here you can do some useful things. Example subscribe to a topic, or set up what should happen if a message is received. (`mqtt.client:on()`)\n\n    end,\n    function(con,reason)\n        print(\"Faild to connect to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort..\", Reason: \"..reason)\n    isMqttAlive = false\n    end)\nend\n</code></pre> <p>This function will be used to connect to the MQTT broker, and the <code>isMqttAlive</code> variable is also will be set here: -If the ESP is successfully connected to the broker, <code>isMqttAlive</code> will be true. -If something goes wrong, this variable will be false.  </p> <ul> <li> <p>Set up mqtt.client:on(\"offline\"....) <pre><code>m:on(\"offline\", function(con) \n    isMqttAlive = false \n    print(\"Disconnected from MQTT\")\nend)\n</code></pre> If the ESP disconnects from the broker this will set <code>isMqttAlive</code> to false.</p> </li> <li> <p>Final steps</p> </li> </ul> <p>After you call <code>connectToMqtt()</code> function you have to check the value of <code>isMqttAlive</code> before each message publication.</p> <p>In my case I use a DHT22 sensor to monitor temperature and humidity, and I send update every 10 minutes. To do this a timer should be used. Example:</p> <pre><code>tmr.alarm(config.mqttUpdateStatusTimerId,config.mqttUpdateStatusInterval, tmr.ALARM_AUTO, function()\nif isMqttAlive == false\n    then\n        print(\"Reconnect To MQTT:\"..config.mqttHost..\" on port: \"..config.mqttPort)\n        connectToMqtt()\n    else\n        print(\"MQTT is OK: \"..config.mqttHost..\" on port: \"..config.mqttPort)\n        mqttUpdateGeneralStatus(m)\n    end\nend)\n</code></pre> <p>As you can see, first,  I check if <code>isMqttAlive</code> is ture or false. - If it is false I call <code>connectToMqtt()</code> function to (re)connect to the broker. - If it is true the <code>mqttUpdateGeneralStatus(m)</code> function will be called, which queries the sensor and send the actual temperature and humidity values to the broker.</p> <ul> <li>A Complete Example</li> </ul> <p><pre><code>local config=require(\"config\")\nlocal setup=require(\"setup\")\nlocal mqttSP=require(\"mqttSP\")\nm=mqtt.Client(config.NodeID, 10, config.mqttUserName, config.mqttpassword)\nm:lwt(config.mqttLwtTopic, \"Inactive\", 0,1)\n\n\nisMqttAlive=false\n\n-- Conenct Function\nfunction connectToMqtt()\n    m:connect(config.mqttHost, config.mqttPort, 0, 0, function(client) \n        isMqttAlive = true \n        print(\"Successfully Conencted to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort)\n        mqttSP.mqttSubsribe(m,config.mqttSubscibeTopics)\n        mqttSP.mqttOnMessage(m)\n        m:publish(config.mqttLwtTopic,\"Active\",0,1)\n        m:on(\"offline\", function(con) \n           isMqttAlive = false \n           print(\"Disconnected from MQTT\")\n        end)\n    end,\n    function(con,reason)\n        print(\"Faild to connect to MQTT broker: \"..config.mqttHost..\" on port: \"..config.mqttPort..\", Reason: \"..reason)\n    end)\nend\n\n-- Connect To Broker\nconnectToMqtt()\n\n-- Start Timer\ntmr.alarm(config.mqttUpdateStatusTimerId,config.mqttUpdateStatusInterval, tmr.ALARM_AUTO, function()\nif isMqttAlive == false\n    then\n        print(\"Reconnect To MQTT:\"..config.mqttHost..\" on port: \"..config.mqttPort)\n        connectToMqtt()\n    else\n        print(\"MQTT is OK: \"..config.mqttHost..\" on port: \"..config.mqttPort)\n        mqttSP.mqttUpdateGeneralStatus(m,setup.getPublicIp())\n    end\nend)\n</code></pre> REFERENCES:</p> <ul> <li>https://nodemcu.readthedocs.io/en/master/en/modules/mqtt/</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/","title":"Very Simple Way to Send Email Using NodeMCU firmware","text":"<p>Caution</p> <p>This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#smtp-server","title":"SMTP Server","text":"<p>Maybe some of you have already thought about sending email from NodeMCU powered module (ESP8266 ESP-01, ESP-07, etc). Maybe some of you have successfully written a code to send email from this little board. Now I want to show you a very simple way to implement email sending. I think there are a lot of way to do it, but instead of writing a code to send email directly using an smtp server I will use third party tool for it. The name of the service is MailGun. I have read an article which described a method using Mailgun for sending email. When I installed this Ghost Blog Engine, I got a warning message which showed that email service had not been configured properly, and gave me a link. In this article you can read about ghost email configuration, and it has a part about Mailgun configuration. Now I'm using this solution in my Ghost instance. But it is actually not important. While I was configuring Mailgun, I found an example api call using curl to send email via Mailgun. Link. The API example:</p> <pre><code>curl -s --user 'api:YOUR_API_KEY' \\\n    https://api.mailgun.net/v3/YOUR_DOMAIN_NAME/messages \\\n    -F from='Excited User &lt;mailgun@YOUR_DOMAIN_NAME&gt;' \\\n    -F to=YOU@YOUR_DOMAIN_NAME \\\n    -F to=bar@example.com \\\n    -F subject='Hello' \\\n    -F text='Testing some Mailgun awesomness!'\n</code></pre> <p>Some days later I was thinking. This curl example is a very simple and NodeMCU firmware is able to send HTTP POST messages. OK, maybe at the first sight it is not clear, but this curl example is actually a POST message. To come to the NodeMCU firmware, you have to build your firmware with HTTP module. To see what kind of messages are sent over the network after this curl example I created a tcpdump. It is important to send this request without SSL. Because with SSL connection you won't see anything in the dump. Example:</p> <pre><code>curl -s --user 'api:key-2bdec103ac5dea85b9378ab2541faecf' \\\n    http://blog.vinczejanos.info/v3/blog.vinczejanos.info/messages \\\n    -F from='blog@blog.vinczejanos.info' \\\n    -F to=janos.vincze@valami.com \\\n    -F subject='Hello' \\\n    -F text='Testing some Mailgun awesomness!'\n</code></pre> <p>This request returns with HTTP 404, but we are interested in only the request, not the response. Command: <code>sudo tcpdump -s0  host api.mailgun.net -w pcap.pcap</code>  This will create a <code>pcap.pcap</code> file, which can be opened in Wireshark. </p> <p>We need the POST message to follow:</p> <p></p> <p></p> <p>As I have written above we need only the request: <pre><code>POST /v3/blog.vinczejanos.info/messages HTTP/1.1\nAuthorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\nUser-Agent: curl/7.38.0\nHost: blog.vinczejanos.info\nAccept: */*\nContent-Length: 507\nExpect: 100-continue\nContent-Type: multipart/form-data; boundary=------------------------40dcfa2d67b56270\n\nHTTP/1.1 100 Continue\n\n--------------------------40dcfa2d67b56270\nContent-Disposition: form-data; name=\"from\"\n\nblog@blog.vinczejanos.info\n--------------------------40dcfa2d67b56270\nContent-Disposition: form-data; name=\"to\"\n\njanos.vincze@valami.com\n--------------------------40dcfa2d67b56270\nContent-Disposition: form-data; name=\"subject\"\n\nHello\n--------------------------40dcfa2d67b56270\nContent-Disposition: form-data; name=\"text\"\n\nTesting some Mailgun awesomness!\n--------------------------40dcfa2d67b56270--\n</code></pre> Ok. The first thing to do is figure out what is the <code>Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==</code> line. This very simple because HTTP Basic auth is base64 encoded so we can decrypt it by using tihs command: <code>echo -n \"YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\" | base64 -d</code> </p> <p>The result: <code>api:key-2bdec103ac5dea85b9378ab2541faecf</code></p> <p>So the basic auth HTTP header contains your MailGun API key. This step is not necessary, I was only curious.  If you do not want to create tcpdump to find out your basic auth key, simply use base64 command: <pre><code>echo -n \"api:key-2bdec103ac5dea85b9378ab2541faecf\" | base64\nYXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\n</code></pre></p> <p>After that we have the first line of the header: <pre><code>Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==  \n</code></pre></p> <p>Based on the tcpdump we can assemble the HTTP header. You can see that MailGun API is using <code>multipart/form-data</code> MIME format, thus we have to use uniq boundary. If you want to know more about boundary or form-data MIME format please read the rfc2388  documentation. I will use dynamic generated boundary to avoid occurrence of it in the other part of the message. I will use <code>table.insert</code> and <code>table.concat</code> to concatenate strings, because this method consume less memory then the simple <code>..</code> (two dots).  So insted of this: <code>variable=\"string\"..var..\"another string\"</code> I will use:</p> <p><pre><code>test_table={}\ntable.insert(test_table, \"string\")\ntable.insert(test_table, var)\ntable.insert(test_table, \"another string\")\nstring_result=table.concat(test_table)\ntest_table=nil\n</code></pre> OK. Maybe it is a bit longer but it's worth it. Please remember to <code>nil</code> the table after concat. </p>"},{"location":"Old_Blog_Contents/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#1-generate-boundary","title":"1. Generate Boundary","text":"<p><pre><code>boundary_table={}\nfor i=1,15\ndo\n    table.insert(boundary_table, string.char(math.random(65, 90))) -- A-Z\n    table.insert(boundary_table, string.char(math.random(48, 57))) -- 0-9\n    table.insert(boundary_table, string.char(math.random(97, 122))) -- a-z\nend\nboundary=table.concat(boundary_table)\nboundary_table=nil\n</code></pre> This will create something like this: <code>B2bO4oH6gL2iZ7oK6jG2zA6vK3zO1wI9dG1gP0wV0tE3p</code> OK. I know this is a bit stupid way to generate random string, but working and just enough for us.</p>"},{"location":"Old_Blog_Contents/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#assemble-the-header-part","title":"Assemble the Header Part","text":"<p><pre><code>header_table={}\ntable.insert(header_table, 'Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\\r\\n')\ntable.insert(header_table, 'Host: api.mailgun.net\\r\\n')\ntable.insert(header_table, 'User-Agent: NodeMCU/testAg\\r\\n')\ntable.insert(header_table, 'Content-Type: multipart/form-data; boundary='..boundary..'\\r\\n')\nheader=table.concat(header_table)\n</code></pre> As you can see I inserted only the minimal necessary lines to the header. Now we have the <code>header</code> variable with these lines:</p> <pre><code>Authorization: Basic YXBpOmtleS0yYmRlYzEwM2FjNWRlYTg1YjkzNzhhYjI1NDFmYWVjZg==\nHost: api.mailgun.net\nUser-Agent: NodeMCU/testAg\nContent-Type: multipart/form-data; boundary=V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u\n</code></pre>"},{"location":"Old_Blog_Contents/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#assemble-the-body-part-post-message","title":"Assemble The \"Body\" Part (POST message)","text":"<pre><code>data_table={}\ntable.insert(data_table, '--'..boundary..'\\r\\n')\ntable.insert(data_table, 'Content-Disposition: form-data; name=\"from\"\\r\\n\\r\\n')\ntable.insert(data_table, 'admin@blog.vinczejanos.info\\r\\n')\ntable.insert(data_table, '--'..boundary..'\\r\\n')\ntable.insert(data_table, 'Content-Disposition: form-data; name=\"to\"\\r\\n\\r\\n')\ntable.insert(data_table, 'jvincze84@gmail.com\\r\\n')\ntable.insert(data_table, '--'..boundary..'\\r\\n')\ntable.insert(data_table, 'Content-Disposition: form-data; name=\"subject\"\\r\\n\\r\\n')\ntable.insert(data_table, 'Hello\\r\\n')\ntable.insert(data_table, '--'..boundary..'\\r\\n')\ntable.insert(data_table, 'Content-Disposition: form-data; name=\"text\"\\r\\n\\r\\n')\ntable.insert(data_table, 'Congratulations Vincze Janos, you just sent an email with Mailgun!  You are truly awesome!\\r\\n\\r\\n')\ntable.insert(data_table, '--'..boundary..'--\\r\\n')\ndata=table.concat(data_table)\ndata_table=nil\n</code></pre> <p>data values contains the following: <pre><code>--V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u\nContent-Disposition: form-data; name=\"from\"\n\nadmin@blog.vinczejanos.info\n--V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u\nContent-Disposition: form-data; name=\"to\"\n\njvincze84@gmail.com\n--V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u\nContent-Disposition: form-data; name=\"subject\"\n\nHello\n--V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u\nContent-Disposition: form-data; name=\"text\"\n\nCongratulations Vincze Janos, you just sent an email with Mailgun!  You are truly awesome!\n\n--V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u--\n</code></pre></p> <p>There are some very important things:</p> <ul> <li>In the header part use your \"raw\" boundary value: Content-Type: multipart/form-data; boundary=V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u</li> <li>In the body part </li> <li>You have to place two <code>-</code> sign before each boundaries: --V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u Content-Disposition: form-data; name=\"from\"</li> <li>BUT In case of the last boundary you have to add two <code>-</code> to the beginning and to the end of the boundary.: --**V1tS3eX4rA9sK6pV2nN2uD5zP7qP7uT5qV6lL0zI8pA4u--**</li> </ul>"},{"location":"Old_Blog_Contents/nodemcu/Very_Simple_Way_To_Send_Email_Using_Nodemcu_Firmware/#send-an-e-mail","title":"Send An E-Mail :)","text":"<p>Finally we can now send the email.</p> <pre><code>http.post('https://api.mailgun.net/v3/blog.vinczejanos.info/messages', header, data,\n   function(code, data)\n    if (code &lt; 0) then\n      print(\"HTTP request failed\")\n    else\n      print(code, data)\n    end\n  end)\n</code></pre> <p>This message should be returned <code>HTTP/200 OK</code> after <code>http.post</code>. <pre><code>&gt; http.post('https://api.mailgun.net/v3/blog.vinczejanos.info/messages', header, post_data,\n&gt;&gt; function(code, data)\n&gt;&gt; if (code &lt; 0) then\n&gt;&gt; print(\"HTTP request failed\")\n&gt;&gt; else\n&gt;&gt; print(code, data)\n&gt;&gt; end\n&gt;&gt; end)\n&gt; 200   {\n  \"id\": \"&lt;20160826110622.24301.92312.4561825D@blog.vinczejanos.info&gt;\",\n  \"message\": \"Queued. Thank you.\"\n}\n</code></pre> And my message is delivered to my mailbox. :)</p> <p>If you don't like this method, you can find many other ways to implement email sending, or you can write a code on your own. For example on github there is implementation which uses smtp communication with NodeMCU net Module.  Honestly I don't like sending email directly from the ESP9266 modules because it has  very limited resources, but there are some cases when you can implement this code. For example if your code has little footprint. So If you write a rather \"big\" and complex code it is possible that not enough memory will be left to assemble the header and body part and call http.post().  Rather than sending email directly from esp8266 I advise to use NodeRED.</p>"},{"location":"Old_Blog_Contents/other/Collect_Network_Statistic_With_Telegraf_Vnstat/","title":"Collect Network Statistic With Telegraf &amp; VNSTAT","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>I use Telegraf on various hosts without any problem, but in some cases I'm facing issues using sysstat plugin on Orange PI zeros.</p> <p>One of the most important thing for me to collect network bandwidth statistic. For this sysstat plugin is perfect, but how to achieve this without it?</p> <p>I was thinking a bit, and found out that with exec plugin and vnstat I can gather information about bandwidth.</p> <p>Here is the configuration: <pre><code>[[inputs.exec]]\n  commands = [\n    \"/usr/bin/vnstat -i eth0 -tr --short --json\",\n    \"/usr/bin/vnstat -i tun0 -tr --short --json\"\n    ]\n  timeout = \"10s\"\n  name_suffix = \"_vnstat\"\n  data_format = \"json\"\n  json_name_key=\"vnstat\"\n  tag_keys= [\"interface\"]\n</code></pre></p> <p>References:</p> <ul> <li>Input Data Formats</li> <li>JSON</li> </ul>"},{"location":"Old_Blog_Contents/other/Collect_Network_Statistic_With_Telegraf_Vnstat/#example-influxdb-commands","title":"Example InfluxDB Commands","text":"<ul> <li> <p>List avaiable hosts: <pre><code>SHOW TAG VALUES  ON telegraf from \"system\" WITH KEY = \"host\"\n</code></pre></p> </li> <li> <p>Show MEASUREMENTS <pre><code>SHOW MEASUREMENTS  WITH MEASUREMENT =~ /exec.*/\n</code></pre> Output: <pre><code>name: measurements\nname\n----\nexec_vnstat\n</code></pre></p> </li> <li> <p>List Series: <pre><code>SHOW SERIES ON telegraf FROM exec_vnstat;\n</code></pre> Output:</p> </li> </ul> <pre><code>key\n---\nexec_vnstat,dc=barber,host=*****-opi0,interface=eth0,rack=opi0\nexec_vnstat,dc=barber,host=*****-opi0,interface=tun0,rack=opi0\n</code></pre>"},{"location":"Old_Blog_Contents/other/Iptables_Examples/","title":"Iptables Examples","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/other/Iptables_Examples/#clear-all-rules","title":"Clear All Rules","text":"<p>The following commands will completely clear all your rules (and ACCEPT everything).</p> <pre><code>iptables -P INPUT ACCEPT\niptables -P FORWARD ACCEPT\niptables -P OUTPUT ACCEPT\niptables -t nat -F\niptables -t mangle -F\niptables -F\niptables -X\n</code></pre>"},{"location":"Old_Blog_Contents/other/Iptables_Examples/#simple-nat","title":"Simple NAT","text":"<p>Situation:</p> <p>There are some ISPs in Hungary (and all over the world) which use Carrier-grade NAT and this \"feature\" makes my life harder.</p> <p>Devices and Networks:</p> <ul> <li>Router IP address: <code>192.168.100.1</code>, Network: <code>192.168.100.0/24</code></li> <li>NVR (Network Video Recorder)</li> <li>It connects to the ISP's router. IP address: <code>192.168.100.4</code></li> <li>But it has its own network for IP cameras. (3 eth port and WiFi) IP address: <code>172.20.18.4</code>, Network: <code>172.20.18.0/24</code></li> <li>Orange PI zero</li> <li><code>eth0</code> - Connected to the router. (<code>192.168.100.230</code>)</li> <li><code>tun0</code> - Connected to the OpenVPN server. (<code>10.50.0.230</code>)</li> <li><code>wlan0</code> - Connected directly to the NVR over WiFi. (<code>172.20.18.0.6</code>)</li> <li>For example 3 IP cameras:</li> <li><code>172.20.18.3</code></li> <li><code>172.20.18.4</code></li> <li><code>172.20.18.5</code></li> </ul> <p>Picture:</p> <p>Mission: Access the NVR and the cameras over the VPN network.</p> <p>So there are three different network:</p> <ul> <li>192.168.100.0/24</li> <li>10.50.0.0./16</li> <li>172.20.18.0/24</li> </ul> <p>First we have to determine on which ports the NVR listens.  Command<pre><code>nmap 192.168.100.4\n</code></pre> Output<pre><code>Starting Nmap 7.40 ( https://nmap.org ) at 2018-11-07 10:30 UTC\nNmap scan report for 192.168.100.4\nHost is up (0.0015s latency).\nNot shown: 995 closed ports\nPORT     STATE SERVICE\n53/tcp   open  domain\n80/tcp   open  http\n554/tcp  open  rtsp\n5000/tcp open  upnp\n8888/tcp open  sun-answerbook\nMAC Address: 08:EA:40:56:95:EB (Shenzhen Bilian Electronicltd)\n\nNmap done: 1 IP address (1 host up) scanned in 2.34 seconds\n</code></pre></p> <p>Solution: <pre><code>iptables -A FORWARD -i tun0 -j ACCEPT\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 192.168.100.4:80\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 192.168.100.4:554\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 8888 -j DNAT --to 192.168.100.4:8888\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 5000 -j DNAT --to 192.168.100.4:5000\n</code></pre></p> <p>It is very important to enable IP(v4) forwarding.  We can enable it temporary: <pre><code># check: cat /proc/sys/net/ipv4/ip_forward\necho -n 1 &gt;/proc/sys/net/ipv4/ip_forward\n</code></pre> Or permanently, by adding the following line to <code>/etc/sysctl.conf</code> file: <pre><code>net.ipv4.ip_forward=1\n</code></pre></p> <p>The whole script: <pre><code>#!/bin/bash\n\nIFS='\n'\n\niptables -P INPUT ACCEPT\niptables -P FORWARD ACCEPT\niptables -P OUTPUT ACCEPT\niptables -t nat -F\niptables -t mangle -F\niptables -F\niptables -X\n\n\necho -n 1 &gt; /proc/sys/net/ipv4/ip_forward\n\niptables -A FORWARD -i tun0 -j ACCEPT\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 192.168.100.4:80\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 192.168.100.4:554\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 8888 -j DNAT --to 192.168.100.4:8888\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 5000 -j DNAT --to 192.168.100.4:5000\n</code></pre></p> <p>Important</p> <p>This will enable all traffic, and completely turn the firewall off. NOT recommended if you do not have any other firewall in your network and/or your device has public IP address.</p> <p>And what about the cameras?  Rules for a camera:</p> <pre><code>iptables -A FORWARD -i tun0 -j ACCEPT\niptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 172.20.18.4:80\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 554 -j DNAT --to 172.20.18.4:554\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34567 -j DNAT --to 172.20.18.4:34567\n</code></pre> <p>I have never tried but it may be possible to configure <code>iptables</code> to access all cameras without reconfigure our rules for each camera.  Example: <pre><code>iptables -t nat -A PREROUTING -i tun0 -p tcp --dport 80 -j DNAT --to 172.20.18.4:80\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 81 -j DNAT --to 172.20.18.3:80\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 82 -j DNAT --to 172.20.18.5:80\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34567 -j DNAT --to 172.20.18.4:34567\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34568 -j DNAT --to 172.20.18.3:34568\niptables -t nat -A PREROUTING -i tun0 -p tcp --dport 34569 -j DNAT --to 172.20.18.5:34569\n</code></pre> In order to work these rule perfectly the listen ports have to be (re)configured on the IP camera side as well. But in my scenario it is not so important, because I don't need frequently access directly the cameras.</p>"},{"location":"Old_Blog_Contents/other/Iptables_Examples/#permanently-save-iptables-rules-on-debian-like-os","title":"Permanently Save Iptables rules (on debian(-like) OS)","text":"<ul> <li>Save the current configuration <code>iptables-save &gt; /etc/iptables.rules</code></li> <li>Restore the saved configuration from file <code>iptables-restore &lt; /etc/iptables.rules</code></li> </ul> <p>For apply <code>iptables</code> rules on startup use this little script: <pre><code>cat &lt;&lt;EOF&gt;/etc/network/if-pre-up.d/firewall\n#!/bin/bash\n/sbin/iptables-restore &lt; /etc/iptables.rules\nEOF\n\nchmod +x /etc/network/if-pre-up.d/firewall\n</code></pre></p>"},{"location":"Old_Blog_Contents/other/Iptables_Examples/#port-forwarding-to-another-host","title":"Port Forwarding To Another Host","text":"<p>My scenario: I wanted to access RTSP port (554) of multiple IP cameras over one host (Gateway).</p> <p>Host IP address: <code>172.16.0.230</code> IP adress of cameras: <code>172.19.1.1</code>-<code>172.19.1.8</code> RTSP listen port: <code>554</code></p> <p>Rules:  <pre><code>iptables -t nat -A PREROUTING -p tcp --dport 5541 -j DNAT --to-destination 172.19.1.1:554 \niptables -t nat -A POSTROUTING -p tcp -d 172.19.1.1 --dport 554 -j SNAT --to-source  172.16.0.230 \n\niptables -t nat -A PREROUTING -p tcp --dport 5542 -j DNAT --to-destination 172.19.1.2:554 \niptables -t nat -A POSTROUTING -p tcp -d 172.19.1.2 --dport 554 -j SNAT --to-source  172.16.0.230 \n\n\niptables -t nat -A PREROUTING -p tcp --dport 5543 -j DNAT --to-destination 172.19.1.3:554 \niptables -t nat -A POSTROUTING -p tcp -d 172.19.1.3 --dport 554 -j SNAT --to-source  172.16.0.230 \n\netc.\n</code></pre></p> <p>The script with traffic monitoring: <pre><code>#!/bin/bash\n\nIFS='\n'\niptables -P INPUT ACCEPT\niptables -P FORWARD ACCEPT\niptables -P OUTPUT ACCEPT\niptables -t nat -F\niptables -t mangle -F\niptables -F\niptables -X\n\n\necho -n 1 &gt; /proc/sys/net/ipv4/ip_forward\n\niptables -A INPUT  -p tcp -m state --state NEW   -j LOG --log-prefix \"INPUT: \"\niptables -A OUTPUT  -p tcp -m state --state NEW   -j LOG --log-prefix \"OUTPUT: \"\n\niptables -t nat -A PREROUTING  -p tcp  -j LOG --log-prefix \"PREROUTING: \"\niptables -t nat -A POSTROUTING  -p tcp  -j LOG --log-prefix \"POSTROUTING: \"\n\n\n###\n## Traffic\n###\niptables -N trafficmon\niptables -A FORWARD -p tcp --sport 554 -m comment --comment \"ALL\" -j trafficmon\n\n\n\nLOCAL_IP=\"172.16.0.230\"\n\nLISTEN=1554\n\n#  NAME | IP | PORT | URL\n\nCAMS[0]=\"Pool|172.19.1.1|554|ucast/11\"\nCAMS[1]=\"Garden|172.19.1.2|554|11\"\nCAMS[2]=\"Garage|172.19.1.3|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\"\nCAMS[3]=\"Workshop|172.19.1.4|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\"\nCAMS[4]=\"Backyard|172.19.1.5|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\"\nCAMS[5]=\"Gate|172.19.1.6|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\"\nCAMS[6]=\"Street|172.19.1.7|554|user=admin_password=tlJwpbo6_channel=1_stream=0.sdp?real_stream\"\nCAMS[7]=\"Corridor|172.19.1.8|554|11\"\n\n\nfor CAM in ${CAMS[@]}\ndo\n  NAME=$( echo $CAM | cut -f 1 -d\"|\" )\n  IPADDR=$( echo $CAM | cut -f 2 -d\"|\" )\n  PORT=$( echo $CAM | cut -f 3 -d\"|\" )\n  URL=$( echo $CAM | cut -f 4 -d\"|\" )\n\n  echo \"URL : rtsp://$LOCAL_IP:$LISTEN/$URL (### $NAME ###)\"\n\n  iptables -t nat -A PREROUTING -p tcp --dport $LISTEN -j DNAT --to-destination $IPADDR:$PORT -m comment --comment \"$NAME\"\n  iptables -t nat -A POSTROUTING -p tcp -d $IPADDR --dport $PORT -j SNAT --to-source $LOCAL_IP -m comment --comment \"$NAME\"\n  iptables -A FORWARD -p tcp --sport 554 --source $IPADDR -m comment --comment \"$NAME\" -j trafficmon\n\n  (( LISTEN ++ ))\ndone\n\n\n\ncat &lt;&lt;EOF\nPlease Run\n\niptables-save &gt; /etc/iptables.rules\n\nto make rules permanent!\nEOF\n</code></pre></p>"},{"location":"Old_Blog_Contents/other/Iptables_Examples/#iptables-quick-commands-cheat-sheets","title":"IPTABLES quick commands &amp; Cheat Sheets","text":"<p><pre><code>manage chain:\n# iptables -N new_chain             // create a chain\n# iptables -E new_chain old_chain       // edit a chain\n# iptables -X old_chain             // delete a chain\n\nredirecting packet to a user chain:\n# iptables -A INPUT -p icmp -j new_chain\n\nlisting rules:\n# iptables -L                   // list all rules of all tables\n# iptables -L -v                // display rules and their counters\n# iptables -L -t nat                // display rules for a specific tables\n# iptables -L -n --line-numbers         // listing rules with line number for all tables\n# iptables -L INPUT -n --line-numbers       // listing rules with line number for specific table\n\nmanage rules:\n# iptables -A chain             // append rules to the bottom of the chain\n# iptables -I chain [rulenum]           // insert in chain as rulenum (default at the top or 1)\n# iptables -R chain rulenum         // replace rules with rules specified for the rulnum\n# iptables -D chain rulenum         // delete rules matching rulenum (default 1)\n# iptables -D chain             // delete matching rules\n\nchange default policy:\n# iptables -P chain target          // change policy on chain to target\n# iptables -P INPUT DROP            // change INPUT table policy to DROP\n# iptables -P OUTPUT DROP           // change OUTPUT chain policy to DROP\n# iptables -P FORWARD DROP          // change FORWARD chain policy to DROP\n</code></pre> Reference: iptables-quick-command-list/</p>"},{"location":"Old_Blog_Contents/other/Nokia_6120c_%28bb5%29_Forgotten_Security_Code/","title":"Nokia 6120c (BB5) Forgotten Security Code","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>Maybe this post is a bit outdated, but still can be useful whose wants to bring back an old Nokia phone to life. My story is very simple and usual. I've forgotten my security code of an old Nokia 6120c. The key problem was that I locked my phone, and could not turn it on. It always asked for the security code, and after 5 tries locked for 5 minutes, thus brute force would have been a bit time-consuming. Oh! Why I wanted to use such an old phone? I had to carry my iPhone to the service and was thinking about going back in time and using one of my really old phone during I'm waiting to get my iPhone back, so I chose my old Nokia 6120c. </p> <p>I had started googling and found some article about how to unlock BB5 phones. Unfortunately there are a lot of article which can lead you to the wrong way. For example generating master unlock code is not possible for bb5 phones, and please aware of downloading software from unknown sources.</p> <p>NOTE: Resetting your phone won't set your security code to its original (12345)! By the way, you can hard reset your phone by following these easy steps:</p> <ol> <li>Turn OFF your phone.</li> <li>Press and hold \"Green phone\" + \"3\" + \"*\" button.</li> <li>Press and hold the power button while the phone is turned on.</li> <li>After displaying the \"Nokia\" message and the phone turned on (or asking for security code, if it is locked) you can release the mentioned button in the 2. step.</li> </ol> <p>REFERENCE Link:</p> <p>http://forum.gsmhosting.com/vbb/f299/get-your-phone-lock-code-security-code-without-reset-format-nokia-bb5-phones-teste-667948/</p> <p>It the post linked below you can find everything you need to successfully retrieve your security code. Here I want to give you a \"real\" step by step guide with screenshots and examples. </p> <p>What will you need?</p> <ol> <li>A locked phone with forgotten security code (You don't really need this, you can do it as hobby, as well :) )</li> <li>An USB cable</li> <li>An ~5K resistor</li> <li>Installed NSS (Nemesis Service Suite 1.0.38.15)  During the install please choose the \"Virtual USB device\".  Download link:</li> <li>Installed \"Nokia Suite\" or \"Nokia Connectivity Cable Driver\"</li> <li>(Optional) Charger</li> </ol> <p>First you have to connect the 5K resistor between BSI (middle) and GNS (\"-\") pins of the battery connector. DO NOT short circuit your battery by connecting the \"+\" and \"-\" to each other even through the resistor!</p> <p></p>"},{"location":"Old_Blog_Contents/other/Nokia_6120c_%28bb5%29_Forgotten_Security_Code/#_1","title":"Nokia 6120c (BB5) Forgotten Security Code","text":"<p>Next turn your phone on. You have to see something like this:</p> <p></p> <p>After that you should connect the phone to your computer with the mini USB cable and start NSS.  Click on the magnifier button on the top right corner:</p> <p></p> <p>Click on the Device Info, then the Scan button:</p> <p></p> <p>Finally click on the \"Permanent Memory\" tab, and \"Read\" button:</p> <p></p> <p>As the last step a .pm file will be created in this directory: \"c:\\Program Files (x86)\\NSS\\Backup\\pm\\\". The file name have to be the imei number of your phone. Example: \"358640012938846.pm\"</p> <p>Look for the line beginning with \"5=\", in my case this is the 218. line. Example: <code>5=38343634350000000000</code></p> <p>Remove every second \"3\" digit and the trailing zeros, the remaining is your security code (<code>84645</code>).</p> <p>That's all! :)</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/","title":"Sonoff Relays With OpenHab And Tasmota Firmware","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#tldr","title":"TL;DR","text":"<p>I'm using OpenHab to control some Lights and equipment in my house and garden. I used to develop my own circuits using ESPs, relays, power supplies etc., but some months ago I found the Sonoff products, which offer the same or more functions than my own devices. Just for comparison here is my 4ch relay with ESP07:</p> <p>And the 4CH relay from Sonoff:</p> <p>However, all my own relays and sensors work fine, I had to develop a firmware for them. Since I'm not a not developer it was a big challenge for me, even with the LUA FW. So I decided to order some Sonoff products and tried them. </p> <p>For the very first time I tried with its factory firmware, but it has a lot of limitations:</p> <ul> <li>It cannot be used with OpenHab. This is a show stopper disadvantage for me.</li> <li>It has no web interface. </li> <li>Very limited scheduler. (No sunset / sunrise option, etc)</li> <li>Scenes cannot be shared. </li> <li>Uses the sonoff own cloud infrastructure. That's why nobody knows what kind of information is sent to the sonoff servers. (The big brother is always watching you. :) )</li> </ul> <p>I started Googling and found the Tasmota firmware easily, which is for Sonoff products. It is available on Github: Sonoff-Tasmota</p> <p>There are a lot of excellent articles on the Internet which are about How to update your sonoff product with Tasmota firmware, so mine is something like \"Yet, another article about....\" :)</p> <p>So,  you need:</p> <ul> <li>One supported sonoff product.  (You can find the supported device list on the official Tasmota Github page.)</li> <li>4 solder-able pins. (For GND, RxD, TxD and Vcc)</li> <li>Soldering Station. (And willingness to solder of course.)</li> <li>USB to TTL converter.</li> <li>A PC or notebook.</li> <li>Screwdriver.</li> </ul> <p>Before we start, let's say some words about Sonoffs. I'v ordered 4 different types of relays: S2 smart Socket, POW (One ch relay with power monitoring capability), Sonoff basic and Sonoff 4ch. All of them are working perfectly with Tasmota. You can visit the sonoff store on Aliexpress and I bet you will be surprised at how cheap these things are. At the moment the cheapest relay is the sonoff basic, it costs only 5$, and you can get a full system for switching things around your house, garden, holiday house even if you are far away.  What about the quality? So-so. For this price I think the quality is acceptable, but not the best. For example the product sheet of the Basic says that the maximum load is 2200W. Seeing the soldering inside the cover I don't think it can bear 2.2kW. But from the other hand there is not a lot of equipment which we want to control over the internet and consumes 2.2kW. In my case I want to control some lights, fans and my water pump with these relays, so sonoff relays are perfectly suitable for my goal.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#1-soldering","title":"1. Soldering","text":"<p>As I wrote above I have 4 types of Sonoff relays. Thanks to God the most expensive one (4ch) have pre-soldered pins on it.  Helpful links:</p> <ul> <li>Hardware-Preparation</li> <li>Sonoff-4CH-and-4CH-Pro</li> </ul> <p>Please be aware (4ch) that \"The printed labels on the PCB for Rx and Tx are incorrectly swapped as can be seen on the image.\"</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#11-sonoff-pow","title":"1.1. Sonoff POW","text":""},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#12-sonoff-s20-socket","title":"1.2. Sonoff S20 (Socket)","text":""},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#13-basic","title":"1.3. Basic","text":"<p>The GPIO14 can be used for example to connect DHT22 sensor (or any other sensor supported by Tasmota) to the sonoff.  I will assist you later how to configure Tasmota to be able to use additional sensor.</p> <p>The button on sonoffs are always connected to GPIO0, except when the board has more then one button (eg.: 4ch), in this case the 1st button is connected to GPIO0.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#2-upload-the-firmware","title":"2. Upload The Firmware","text":"<p>To be honest this is the easiest step.</p> <p>Danger</p> <p>Do not connect AC power and the serial connection at the same time!</p> <p>Connection Matrix: <pre><code>USB 2 TTL &lt;--&gt; Sonoff\nGND       &lt;--&gt; GND\nTxD       &lt;--&gt; RxD\nRxD       &lt;--&gt; TxD\nVCC       &lt;--&gt; VCC\n</code></pre></p> <p>In order to put your device into FW upgrade mode, press and hold the button on it while connecting to VCC.  You can connect all the wires and press the button when you connect the USB2TTL to your computer, or leave the VCC (only the vcc) disconnected, connect the USB2TTL to the computer, press and hold the button and connect the VCC.</p> <p>Useful links:</p> <ul> <li>upgrading-sonoff-stock-firmware-to-sonoff-tasmota-usb-to-serial-and-ota-update-methods</li> <li>https://github.com/arendst/Sonoff-Tasmota/wiki/Esptool</li> </ul>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#21-optional-take-backup","title":"2.1. (Optional) Take Backup","text":"<p>Taking backup is always optional, but essential. If you skip this step, you lose the possibility to restore the factory firmware. (It is not 100% true because I will make the stock firmware available here.)</p> <p>You will need Esptool to achieve this step. If you don't know how to install it, please take a look at this page: esptool For Windows users maybe the \"ESP8266Flasher\" could be an option (nodemcu-flasher).</p> <p>Command<pre><code>esptool.py --port /dev/ttyUSB0 read_flash 0x00000 0x100000 image1M.bin\n</code></pre> Output<pre><code>esptool.py v2.3.1\nConnecting....\nDetecting chip type... ESP8266\nChip is ESP8266EX\nFeatures: WiFi\nUploading stub...\nRunning stub...\nStub running...\n1048576 (100 %)\n1048576 (100 %)\nRead 1048576 bytes at 0x0 in 94.9 seconds (88.4 kbit/s)...\nHard resetting via RTS pin...\n</code></pre></p> <p>Please be aware that after any esptool command you have to reconnect to the device. (So, you have to disconnect vcc, press and hold the button and connect VCC again.)</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#22-optional-erase-flash","title":"2.2. (Optional) Erase Flash","text":"<p>This step is also optional, but recommended.</p> <p>Command<pre><code>esptool.py --port /dev/ttyUSB0 erase_flash\n</code></pre> Output<pre><code>esptool.py v2.3.1\nConnecting....\nDetecting chip type... ESP8266\nChip is ESP8266EX\nFeatures: WiFi\nUploading stub...\nRunning stub...\nStub running...\nErasing flash (this may take a while)...\nChip erase completed successfully in 3.2s\nHard resetting via RTS pin...\n</code></pre></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#23-flashing-the-firmware","title":"2.3. Flashing The Firmware","text":"<p>Till this step I haven't mentioned the firmware itself.  How can you download the pre-compiled firmware? You don't need to compile the firmware, it can be downloaded from github: releases</p> <p>The <code>sonoff.bin</code> has always worked for me: sonoff.bin Before you use this link, please make sure that no newer version has been released. </p> <p>So, let's burn the firmware:</p> <p>Command<pre><code>esptool.py --port /dev/ttyUSB0 write_flash -fs 1MB -fm dout 0x0 sonoff.bin\n</code></pre> Output<pre><code>esptool.py v2.3.1\nConnecting....\nDetecting chip type... ESP8266\nChip is ESP8266EX\nFeatures: WiFi\nUploading stub...\nRunning stub...\nStub running...\nConfiguring flash size...\nCompressed 539040 bytes to 368171...\nWrote 539040 bytes (368171 compressed) at 0x00000000 in 32.6 seconds (effective 132.4 kbit/s)...\nHash of data verified.\n\nLeaving...\nHard resetting via RTS pin...\n</code></pre></p> <p>That's all. You can assemble your sonoff and start using it. :)</p> <p>In the future chapters I'll give you some tips for configuring the devices and using them with MQTT and OpenHAB.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#3-settings","title":"3. Settings","text":""},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#31-connect-the-device-to-your-local-network-wifi-setup","title":"3.1. Connect The Device To Your Local Network - WiFi Setup","text":"<p>The first things to do is understand how the button with the new Tasmote FW works. Here is the link: Button-Usage</p> <ul> <li> <p>1 short press: Toggles the relay either directly or by sending a MQTT message like cmnd/sonoff/POWER1 ON. This will blink the LED twice and sends a MQTT status message like stat/sonoff/POWER1 ON. If cmnd/sonoff/ButtonRetain on has been used the MQTT message will also contain the MQTT retain flag.</p> </li> <li> <p>2 short presses: Toggles the relay 2 if available on the device like Sonoff Dual. This will blink the LED twice and sends a MQTT status message like stat/sonoff/POWER2 on.</p> </li> <li> <p>3 short presses: Start Wifi smartconfig allowing for SSID and Password configuration using an Android mobile phone with the ESP8266 SmartConfig app. The LED will blink during the config period. A single button press during this period will abort and restart sonoff.</p> </li> <li> <p>4 short presses: Start Wifi manager providing an Access Point with IP address 192.168.4.1 and a web server allowing the configuration of Wifi. The LED will blink during the config period. A single button press during this period will abort and restart sonoff.</p> </li> <li> <p>5 short presses: Start Wifi Protected Setup (WPS) allowing for SSID and Password configuration using the router's WPS button or webpage. The LED will blink during the config period. A single button press during this period will abort and restart sonoff.</p> </li> <li> <p>6 short presses: Will restart the module</p> </li> <li> <p>7 short presses: Start OTA download of firmware. The green LED is lit during the update</p> </li> </ul> <p>Pressing the button for over forty seconds: Reset settings to defaults as defined in user_config.h and restarts the device</p> <p>I always used the 4 short presses option to configure WiFi.</p> <p>After the WiFi is successfully set up you can configure your device via your web browser using the IP address.  In order to figure out the IP address use your router configuration page or console if accessible. Or you can use <code>nmap</code> to find IP addresses: <code>sudo nmap -sP 172.20.1.*</code></p> <p>Example output:</p> <pre><code>Starting Nmap 7.40 ( https://nmap.org ) at 2018-07-01 18:35 CEST\nNmap scan report for 172.20.1.1\nHost is up (0.022s latency).\nMAC Address: B4:E6:2D:15:82:48 (Unknown)\nNmap scan report for 172.20.1.2\nHost is up (0.0045s latency).\nMAC Address: 60:01:94:9C:65:48 (Espressif)\n</code></pre>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#32-configure-module","title":"3.2. Configure Module","text":"<p>The first thing to do after the WiFi settings is tell the Firmware which module we are using (Basic/POW/4CH/etc.).</p> <p>To do this open the module configuration page. Example: <code>http://172.20.1.9/md</code> Or simply open the main page: <code>http://172.20.1.9/</code>, select \"Configuration\" then \"Configure Module\" option. Use the drop-down list to select your device type.</p> <p></p> <p>This step is extremely important but straightforward. Without proper module selection you can't use your device features and it can lead to improper behavior. For example if you have a 4CH sonoff device, without selecting the right module you can control only the 1st channel, or with POW module you can't see the sensor data.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#33-configure-mqtt","title":"3.3. Configure MQTT","text":"<p>Actually you can use your device via its web interface without MQTT, but one of the goals of this post is to integrate sonoff devices to OpenHab, and the best way (or the only way) to do this is use MQTT.</p> <p>Open the MQTT configuration page (eg.: <code>http://172.20.1.9/mq</code>) Example configuration:</p> <p></p> <p>Without manual mqtt configuration the firmware tries to connect to the mqtt server using mDNS. I've never tried this feature so I don't know if it works or not, and don't know what happens when multiple MQTT servers are available, or when user/password is needed to connect.</p> <p>With the configuration on the screenshot above you can subscribe to the topic and see what happens.  I'm using the following syntax as topic: <code>sonoff/[MAC ADDRESS without colon]/%prpefix%</code>  The prefix can be tele,state or cmnd.</p> <p>Example subscribe: <pre><code>mosquitto_sub -v -h 172.16.0.250 -u ****** -P ***** -t 'sonoff/B4E62D14BE5A/#'\n</code></pre> Example outputs: <pre><code>1. sonoff/B4E62D14BE5A/tele/SENSOR {\"Time\":\"2018-07-01T18:57:27\",\"ENERGY\":{\"Total\":2.262,\"Yesterday\":1.330,\"Today\":0.077,\"Power\":2,\"Factor\":0.05,\"Voltage\":222,\"Current\":0.146}}\n\n2. sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T18:57:33\",\"Uptime\":\"2T00:34:12\",\"Vcc\":3.142,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":52,\"APMac\":\"6C:3B:6B:A0:D2:79\"}}\n\n3. sonoff/B4E62D14BE5A/stat/POWER OFF\n</code></pre></p> <p>Explanation:</p> <ul> <li> <p>This is a Sonoff POW module which reports power usage. You can see that the message is in JSON format.  <pre><code>{\"Time\":\"2018-07-01T18:57:27\",\"ENERGY\":{\"Total\":2.262,\"Yesterday\":1.330,\"Today\":0.077,\"Power\":2,\"Factor\":0.05,\"Voltage\":222,\"Current\":0.146}}\n</code></pre></p> </li> <li> <p>Tasmota firmware reports telemetry information (RSSI, Uptime, power state, etc.) periodically (in every 300s by default). This message is also in JSON format. </p> </li> <li> <p>When you turn the relay on or off (even with the web interface or via MQTT message) the device reports the new state. In this particular situation I turned the device off using the web interface.</p> </li> </ul> <p>These three steps are essential to use your device with OpenHAB &amp; MQTT. You can go through all web configuration elements/options, but this post doesn't aim to give you a complete guideline to Tasmota firmware.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#34-sending-commands-to-the-device-via-mqtt","title":"3.4. Sending Commands To The Device Via MQTT","text":"<p>Before you start you can check all available commands on Tasmota GitHub page: https://github.com/arendst/Sonoff-Tasmota/wiki/Commands If you are planning to deal with Tasmota firmware I recommend you to bookmark this page, it is very helpful.</p> <p>When you want to send commands to tasmota you always have to use the <code>cmnd</code> prefix. </p> <p>Since we are speaking about relays, maybe the first question comes up: how to ON or OFF them.  Let's see the web page mentioned before (Commands): </p> <p>In this first example I will explain all available options to make the configuration method as clear as possible.</p> <p>So in a terminal tab I always subscribe to the device topic to see what happens. Command: <code>mosquitto_sub -v -h 172.16.0.250 -u ***** -P ***** -t 'sonoff/B4E62D14BE5A/#'</code></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#341-check-the-state-of-the-relay","title":"3.4.1. Check The State Of The Relay:","text":"<p>Command: <code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -n</code></p> <p>On the \"subscribe\" tab you can see these messages: <pre><code>sonoff/B4E62D14BE5A/cmnd/power (null)\nsonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"}\nsonoff/B4E62D14BE5A/stat/POWER ON\n</code></pre> * First Line: The command you sent to the device.  * Second Line: The result of your command. To any command tasmota relies with a \"RESULT\" message.  * Third Line: Status of the relay</p> <p>If you have for example Sonoff Dual, or Sonoff 4ch you can specify which channel you want to use.  For example if you want to check the 3rd channel you can use this command: <code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power3 -n</code></p> <p>I'm using a POW module for introduction, it has no 3rd channel so get \"unknown\" result: <pre><code>sonoff/B4E62D14BE5A/cmnd/power3 (null)\nsonoff/B4E62D14BE5A/stat/RESULT {\"Command\":\"Unknown\"}\n</code></pre></p> <p>If your device has only one channel do not use the number, so use only the \"POWER\" word instead of \"POWER1\". But in case of any multi-channels you have to use POWER1, POWER2, etc.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#342-onofftoggle-the-relay","title":"3.4.2. ON/OFF/TOGGLE The Relay","text":"<p>Regarding relays the most important activity is to turn on and off them.</p> <p>Command: <code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m on</code></p> <p>MQTT messages: <pre><code>sonoff/B4E62D14BE5A/cmnd/power on\nsonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"}\nsonoff/B4E62D14BE5A/stat/POWER ON\n</code></pre></p> <p>Command: <code>mosquitto_pub -h 172.16.0.250 -u vinyo -P Timike -t sonoff/B4E62D14BE5A/cmnd/power -m on</code></p> <p>MQTT messages: <pre><code>sonoff/B4E62D14BE5A/cmnd/power off\nsonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"OFF\"}\nsonoff/B4E62D14BE5A/stat/POWER OFF\n</code></pre></p> <p>As you can see in the table you can use numbers instead of command, eg: 2 / toggle.</p> <p>Let's fire the toggle command twice: <pre><code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 2\nmosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 2\n</code></pre></p> <p>MQTT messages: <pre><code>sonoff/B4E62D14BE5A/cmnd/power 2\nsonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"OFF\"}\nsonoff/B4E62D14BE5A/stat/POWER OFF\n\nsonoff/B4E62D14BE5A/cmnd/power 2\nsonoff/B4E62D14BE5A/stat/RESULT {\"POWER\":\"ON\"}\nsonoff/B4E62D14BE5A/stat/POWER ON\n</code></pre></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#343-blink-the-relay","title":"3.4.3. BLINK The Relay","text":"<p>This is an interesting feature of Tasmota FW.  But first please check the following options related to blinking. </p> <p>With this option the relay will turn on \"BlinkCount\" times for \"BlinkTime\" second*0.1.</p> <p>Check the default settings using these commands: <pre><code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/BlinkCount -n\nmosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/BlinkTime -n\n</code></pre></p> <p>MQTT messages: <pre><code>sonoff/B4E62D14BE5A/cmnd/BlinkCount (null)\nsonoff/B4E62D14BE5A/stat/RESULT {\"BlinkCount\":10}\nsonoff/B4E62D14BE5A/cmnd/BlinkTime (null)\nsonoff/B4E62D14BE5A/stat/RESULT {\"BlinkTime\":10}\n</code></pre></p> <p>If you connected a light to your relay, it will turn on for 1 second, 10 times. So, turn on for 1 sec, then turn off for 1 sec, turn on for 1 sec, turn on for 1 sec, and so on.</p> <p>After turning on the relay ten times it remains OFF. But you can terminate blinking with \"4/blinkoff\" command.</p> <p>Examples:</p> <ul> <li>Start Blinking: <code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 3</code></li> <li>Terminate Blinking: <code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/B4E62D14BE5A/cmnd/power -m 4</code></li> </ul> <p>Maybe the missing feature of this equipment that you cannot configure BlinkCount and BlinkTime for each relays when use device with multiple channels.</p> <p>In the rest of this configuration section I will show you some useful and exciting features of tasmota firmware.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#344-set-telemetry-period","title":"3.4.4. Set Telemetry Period","text":"<p>If you need the status information of your device more frequently than its default (300s) you can configure the telemetry period with the following command:</p> <pre><code>mosquitto_pub -h 172.16.0.250 -u ***** -P ****** -t sonoff/B4E62D14BE5A/cmnd/TelePeriod -m 30\n</code></pre> <p>After runing this command tasmota will publish \"STATE\" messages in every 30 seconds: <pre><code>sonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T20:53:09\",\"Uptime\":\"2T02:29:48\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":58,\"APMac\":\"6C:3B:6B:A0:D2:79\"}}\n\nsonoff/B4E62D14BE5A/tele/STATE {\"Time\":\"2018-07-01T20:53:39\",\"Uptime\":\"2T02:30:18\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":58,\"APMac\":\"6C:3B:6B:A0:D2:79\"}}\n</code></pre> Times: <pre><code>2018-07-01T20:53:09\n2018-07-01T20:53:39\n</code></pre></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#345-set-timezone","title":"3.4.5. Set Timezone","text":"<p>Command:</p> <p><pre><code>mosquitto_pub -h 172.16.0.250 -u ***** -P ***** -t sonoff/B4E62D14BE5A/cmnd/Timezone -m 2\n</code></pre> </p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#346-set-pulse-time-for-relays","title":"3.4.6. Set Pulse Time For Relay(s)","text":"<p>This feature is useful when you always want to turn off your relay after a certain time. Actually it is a simple timer. If you set up pulse time to 10 minutes, the relay will turn off after 10 minutes every time you turn it on. </p> <p></p> <p>I think these examples are far enough to understand how mqtt messages work, and after now that you know that you should be able to send your own commands to your device(s).</p> <p>The next section aims to demonstrate how to use Tasmota firmware with OpenHAB.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#4-openhab-integration","title":"4. OpenHAB Integration","text":"<p>So, I think this section will be the most interesting and useful part of this post. Since this post is about Tasmota firmware and its integration to OpenHAB I don't want to go deep inside the Mosquito install &amp; setup and MQTT binding settings in OpenHAB. You can install and enable MQTT binding in PaperUI, and here is an example of <code>mqtt.cfg</code>.</p> <pre><code>mqtt:openhabPI.url=tcp://localhost:1883\nmqtt:openhabPI.clientId=openhabPI\nmqtt:openhabPI.user=*********\nmqtt:openhabPI.pwd=*********\nmqtt:openhabPI.retain=true\n</code></pre>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#41-openhab-items","title":"4.1. OpenHAB Items","text":"<p>As always first we need to set up the items.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#411-turn-the-relays-on-and-off-items","title":"4.1.1. Turn The Relay(s) On And Off Item(s)","text":"<p>My first example is for 1CH sonoff modules (S20, Basic, POW, etc). <pre><code>Switch prod_sonoff_BCDDC28027AD_switch1        \"Pantry Fan\"                         &lt;fan&gt;    (sonoffsw,Sonoff)   \n    { mqtt=\"&gt;[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default],\n            &lt;[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)],\n            &lt;[openhabPI:sonoff/BCDDC28027AD/stat/POWER:state:default]\", autoupdate=\"true\" }\n</code></pre></p> <p>The second example is for 4CH modules: <pre><code>Switch prod_sonoff_6001949C6548_switch1         \"SL-Right 1,3\"                         &lt;light&gt;    (sonoffsw,Sonoff,SL_ALL,SL_RIGHT,SL_13,Lights)\n   { mqtt=\"&gt;[openhabPI:sonoff/6001949C6548/cmnd/power1:command:*:default],\n           &lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER1)],\n           &lt;[openhabPI:sonoff/6001949C6548/stat/POWER1:state:default]\", autoupdate=\"true\" }\n\nSwitch prod_sonoff_6001949C6548_switch2         \"SL-Right 2,4\"                         &lt;light&gt;    (sonoffsw,Sonoff,SL_ALL,SL_RIGHT,SL_24,Lights)\n   { mqtt=\"&gt;[openhabPI:sonoff/6001949C6548/cmnd/power2:command:*:default],\n           &lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER2)],\n           &lt;[openhabPI:sonoff/6001949C6548/stat/POWER2:state:default]\", autoupdate=\"true\" }\n\nSwitch prod_sonoff_6001949C6548_switch3         \"SL-Left 1,3\"                          &lt;light&gt;    (sonoffsw,Sonoff,SL_ALL,SL_LEFT,SL_13,Lights)\n   { mqtt=\"&gt;[openhabPI:sonoff/6001949C6548/cmnd/power3:command:*:default],\n           &lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER3)],\n           &lt;[openhabPI:sonoff/6001949C6548/stat/POWER3:state:default]\", autoupdate=\"true\" }\n\nSwitch prod_sonoff_6001949C6548_switch4         \"SL-Left 2,4\"                          &lt;light&gt;    (sonoffsw,Sonoff,SL_ALL,SL_LEFT,SL_24,Lights)\n   { mqtt=\"&gt;[openhabPI:sonoff/6001949C6548/cmnd/power4:command:*:default],\n           &lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER4)],\n           &lt;[openhabPI:sonoff/6001949C6548/stat/POWER4:state:default]\", autoupdate=\"true\" }\n</code></pre></p> <p>I try to explain my setup step by step. There are 3 mqtt settings, 1 inbound and 2 outbounds:</p> <ul> <li> <p><code>&gt;[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default]</code> This is the outbound one. It's intended to control (turn on / off) the relay. When you turn the relay on / off in OpenHAB the proper command will be sent to the device. Topic: <code>sonoff/BCDDC28027AD/cmnd/power1</code> Type: <code>command</code> Trgger: <code>*</code> Transformation: <code>default</code> When you turn ON/OFF the relay OpenHAB publish ON/OFF message to <code>sonoff/6001949C6548/cmnd/power4</code> topic. It is the same as described in 3.4.2. section: <code>mosquitto_pub -h 172.16.0.250 -u ****** -P ****** -t sonoff/BCDDC28027AD/cmnd/power1 -m on</code></p> </li> <li> <p><code>&lt;[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)]</code> With this you subscribe to the <code>sonoff/BCDDC28027AD/tele/STATE</code> topic, and every time the sonoff device publish telemetry information the item's (relay) state will be updated to the actual state. </p> </li> <li> <p><code>&lt;[openhabPI:sonoff/BCDDC28027AD/stat/POWER:state:default]</code> This is very similar to the previous one, this updates the state of this item (prod_sonoff_BCDDC28027AD_switch1).</p> </li> </ul> <p>Maybe you are wondering why I'm using two different mqtt topics to update the item state. The reason is very simple: I want to make sure that the item state is always the actual state of the relay.  The telemetry (tele) topic is useful when you restart the OpenHAB and you don't have persistence set up for this item. In this case you lose the item state, but after the device posts its telemetry information the item state is updated.  The second topic (stat) updates the item state immediately when you manually turn on/off the relay (with the button(s) on it). This configuration can be useful, when you control the relay via mosquitto_pub command, or using another application.</p> <p>With these two inbound settings you can make sure that the item is always up-to-date. </p> <p>There is an important thing to notice: the difference between 1ch and 4ch configuration:</p> <p><pre><code>1CH:\n&gt;[openhabPI:sonoff/BCDDC28027AD/cmnd/power1:command:*:default]\n4CH:\n&gt;[openhabPI:sonoff/6001949C6548/cmnd/power1:command:*:default]\n&gt;[openhabPI:sonoff/6001949C6548/cmnd/power2:command:*:default]\n&gt;[openhabPI:sonoff/6001949C6548/cmnd/power3:command:*:default]\n&gt;[openhabPI:sonoff/6001949C6548/cmnd/power4:command:*:default]\n</code></pre> You can use <code>power</code> and even <code>power1</code> with 1CH device, BUT in case of multi-channel device you have to use the appropriate channel number. <pre><code>1CH:\n&lt;[openhabPI:sonoff/BCDDC28027AD/tele/STATE:state:JSONPATH($.POWER)]\n4CH:\n&lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER1)]\n&lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER2)]\n&lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER3)]\n&lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.POWER4)]\n</code></pre></p> <p>BUT! When you configure telemetry topic subscription you can't use <code>POWER1</code> for 1CH device, the right configuration is just the <code>POWER</code> without the number. The situation is the same when you configure <code>stat</code> topic (<code>sonoff/BCDDC28027AD/stat/POWER</code>). The version without the number has to be used!</p> <p>Of course, the multi-channel device configurations have to contain the number for each channel. </p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#412-telemetry-information","title":"4.1.2. Telemetry Information","text":"<p>If you want to display information about your device you can use its telemetry topic. Example: <pre><code>String prod_sonoff_6001949C6548_lwt            \"S20A - Status [MAP(status_sonoff.map):%s]\"  &lt;lwt&gt;         (g_slwt)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/tele/LWT:state:default]\", autoupdate=\"true\" }\n\nNumber prod_sonoff_6001949C6548__RSSI          \"RSSI [%d %%]\"                               &lt;signal&gt;      (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.Wifi.RSSI)]\" }\n\nString prod_sonoff_6001949C6548_uptime         \"Uptime [%s]\"                                &lt;timer&gt;       (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/tele/STATE:state:JSONPATH($.Uptime)]\" }\n</code></pre></p> <p>JSON example (provided by the device): <pre><code>{\n   \"Time\":\"2018-07-01T18:57:33\",\n   \"Uptime\":\"2T00:34:12\",\n   \"Vcc\":3.142,\n   \"POWER\":\"ON\",\n   \"Wifi\":{\n      \"AP\":1,\n      \"SSId\":\"Vinyo-Net\",\n      \"RSSI\":52,\n      \"APMac\":\"6C:3B:6B:A0:D2:79\"\n   }\n}\n</code></pre></p> <p>If you have a bit experience with JSONs I think this configuration should be clear for you. So if you need the WIFI RSSI value, the right configuration is: <code>JSONPATH($.Wifi.RSSI)</code></p> <p>Tasmota firmware provide LWT (Last Will Testament) with retain flag, so you can use it to show the device status (Active/Inactive/Unknown). The device is in UNKNOWN state when you restart OpenHAB and the LWT topic is not updated since the restart.  Here is my transformation map for LWT: <pre><code>Online=ACTIVE\nOffline=INACTIVE\n-=UNKNOWN\n=UNKNOWN\nNULL=No data\n</code></pre> Using persistence (not retain!) for LWT is a bit dangerous, because while the OpehHAB is offline, the state of the device may change, if so, after the OpenHAB becames online again, the last saved state will be displayed, not the actual one.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#413-other-examples","title":"4.1.3. Other Examples","text":"<p>This chapter is actually about more examples of displaying more information about your Sonoff device.</p> <pre><code>String prod_sonoff_6001949C6548_hostname       \"Hostname [%s]\"                              &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Hostname)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_BuildDateTime  \"FW Build Date [%s]\"                         &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS2:state:JSONPATH($.StatusFWR.BuildDateTime)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_Vcc            \"Vcc [%s]\"                                   &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Vcc)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_Time           \"Time [%s]\"                                  &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Time)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_SSId           \"SSId [%s]\"                                  &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS11:state:JSONPATH($.StatusSTS.Wifi.SSId)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_IPAddress      \"IPAddress [%s]\"                             &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.IPAddress)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_Mac            \"Mac [%s]\"                                   &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Mac)]\", autoupdate=\"true\"  }\n\nString prod_sonoff_6001949C6548_Subnetmask     \"Subnetmask [%s]\"                            &lt;info&gt;        (Sonoff)    { mqtt=\"&lt;[openhabPI:sonoff/6001949C6548/stat/STATUS5:state:JSONPATH($.StatusNET.Subnetmask)]\", autoupdate=\"true\"  }\n</code></pre> <p>OK. Haven't you noticed something? Look closer. :) That's it, the topics:</p> <ul> <li>sonoff/6001949C6548/stat/STATUS5</li> <li>sonoff/6001949C6548/stat/STATUS11</li> <li>sonoff/6001949C6548/stat/STATUS2</li> </ul> <p>This information isn't posted automatically, or after a certain amount of time. In order to receive them you have to publish <code>0</code> to <code>sonoff/6001949C6548/cmnd/STATUS</code> topic.  You can try this with <code>mosquitto_sub</code> command: <pre><code>mosquitto_pub -h 172.16.0.250 -u ***** -P ***** -t sonoff/B4E62D14BE5A/cmnd/STATUS -m 0\n</code></pre> And the result (from a POW module): <pre><code>sonoff/B4E62D14BE5A/stat/STATUS {\"Status\":{\"Module\":6,\"FriendlyName\":[\"Sonoff\"],\"Topic\":\"sonoff\",\"ButtonTopic\":\"0\",\"Power\":1,\"PowerOnState\":1,\"LedState\":6,\"SaveData\":1,\"SaveState\":1,\"ButtonRetain\":0,\"PowerRetain\":0}}\n\nsonoff/B4E62D14BE5A/stat/STATUS1 {\"StatusPRM\":{\"Baudrate\":115200,\"GroupTopic\":\"sonoffs\",\"OtaUrl\":\"http://sonoff.maddox.co.uk/tasmota/sonoff.bin\",\"RestartReason\":\"Software/System restart\",\"Uptime\":\"19T13:57:19\",\"StartupUTC\":\"2018-07-11T03:38:44\",\"Sleep\":0,\"BootCount\":18,\"SaveCount\":171,\"SaveAddress\":\"F9000\"}}\n\nsonoff/B4E62D14BE5A/stat/STATUS2 {\"StatusFWR\":{\"Version\":\"5.14.0\",\"BuildDateTime\":\"2018-05-15T15:29:54\",\"Boot\":31,\"Core\":\"2_3_0\",\"SDK\":\"1.5.3(aec24ac9)\"}}\n\nsonoff/B4E62D14BE5A/stat/STATUS3 {\"StatusLOG\":{\"SerialLog\":2,\"WebLog\":2,\"SysLog\":0,\"LogHost\":\"\",\"LogPort\":514,\"SSId\":[\"Vinyo-Net\",\"\"],\"TelePeriod\":30,\"SetOption\":[\"00008009\",\"55818000\"]}}\n\nsonoff/B4E62D14BE5A/stat/STATUS4 {\"StatusMEM\":{\"ProgramSize\":526,\"Free\":476,\"Heap\":19,\"ProgramFlashSize\":1024,\"FlashSize\":4096,\"FlashMode\":3}}\n\nsonoff/B4E62D14BE5A/stat/STATUS5 {\"StatusNET\":{\"Hostname\":\"sonoff_B4E62D14BE5A\",\"IPAddress\":\"172.20.1.9\",\"Gateway\":\"172.16.0.1\",\"Subnetmask\":\"255.240.0.0\",\"DNSServer\":\"172.16.0.1\",\"Mac\":\"B4:E6:2D:14:BE:5A\",\"Webserver\":2,\"WifiConfig\":2}}\n\nsonoff/B4E62D14BE5A/stat/STATUS6 {\"StatusMQT\":{\"MqttHost\":\"172.16.0.250\",\"MqttPort\":1883,\"MqttClientMask\":\"DVES_%06X\",\"MqttClient\":\"DVES_14BE5A\",\"MqttUser\":\"vinyo\",\"MqttType\":1,\"MAX_PACKET_SIZE\":1000,\"KEEPALIVE\":15}}\n\nsonoff/B4E62D14BE5A/stat/STATUS7 {\"StatusTIM\":{\"UTC\":\"Mon Jul 30 17:36:03 2018\",\"Local\":\"Mon Jul 30 19:36:03 2018\",\"StartDST\":\"Sun Mar 25 02:00:00 2018\",\"EndDST\":\"Sun Oct 28 03:00:00 2018\",\"Timezone\":2,\"Sunrise\":\"06:21\",\"Sunset\":\"21:32\"}}\n\nsonoff/B4E62D14BE5A/stat/STATUS9 {\"StatusPTH\":{\"PowerDelta\":80,\"PowerLow\":0,\"PowerHigh\":0,\"VoltageLow\":0,\"VoltageHigh\":0,\"CurrentLow\":0,\"CurrentHigh\":0}}\n\nsonoff/B4E62D14BE5A/stat/STATUS10 {\"StatusSNS\":{\"Time\":\"2018-07-30T19:36:03\",\"ENERGY\":{\"Total\":40.988,\"Yesterday\":2.198,\"Today\":0.058,\"Power\":3,\"Factor\":0.09,\"Voltage\":220,\"Current\":0.161}}}\n\nsonoff/B4E62D14BE5A/stat/STATUS11 {\"StatusSTS\":{\"Time\":\"2018-07-30T19:36:03\",\"Uptime\":\"19T13:57:19\",\"Vcc\":3.143,\"POWER\":\"ON\",\"Wifi\":{\"AP\":1,\"SSId\":\"Vinyo-Net\",\"RSSI\":52,\"APMac\":\"6C:3B:6B:A0:D2:79\"}}}\n</code></pre></p> <p>All these information can be displayed in OpenHAB if you want.</p> <p>Only one thing left: as I mentioned before these topics aren't updated automatically, so you need a button for update them, but first an item must be created: <pre><code>Switch prod_sonoff_6001949C6548_update         \"UpdateInfo\"                                 &lt;update&gt;      (Sonoff)    { mqtt=\"&gt;[openhabPI:sonoff/6001949C6548/cmnd/STATUS:command:*:0]\", autoupdate=\"false\"}\n</code></pre> Regardless of the switch command (ON/OFF) we have to publish <code>0</code> to the device: <code>command:*:0</code></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#42-openhab-sitemap","title":"4.2. OpenHAB Sitemap","text":"<p>Now we have the items configured, but I think it would be useful to display them in the OpenHAB. :) To do that we have to configure our sitemap, as well.</p> <p>Since this post is not an 'OpenHAB how to...', I will show you only one way to use the configured items.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#421-power-switches","title":"4.2.1. Power Switches","text":"<p>Maybe the most useful step if I post the configuration of my 4CH device: <pre><code>Frame label=\"Switches\" {\nText item=prod_sonoff_6001949C6548_lwt\nSwitch item=prod_sonoff_6001949C6548_switch1 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"]\nSwitch item=prod_sonoff_6001949C6548_switch2 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"]\nSwitch item=prod_sonoff_6001949C6548_switch3 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"]\nSwitch item=prod_sonoff_6001949C6548_switch4 visibility=[prod_sonoff_6001949C6548_lwt==\"Online\"]\n}\n</code></pre></p> <p>The only interesting thing in this configuration is the <code>visibility</code> part. Visibility configurations are intended to hide the switches when the device is not in \"Online\" state. You can skip this part, but I think it makes no sense to switch an unavailable device. </p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#422-display-information","title":"4.2.2. Display Information","text":"<pre><code>Frame label=\"Info\" {\nText item=prod_sonoff_6001949C6548__RSSI\nText item=prod_sonoff_6001949C6548_uptime\n\nSwitch item=prod_sonoff_6001949C6548_update mappings=[ON=\"Go!\"]\nText item=prod_sonoff_6001949C6548_Time visibility=[prod_sonoff_6001949C6548_Time!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_fwver visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_hostname visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_BuildDateTime visibility=[prod_sonoff_6001949C6548_fwver!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_Vcc visibility=[prod_sonoff_6001949C6548_Vcc!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_SSId visibility=[prod_sonoff_6001949C6548_SSId!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_IPAddress visibility=[prod_sonoff_6001949C6548_IPAddress!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_Mac visibility=[prod_sonoff_6001949C6548_Mac!=\"NULL\"]\nText item=prod_sonoff_6001949C6548_Subnetmask visibility=[prod_sonoff_6001949C6548_Subnetmask!=\"NULL\"]\n} // END : label=\"Info\"\n</code></pre> <p>The RSSI and uptime values are posted with the telemetry information, so these are updated regularly. But the <code>items</code> after the <code>Swtich</code> item are updated only when you post <code>0</code> to the <code>STATUS</code> topic (Section: 4.1.3. Other Examples).</p> <p>To make it much more understandable here is the <code>item</code> configuration again: <pre><code>Switch prod_sonoff_6001949C6548_update         \"UpdateInfo\"                                 &lt;update&gt;      (Sonoff)    { mqtt=\"&gt;[openhabPI:sonoff/6001949C6548/cmnd/STATUS:command:*:0]\", autoupdate=\"false\"}  \n</code></pre></p> <p>And all these items are displayed only if they are not \"NULL\". </p> <p>I hope it is clear. If you read this article carefully you can configure your own device.</p> <p>Finally here are two screenshot about how it should look like:</p> <p></p> <p></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#5-use-tasmota-fw-with-your-own-setup","title":"5. Use Tasmota FW With Your Own Setup","text":"<p>And last but not least, this is my bonus chapter. :) What is this chapter about? Since Tasmota is an brilliant OpenSource software and Sonoff devices are base on ESP8266 OpenSource hardware you can build your own smart home switch.</p> <p>You will need:</p> <ul> <li>ESP8266 module (ESP01, ESP07, NodeMCU devkit, etc).</li> <li>Relay (1CH, 2CH, etc) 5V</li> <li>5V Power Supply</li> <li>5V to 3V3 converter</li> <li>etc. :)</li> </ul> <p>Please take a look at my 4CH setup in the beginning of this post for details. Of course you can create your own PCB, as well. :)</p> <p>To customize Tasmota you will need an IDE (**I**ntegrated **D**evelopment **E**nvironment). If you do a google search for \"compile tasmota\" the first hit is exactly what we need. :) Beginner Guide Create your own Firmware Build So please forgive me, but I don't bother with a guide of \"How to install and configure Atom\", especially because it is only a few steps.</p> <p>As I mentioned, some times, I'm not a developer so maybe my explanations are not always 100% right or clear, but always try to give you working solutions.</p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#51-user_configh","title":"5.1. <code>user_config.h</code>","text":"<p>It this file you can pre-configure some values, most of them are configurable via the web interface. </p> <p>Firstly, the most important thing is the Wifi set up. Why? Please, imagine the situation when you use an ESP01 and you used up all its GPIO port for switching relays. Why is it a problem? Because you don't have any GPIO pin left for a button, which is essential to put the device in Wifi config mode. You can pre-configure two different stations: <pre><code>#define STA_SSID1              \"My Wifi Station\"\n#define STA_PASS1              \"UnbrakeablePassword\"\n#define STA_SSID2              \"\"\n#define STA_PASS2              \"\"\n</code></pre> After you compile your firmware with these setup the device will automatically connect to your wifi network.</p> <p>If you scroll down a bit in the <code>user_config.h</code>, you can find the MQTT related settings. There is no reason to leave it blank. :) Example: <pre><code>#define MQTT_USE               1\n\n#define MQTT_HOST              \"172.16.0.250\"\n#define MQTT_FINGERPRINT1      \"A5 02 FF 13 99 9F 8B 39 8E F1 83 4F 11 23 65 0B 32 36 FC 07\"\n#define MQTT_FINGERPRINT2      \"A5 02 FF 13 99 9F 8B 39 8E F1 83 4F 11 23 65 0B 32 36 FC 07\"\n#define MQTT_PORT              1883\n#define MQTT_USER              \"userName\"\n#define MQTT_PASS              \"Password\" \n</code></pre> Please modify only the necessary fields. Moreover, you can configure the topic: <pre><code>#define MQTT_FULLTOPIC         \"%topic%//%prefix%/\"\n</code></pre></p> <p>Some other exciting options:</p> <ul> <li>NTP server: <pre><code>#define NTP_SERVER1            \"pool.ntp.org\"       \n#define NTP_SERVER2            \"nl.pool.ntp.org\"    \n#define NTP_SERVER3            \"0.nl.pool.ntp.org\" \n</code></pre></li> <li>Time Zone: <pre><code>#define APP_TIMEZONE           1  \n</code></pre></li> <li> <p>Switch Mode: <pre><code>#define SWITCH_MODE            TOGGLE            // [SwitchMode] TOGGLE, FOLLOW, FOLLOW_INV, PUSHBUTTON, PUSHBUTTON_INV, PUSHBUTTONHOLD, PUSHBUTTONHOLD_INV, PUSHBUTTON_TOGGLE (the wall switch state)\n</code></pre></p> </li> <li> <p>MQTT retain: <pre><code>#define MQTT_TELE_RETAIN     0                   // Tele messages may send retain flag (0 = off, 1 = on)\n</code></pre></p> </li> <li> <p>And so on, and so on.... Please scroll down the file and configure everything you need.</p> </li> <li>One more thing: In this file you have to configure which sensors you plan to use with your device, but do not select too many of them, because you can easily run into OOM exception. </li> </ul>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#52-sonoff_templateh","title":"5.2. <code>sonoff_template.h</code>","text":"<p>Here I had the feeling that I should know more about programming. :) </p> <p>First create your own template. Example: <pre><code>  { \"Sonoff Custom\",   // Sonoff Basic (ESP8266)\n     GPIO_REL1_INV,    // GPIO00 Button\n     GPIO_USER,        // GPIO01 Serial RXD and Optional sensor\n     GPIO_REL2_INV,    // GPIO02\n     GPIO_USER,        // GPIO03 Serial TXD and Optional sensor\n     GPIO_USER,        // GPIO04 Optional sensor\n     0,                // GPIO05\n     GPIO_USER,        // GPIO06 (SD_CLK   Flash)\n     0,                // GPIO07 (SD_DATA0 Flash QIO/DIO/DOUT)\n     0,                // GPIO08 (SD_DATA1 Flash QIO/DIO/DOUT)\n     0,                // GPIO09 (SD_DATA2 Flash QIO)\n     0,                // GPIO10 (SD_DATA3 Flash QIO)\n     0,                // GPIO11 (SD_CMD   Flash)\n     0,                // GPIO12 Red Led and Relay (0 = Off, 1 = On)\n     GPIO_LED1_INV,    // GPIO13 Green Led (0 = On, 1 = Off)\n     GPIO_USER,        // GPIO14 Optional sensor\n     0,                // GPIO15\n     0,                // GPIO16\n     0                 // ADC0 Analog input\n  },\n</code></pre> As you can see I modified the template of the \"Basic\" module. My first goal was to use an ESP01 with Tasmota. ESP01 modules have only 2 usable GPIO pins (GPIO0 and GPIO2). Both of them are connected to the relay: <code>GPIO_REL1_INV</code>,<code>GPIO_REL2_INV</code>. Originally the GPIO0 was used to connect it with a button (I haven't modified the comment here on purpose for demonstration.)</p> <p>What functionality can be used for the PINs? You can find all available functions here: </p> <pre><code>// User selectable GPIO functionality\nenum UserSelectablePins {\n  GPIO_NONE,           // Not used\n  GPIO_DHT11,          // DHT11\n  GPIO_DHT22,          // DHT21, DHT22, AM2301, AM2302, AM2321\n...\n...\n  GPIO_REL1,           // Relays\n  GPIO_REL2,\n...\n...\n  GPIO_REL1_INV,\n  GPIO_REL2_INV,\n...\n...\n</code></pre> <p>So what is the difference between <code>GPIO_REL1</code> and <code>GPIO_REL1_INV</code>? This configuration is related to <code>NO</code> (**N**ormally **O**pen) and <code>NC</code> (**N**ormally **C**losed) setup of the relay (if applicable).  Most relays have 3 connections: NC, NO, COM (common). When you set the PIN to low:</p> <ul> <li>NC means that the circuit is closed.</li> <li>NO means that the circuit is open.</li> </ul> <p>When you set the PIN to high the meanings of NC an NO are reverse.</p> <p>So the usage of <code>GPIO_REL1</code> and <code>GPIO_REL1_INV</code> depends on your hardware setup. If you connect your stuff (which you want to turn OFF and ON) to <code>NO</code> you want to use the inverse version, because the relay will close the circuit when the PIN is put to low (0) state. If you are using <code>NC</code>, you should choose the <code>GPIO_REL1</code>.  I hope this is clear for you, if not, give it a try. :)</p> <p>There is special functionality which is the <code>GPIO_USER</code>. If you set a GPIO pin to this, you will be able to select its functionality on the web interface: </p> <p>And where does the list come from?  <pre><code>// Text in webpage Module Parameters and commands GPIOS and GPIO\nconst char kSensorNames[] PROGMEM =\n  D_SENSOR_NONE \"|\"\n...\n...\n</code></pre></p>"},{"location":"Old_Blog_Contents/other/Sonoff_Relays_With_Openhab_And_Tasmota_Firmware/#53-example","title":"5.3. Example","text":"<p>Maybe it will be more clear if I show you an example. For demonstration I used a NodeMCU DevKit v0.9, 1CH 5V relay and a DHT22 sensor.</p> <p>Hardware connections:</p> <ul> <li>DHT22</li> <li>VCC --&gt; 3V3</li> <li>GND --&gt; GND</li> <li>DATA --&gt; D2</li> <li>Relay</li> <li>VCC --&gt; 5V</li> <li>GND --&gt; GND</li> <li>INPUT --&gt; D3</li> </ul> <p>To be able to choose the appropriate GPIO pins we have to know what D2 and D3 means.  You can see in the picture that D1 is actually the GPIO5 and D2 is the GPIO4.</p> <p>So the following modifications are needed in the <code>sonoff_template.h</code> file: <pre><code>--- comp/Sonoff-Tasmota-development/sonoff/sonoff_template.h    2018-07-31 20:10:22.000000000 +0200\n+++ Sonoff-Tasmota-development/sonoff/sonoff_template.h 2018-08-01 19:45:49.785921996 +0200\n@@ -157,6 +157,7 @@\n // Supported hardware modules\n enum SupportedModules {\n   SONOFF_BASIC,\n+  BLOGTST,\n   SONOFF_RF,\n   SONOFF_SV,\n   SONOFF_TH,\n@@ -220,6 +221,7 @@\n\n const uint8_t kNiceList[MAXMODULE] PROGMEM = {\n   SONOFF_BASIC,\n+  BLOGTST,\n   SONOFF_RF,\n   SONOFF_TH,\n   SONOFF_DUAL,\n@@ -288,6 +290,26 @@\n      0,                // GPIO16\n      0                 // ADC0 Analog input\n   },\n+  { \"Sonoff Blogtst\", // Sonoff Basic (ESP8266)\n+     GPIO_KEY1,        // GPIO00 Button\n+     GPIO_USER,        // GPIO01 Serial RXD and Optional sensor\n+     0,                // GPIO02\n+     GPIO_USER,        // GPIO03 Serial TXD and Optional sensor\n+     GPIO_DHT22,        // GPIO04 Optional sensor\n+     GPIO_REL1,        // GPIO05\n+     0,                // GPIO06 (SD_CLK   Flash)\n+     0,                // GPIO07 (SD_DATA0 Flash QIO/DIO/DOUT)\n+     0,                // GPIO08 (SD_DATA1 Flash QIO/DIO/DOUT)\n+     0,                // GPIO09 (SD_DATA2 Flash QIO)\n+     0,                // GPIO10 (SD_DATA3 Flash QIO)\n+     0,                // GPIO11 (SD_CMD   Flash)\n+     0,        // GPIO12 Red Led and Relay (0 = Off, 1 = On)\n+     GPIO_LED1_INV,    // GPIO13 Green Led (0 = On, 1 = Off)\n+     GPIO_USER,        // GPIO14 Optional sensor\n+     0,                // GPIO15\n+     0,                // GPIO16\n+     0                 // ADC0 Analog input\n+  },\n   { \"Sonoff RF\",       // Sonoff RF (ESP8266)\n      GPIO_KEY1,        // GPIO00 Button\n      GPIO_USER,        // GPIO01 Serial RXD and Optional sensor\n@@ -970,4 +992,4 @@\n\n */\n\n-#endif  // _SONOFF_TEMPLATE_H_\n\\ No newline at end of file\n+#endif  // _SONOFF_TEMPLATE_H_\n</code></pre></p> <p>I defined a new template for my setup with \"Sonoff Blogtst\" name based on the \"Sonoff Basic\" module template.  The following 3 lines were modified (GPIO04,05,12): <pre><code>+     GPIO_DHT22,        // GPIO04 Optional sensor\n+     GPIO_REL1,        // GPIO05\n+     0,        // GPIO12 Red Led and Relay (0 = Off, 1 = On)\n</code></pre> * GPIO04 is connected to my DHT22 sensor. * GPIO05 is connected to the relay \"IN\" pin. * Originally the Sonoff Basic modules use the GPIO12 to control the relay, but we don't, so set it to \"0\".</p> <p>Important</p> <p>If you define a completely new template you have to add two additional item to two different array: <code>kNiceList</code>, <code>SupportedModules</code>.</p> <p><pre><code> enum SupportedModules {\n   SONOFF_BASIC,\n+  BLOGTST,\n</code></pre> <pre><code>const uint8_t kNiceList[MAXMODULE] PROGMEM = {\n   SONOFF_BASIC,\n+  BLOGTST,\n</code></pre> And in the right order! Example: if you write your new template definition before the \"Sonoff Basic\" definition, you should put <code>BLOGTST</code> before <code>SONOFF_BASIC</code> in the kNiceList and SupportedModules arrays, as well.</p> <p>You are almost done. Since the NodeMCU DevKit has \"user\" button we have two options:</p> <ul> <li>Configure the firmware to use this button. In this scenario you can use the user button to put the device to WiFi configuration mode. If you scroll back a bit to the picture about the pinout you can see that the button is connected to GPIO16. If you choose this option set the GPIO16 from <code>0</code> to <code>GPIO_KEY1</code>.</li> <li>Configure the WiFi parameters in the <code>user_config.h</code>. To do this you should simply modify the <code>STA_SSID1</code> and <code>STA_PASS1</code>. Example: <pre><code>--- comp/Sonoff-Tasmota-development/sonoff/user_config.h    2018-07-31 20:10:22.000000000 +0200\n+++ Sonoff-Tasmota-development/sonoff/user_config.h 2018-08-01 19:34:45.629732934 +0200\n@@ -59,8 +59,8 @@\n #define WIFI_SUBNETMASK        \"255.255.255.0\"   // [IpAddress3] If not using DHCP set Network mask\n #define WIFI_DNS               \"192.168.2.27\"    // [IpAddress4] If not using DHCP set DNS IP address (might be equal to WIFI_GATEWAY)\n\n-#define STA_SSID1              \"\"                // [Ssid1] Wifi SSID\n-#define STA_PASS1              \"\"                // [Password1] Wifi password\n+#define STA_SSID1              \"**********\"                // [Ssid1] Wifi SSID\n+#define STA_PASS1              \"**********\"                // [Password1] Wifi password\n #define STA_SSID2              \"\"                // [Ssid2] Optional alternate AP Wifi SSID\n #define STA_PASS2              \"\"                // [Password2] Optional alternate AP Wifi password\n #define WIFI_CONFIG_TOOL       WIFI_WAIT         // [WifiConfig] Default tool if wifi fails to connect\n</code></pre> To use DHCP (default) take a look at this line: <pre><code>#define WIFI_IP_ADDRESS        \"0.0.0.0\"         // [IpAddress1] Set to 0.0.0.0 for using DHCP or IP address\n</code></pre></li> </ul> <p>Next steps: build! &amp; upload. :) I've already written about uploading the firmware, but here is the example: <pre><code>sudo esptool.py --port /dev/ttyUSB0 write_flash -fs 1MB -fm dout 0x0 firmware.bin\n</code></pre></p> <p>Open the web interface (configure module option) and select the newly configured template:</p> <p></p> <p>If everything is fine, after the reboot you should see the temperature and humidity values on the main page:</p> <p></p> <p>And, of course you can turn the relay on and off with the \"Toggle\" button.</p> <p>In a nutshell this is how you can use Tasmota firmware with your own hardware setup in nutshell. </p> <p>Maybe my explanations are not always the best, but I really hope this post is useful for you in case you want to work with Sonoff / Tasmota / OpenHAB or both of them... :) If you understand all these things I'm pretty sure you can build you own setup, even with custom hardware. I think at this time Sonoff+Tasmota is the cheapest solution to control equipment with OpenHab. Maybe with custom hardware setup could be cheaper but not much, and you should be aware of the time of assembling.</p>"},{"location":"Old_Blog_Contents/raspberry/Compile_Go_Language_On_Raspberry_Pi/","title":"Compile Go Language On Raspberry Pi","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/raspberry/Compile_Go_Language_On_Raspberry_Pi/#compile-go-language-on-raspberry-pi","title":"Compile GO language on Raspberry PI","text":"<p>There are many ways to install GO language, eg. you can compile from source or use the pre-compiled binaries. Here I want to show you how you can compile GO from source.</p>"},{"location":"Old_Blog_Contents/raspberry/Compile_Go_Language_On_Raspberry_Pi/#1-prerequirements","title":"1. PreRequirements:","text":"<ul> <li>Raspberry PI 2 or 3</li> <li>Raspbian I used this version: <code>Linux raspberrypi 4.4.13-v7+ #894 SMP Mon Jun 13 13:13:27 BST 2016 armv7l GNU/Linux</code> <code>2016-05-27-raspbian-jessie.zip</code> can be downloaded from the official RPI page.</li> <li>gcc</li> <li> <p>And you check these links:  </p> <ul> <li>System Requirements And Install </li> <li>Installing Go from source </li> <li>Official GO git repo </li> <li>GitHub repo</li> </ul> </li> </ul>"},{"location":"Old_Blog_Contents/raspberry/Compile_Go_Language_On_Raspberry_Pi/#2-install-from-source-using-git","title":"2. Install from source using git","text":"<p>In order to install the latest <code>go</code> you have to install 1.4.x version first. I followed this useful link. </p> <ul> <li>create a dedicated user: <code>gcp</code> <code>root@raspberrypi:~# useradd gcp</code></li> <li>I set the home dir to /opt/google-cloud-print <pre><code>cat /etc/passwd|grep gcp\ngcp:x:1002:1002::/opt/google-cloud-print:/bin/bash\n</code></pre></li> <li>Clone the repository from GIT: <code>git clone https://github.com/golang/go</code></li> <li>Rename and copy \u00e2\u20ac\u0153go\u00e2\u20ac directory for different versions: <pre><code>mv go go1.4.3\ncp -r go1.4.3 go1.5.2\ncp -r go1.4.3 go1.6.3\n</code></pre></li> <li> <p>Checkout the versions <pre><code>cd go1.4.3/\ngit checkout go1.4.3\ncd ..\n\ncd go1.5.2/\ngit checkout go1.5.2\ncd ..\n\ncd go1.6.3/\ngit checkout go1.6.3\n</code></pre> As you can see I will compile 3 different versions: 1.4.3, 1.5.2 and the latest one: 1.6.3.</p> </li> <li> <p>First of all we will compile 1.4.3: <pre><code>cd /opt/google-cloud-print/git/go1.4.3/src\n./all.bash\n</code></pre></p> </li> <li>When the compilation is successfully finished, you have to set some system environments: <pre><code>export GOROOT_BOOTSTRAP=/opt/google-cloud-print/git/go1.4.3\nexport GOROOT=\"/opt/google-cloud-print/git/go1.4.3\"\nexport GOROOT_BOOTSTRAP=\"$GOROOT\"\nexport GOPATH=\"$GOROOT/src\"\nexport PATH=\"$PATH:$GOROOT/bin\"\n</code></pre></li> <li>Next compile the newer versions.  I move forward with 1.6.x: <pre><code>cd /opt/google-cloud-print/git/go1.6.3/src\n./all.bash\n</code></pre></li> </ul> <p>Note</p> <p>You can run into \"not enough\" memory issue while package is being tested, but despite the errors <code>go</code> will work properly.</p> <p>If everything was fine you use different versions of GO, by exporting system environments. I created 3 files to manage different versions:</p> <p>setGoEnv1.4.3.sh <pre><code>#!/bin/bash\nexport GOROOT=\"/opt/google-cloud-print/git/go1.4.3\"\nexport GOROOT_BOOTSTRAP=\"$GOROOT\"\nexport GOPATH=\"$GOROOT/src\"\nexport PATH=\"$PATH:$GOROOT/bin\"\n</code></pre> setGoEnv1.5.2.sh <pre><code>#!/bin/bash\nexport GOROOT=\"/opt/google-cloud-print/git/go1.5.2\"\nexport GOROOT_BOOTSTRAP=\"$GOROOT\"\nexport GOPATH=\"$GOROOT/src\"\nexport PATH=\"$PATH:$GOROOT/bin\"\n</code></pre></p> <p>setGoEnv1.6.3.sh <pre><code>#!/bin/bash\nexport GOROOT=\"/opt/google-cloud-print/git/go1.6.3\"\nexport GOROOT_BOOTSTRAP=\"$GOROOT\"\nexport GOPATH=\"$GOROOT/src\"\nexport PATH=\"$PATH:$GOROOT/bin\"\n</code></pre> * The final step is to check if GO working correctly or not. Directory Structure: <pre><code>gcp@raspberrypi:~/git$ pwd\n/opt/google-cloud-print/git\ngcp@raspberrypi:~/git$ ls -al\ntotal 32\ndrwxr-xr-x  5 gcp gcp 4096 Aug 11 14:35 .\ndrwxr-xr-x  4 gcp gcp 4096 Aug 11 11:55 ..\ndrwxr-xr-x 12 gcp gcp 4096 Aug 11 10:49 go1.4.3\ndrwxr-xr-x 11 gcp gcp 4096 Aug 11 11:17 go1.5.2\ndrwxr-xr-x 11 gcp gcp 4096 Aug 11 13:57 go1.6.3\n-rwxr-xr-x  1 gcp gcp  158 Aug 11 14:34 setGoEnv1.4.3.sh\n-rwxr-xr-x  1 gcp gcp  158 Aug 11 14:35 setGoEnv1.5.2.sh\n-rwxr-xr-x  1 gcp gcp  158 Aug 11 14:35 setGoEnv1.6.3.sh\n</code></pre> Check versions: <pre><code>gcp@raspberrypi:~/git$ . setGoEnv1.5.2.sh\ngcp@raspberrypi:~/git$ go version\ngo version go1.5.2 linux/arm\n</code></pre> <pre><code>gcp@raspberrypi:~/git$ source setGoEnv1.4.3.sh \ngcp@raspberrypi:~/git$ go version\ngo version go1.4.3 linux/arm\n</code></pre></p> <p>Note</p> <p>Please keep in mind that you have to log out from the current shell after using one of the installed go, because the scripts I\u00e2\u20ac\u2122m using to set the environment which will concatenate the PATH after each run. For example you set the sysenv to use 1.4.3, the script will add <code>/opt/google-cloud-print/git/go1.4.3/bin/</code> directory to the PATH env. Or you have to remove the current go path from system <code>PATH</code> before sourceing another environment.</p>"},{"location":"Old_Blog_Contents/raspberry/Compile_Go_Language_On_Raspberry_Pi/#3-install-from-downloaded-source","title":"3. Install from downloaded source","text":"<p>You can download source code form the offical web page: https://golang.org/dl/</p> <p>Note</p> <p>As you can see there are precompiled binary file for ARM CPUs. But only for ARMv6! Be sure which type of CPU you are using before downloading GO. (Raspberry 2 has ARMv7 CPU!) The install steps are the same as in case of using GIT. Just for fun I will show you how easy to use these official source codes.</p> <ul> <li>Download the source code of version 1.4.3.  <pre><code>gcp@raspberrypi:~/sources$ wget https://storage.googleapis.com/golang/go1.4.3.src.tar.gz\n--2016-08-12 16:19:08--  https://storage.googleapis.com/golang/go1.4.3.src.tar.gz\nResolving storage.googleapis.com (storage.googleapis.com)... 216.58.208.48, 2a00:1450:4001:817::2010\nConnecting to storage.googleapis.com (storage.googleapis.com)|216.58.208.48|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10875170 (10M) [application/octet-stream]\nSaving to: \u00e2\u20ac\u02dcgo1.4.3.src.tar.gz.1\u00e2\u20ac\u2122\n\ngo1.4.3.src.tar.gz.1                            100%[=======================================================================================================&gt;]  10.37M  5.26MB/s   in 2.0s   \n\n2016-08-12 16:19:10 (5.26 MB/s) - \u00e2\u20ac\u02dcgo1.4.3.src.tar.gz.1\u00e2\u20ac\u2122 saved [10875170/10875170]\n</code></pre></li> <li>Extract <pre><code>gcp@raspberrypi:~/sources$ tar xf go1.4.3.src.tar.gz \ngcp@raspberrypi:~/sources$ mv go go1.4.3\ngcp@raspberrypi:~/sources$ \n</code></pre></li> <li>Compile <pre><code>gcp@raspberrypi:~/sources/go1.4.3/src$ ./all.bash \n</code></pre></li> </ul> <p>After the compilation is done you can compile later versions of <code>go</code>. Don't forget to set system environments before compiling.</p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/","title":"Google Cloud Print With Orange Pi or Rpi","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#google-cloud-print-with-orange-pi-or-rpi","title":"Google Cloud Print With Orange PI (or RPI)","text":""},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#-1-update-your-system-install-requirements","title":"-1. Update your system &amp; Install Requirements","text":"<pre><code>apt-get update\napt-ge upgrade\napt-get install gcc make # If they aren't installed (for golang)\napt-get install build-essential libcups2-dev libavahi-client-dev git bzr # for cloud connector\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#0-install-go-language","title":"0. Install Go Language","text":"<p>Install go lang. You can follow \"Compile GO language on Raspberry PI\" to complete this step.  I don't want to write detailed install steps here, instead of I paste only the commands to this chapter without any / detailed comments. (Maybe it will helpful for someone).</p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#1-clone-go","title":"1. Clone Go","text":"<pre><code>mkdir /opt/go/\ncd /opt/go/\ngit clone https://github.com/golang/go`\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#2-checkout-go","title":"2. Checkout Go","text":"<pre><code>cd /opt/go/go\ngit tag -l\ncd ..\nmv go go1.4.3\ncp -r go1.4.3 go1.6.3\ncd go1.4.3/\ngit checkout go1.4.3 \ncd /opt/go/go1.6.3\ngit checkout go1.6.3 \n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#3-compile-go-143","title":"3. Compile GO 1.4.3","text":"<p><pre><code>export GOARCH=\"arm\"\nexport GOOS=\"linux\"\nexport GOARM=\"7\"\ncd /opt/go/go1.4.3/src\n/opt/go/go1.4.3/src\n./all.bash | tee -a output.log\n</code></pre> Sample Output: Link</p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#4-set-system-environments","title":"4. Set System environments","text":"<pre><code>export GOROOT_BOOTSTRAP=\"/opt/go/go1.4.3\"  \nexport GOROOT=\"/opt/go/go1.4.3\"  \nexport GOROOT_BOOTSTRAP=\"$GOROOT\"  \nexport GOPATH=\"$GOROOT/src\"  \nexport PATH=\"$PATH:$GOROOT/bin\" \nexport GOARCH=\"arm\"\nexport GOOS=\"linux\"\nexport GOARM=\"7\"\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#5-some-performance-tuning","title":"5. Some Performance tuning","text":"<pre><code>fallocate --length 2GiB /root/2G.swap  \nchmod 0600 /root/2G.swap  \nmkswap /root/2G.swap  \nswapon /root/2G.swap \n\nulimit -s 65536\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#6-compile-go-163","title":"6. Compile GO 1.6.3","text":"<p><pre><code>cd /opt/go/go1.6.3/src\n./all.bash | tee -a output.log\n</code></pre> Sample output: Link</p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#7-set-system-environments-to-use-go163","title":"7. Set System Environments to use go1.6.3.","text":"<p>You should unset all GO related ENV, or logout from current shell and create new shell. <pre><code>export GOROOT_BOOTSTRAP=\"/opt/go/go1.4.3\"  \nexport GOROOT=\"/opt/go/go1.6.3\"  \nexport GOROOT_BOOTSTRAP=\"$GOROOT\"  \nexport GOPATH=\"$GOROOT/src\"  \nexport PATH=\"$PATH:$GOROOT/bin\"  \nexport GOARCH=\"arm\"\nexport GOOS=\"linux\"\nexport GOARM=\"7\"\n</code></pre></p> <p>Check: <pre><code>go version\ngo version go1.6.3 linux/arm\n</code></pre> Your GO is ready to use. :)</p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#install-google-cloud-print-connector","title":"Install Google Cloud Print connector","text":"<p>Simply run the following command: <code>go get github.com/google/cloud-print-connector/...</code> Go will download and install packages and dependencies.</p> <p>Q: What means <code>...</code>? A: if you want all packages in that repository, use <code>...</code></p> <p>Reference: https://github.com/google/cloud-print-connector/wiki/Build-from-source Run automatically on system boot (systemd): https://github.com/google/cloud-print-connector/wiki/Run-Connector-Automatically-on-Boot</p> <p>This will create two new files to <code>/opt/go/go1.6.3/src/bin</code> directory: <pre><code>root@OrangePI:/opt/go/go1.6.3/src/bin# ls -al\ntotal 17504\ndrwxr-xr-x  2 root root    4096 Sep  6 13:11 .\ndrwxr-xr-x 47 root root    4096 Sep  6 13:11 ..\n-rwxr-xr-x  1 root root 8358992 Sep  6 13:11 gcp-connector-util\n-rwxr-xr-x  1 root root 9554384 Sep  6 13:11 gcp-cups-connector\n</code></pre></p> <p>And the source is available int <code>/opt/go/go1.6.3/src/src/github.com/google/cloud-print-connector</code></p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#configure-google-cloud-print-connector","title":"Configure Google Cloud Print connector","text":"<p>To configure Google Cloud Print (GCP) you simply run <code>gcp-connector-util</code> from <code>/opt/go/go1.6.3/src/bin</code> directory, answer some question.</p> <pre><code>root@OrangePI:/opt/go/go1.6.3/src/bin# ./gcp-connector-util init\n\"Local printing\" means that clients print directly to the connector via\nlocal subnet, and that an Internet connection is neither necessary nor used.\n\nEnable local printing?\nyes\n\n\"Cloud printing\" means that clients can print from anywhere on the Internet,\nand that printers must be explicitly shared with users.\nEnable cloud printing?\nyes\n\nRetain the user OAuth token to enable automatic sharing?\nno\n\nProxy name for this connector:\nopi_sam_ml1510\n\nVisit https://www.google.com/device, and enter this code. I'll wait for you.\nGGVY-****\n</code></pre> <p>At this point you have to login to your google account, visit https://www.google.com/device and give your code.  After you enter your code you will get the following message on linux box:</p> <pre><code>The config file /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json is ready to rock.\nKeep it somewhere safe, as it contains an OAuth refresh token.\n</code></pre> <p>The next step is checking the configuration:  </p> <ul> <li>Start GCP connector: <code>./gcp-cups-connector --config-filename gcp-cups-connector.config.json --log-to-console</code></li> <li>Check your printer by visiting https://www.google.com/cloudprint/#printers website.If everything went fine you should see your printer. </li> </ul> <p>The last step is change log file location by editing <code>gcp-cups-connector.config.json</code>, and modify <code>log_file_name</code> entry according to where you want to save logs.</p> <p>Run in the background: <code>nohup ./gcp-cups-connector --config-filename gcp-cups-connector.config.json &amp;</code></p>"},{"location":"Old_Blog_Contents/raspberry/Google_Cloud_Print_With_Orange_Pi_or_Rpi/#run-automatically-on-boot","title":"Run Automatically On boot","text":"<p>If you want to start GCP at system boot follow these steps. Reference: https://github.com/google/cloud-print-connector/wiki/Run-Connector-Automatically-on-Boot</p> <ul> <li>You can find a sample config file in <code>/opt/go/go1.6.3/src/src/github.com/google/cloud-print-connector/systemd</code> directory.</li> <li>Make a backup: <code>cp cloud-print-connector.service cloud-print-connector.service-orig</code></li> <li>Edit this file. Modify <code>ExecStart</code> and <code>User</code> property to this: <code>ExecStart=/opt/go/go1.6.3/src/bin/gcp-cups-connector -config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json</code> <code>User=root</code></li> <li>Install systemd service: <code>install -o root -m 0660 cloud-print-connector.service /etc/systemd/system</code></li> <li>Enable the service: <code>systemctl enable cloud-print-connector.service</code></li> <li>Start service: <code>systemctl start cloud-print-connector.service</code></li> <li>Check status: <code>systemctl status cloud-print-connector.service</code></li> </ul> <p>Done. :) Now everytime you restart your OPI GCP will start automatically.</p> <p>Update </p> <p>Unfortunately GCP can't start at boot time because of the following error: <pre><code>systemctl status  cloud-print-connector.service\n\u00e2\u2014\u008f cloud-print-connector.service - Google Cloud Print Connector\n   Loaded: loaded (/etc/systemd/system/cloud-print-connector.service; enabled)\n   Active: failed (Result: start-limit) since Thu 1970-01-01 00:24:04 UTC; 46 years 8 months ago\n     Docs: https://github.com/google/cloud-print-connector\n  Process: 867 ExecStart=/opt/go/go1.6.3/src/bin/gcp-cups-connector --config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json (code=exited, status=1/FAILURE)\n Main PID: 867 (code=exited, status=1/FAILURE)\n\nJan 01 00:24:03 OrangePI systemd[1]: Unit cloud-print-connector.service entered failed state.\nJan 01 00:24:04 OrangePI systemd[1]: cloud-print-connector.service holdoff time over, scheduling restart.\nJan 01 00:24:04 OrangePI systemd[1]: Stopping Google Cloud Print Connector...\nJan 01 00:24:04 OrangePI systemd[1]: Starting Google Cloud Print Connector...\nJan 01 00:24:04 OrangePI systemd[1]: cloud-print-connector.service start request repeated too quickly, refusing to start.\nJan 01 00:24:04 OrangePI systemd[1]: Failed to start Google Cloud Print Connector.\nJan 01 00:24:04 OrangePI systemd[1]: Unit cloud-print-connector.service entered failed state.\n</code></pre></p> <p>I've checked the log files and I saw that after boot the log file was accessed in 1970, and the log entries was also from 1970: <pre><code>I [01/Jan/1970:00:24:03 +0000] Using config file /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json\nI [01/Jan/1970:00:24:03 +0000] Cloud Print Connector for CUPS version DEV-linux\nX [01/Jan/1970:00:24:03 +0000] While starting XMPP, failed to get access token (password): Post https://accounts.google.com/o/oauth2/token: x509: certificate has expired or is not yet valid\n</code></pre></p> <p>Based on these errors I tried to modify \"After\" in cloud print service file to start service after ntp.service and some other services: <code>After=cups.service avahi-daemon.service network-online.target ntp.service networking.service exim4.service ssh.service NetworkManager.service wpa_supplicant.service</code> But It did not help. After some tries I gave up and wrote a shell script to start this service: <pre><code>#!/bin/bash\nIFS='\n'\n\nps aux| grep -v grep  | grep -q gcp-cups-connector\nif [ $? -eq 0 ]\nthen\necho \"Already running\"\nexit 0\nfi\n\nOK=0\nwhile [ $OK -eq 0 ]\ndo\n  YEAR=$( date +%Y )\n  if [ $YEAR -gt 2015 ]\n  then \n    echo ok\n    OK=1\n    /opt/go/go1.6.3/src/bin/gcp-cups-connector --config-filename /opt/go/go1.6.3/src/bin/gcp-cups-connector.config.json\n  else\n    echo \"Sleep for 1 secs\"\n    sleep 1\n  fi\n\ndone\n</code></pre> This little script check the year in every sec and if it is grater then 2015 the GCP will be started. </p> <p>The problem was:</p> <ul> <li>OrangePI has no built in (hw) clock, thus before the ntp service start it doesn't know the current time.</li> <li>GCP was started before the network service came up that's why it could not connect to google service (this happened despite my modification on service file (after property)).</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/","title":"How To Install Nodered On Raspberry Pi","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#how-to-install-nodered-on-raspberry-pi","title":"How To Install NodeRED on Raspberry PI","text":"<p>What is NodeRed?</p> <p>Node-RED is a tool for wiring together hardware devices, APIs and online services in new and interesting ways.</p>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#0-update-your-system","title":"0. Update Your System","text":"<p>As always we start with updating our system. Run this with root:</p> <pre><code>apt-get update ; apt-get upgrade\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#1-create-user-for-nodered-nodejs","title":"1. Create User For NodeRed &amp; NodeJS","text":"<p>We will run node-red with a non-root user.</p> <ul> <li>Add new user: <code>useradd nodered</code></li> <li>Move user's home to /opt/nodered: <code>usermod -m -d /opt/nodered nodered</code></li> <li>Change home directory's owner: <code>chown -R nodered:nodered /opt/nodered/</code></li> <li>Change user's shell: <code>usermod -s /bin/bash nodered</code></li> </ul> <p>Login to user nodemcu: <code>sudo su - nodemcu</code></p>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#2-get-the-necessary-packages","title":"2. Get The Necessary Packages","text":"<p>NodeRed is written in NodeJS, so the first step is to build a NodeJS runtime environment. We will build it from source, but the binaries are also available for multiple platforms on the NodeJS official page.</p> <p>To compile from source we node some packages on our Linux system.^1 In my case I had to install only gcc, g++, clang and make packages with this command: <code>apt-get install gcc g++ clang make</code></p>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#21-download-compile-nodejs","title":"2.1. Download &amp; Compile NodeJS","text":"<ul> <li>Create a directory to store the source codes: <code>mkdir sources</code></li> <li>Change to \"sources\" directory: <code>cd sources</code></li> <li>Download the latest source code: <code>wget https://nodejs.org/dist/v7.2.0/node-v7.2.0.tar.gz</code></li> <li>Untar: <code>tar xf sources/node-v7.2.0.tar.gz</code></li> <li>Check the available options of the 'configure' script: <code>cd /opt/nodered/sources/node-v7.2.0</code> <code>./configure --help</code> If you are using 'root' during the install you don't have to bother with options, simply run <code>./configure</code>. But now I'm using <code>nodered</code> user to compile nodered, so the 'prefix' option have to be used. Most of the configure script has this option, with this you can specify the directory you want to install the software (make install). The default is <code>/usr/local</code>, but nodered user do not have write access to this direcrory, thus we will specify a custom install localtion.</li> <li>Run the configure script: <code>./configure --prefix=/opt/nodered/node-v7.2.0</code></li> <li>Run make: <code>make</code> This will take a long time (~2-3 hours). Please be patient. :)</li> <li>The last step is to run: <code>make install</code></li> </ul> <p>After the <code>make install</code> finished you will have the NodeJS runtime environment here: <pre><code>ls -al /opt/nodered/node-v7.2.0/\ntotal 24\ndrwxr-xr-x 6 nodered nodered 4096 Nov 25 07:12 .\ndrwxr-xr-x 4 nodered nodered 4096 Nov 25 07:13 ..\ndrwxr-xr-x 2 nodered nodered 4096 Nov 25 07:12 bin\ndrwxr-xr-x 3 nodered nodered 4096 Nov 25 07:12 include\ndrwxr-xr-x 3 nodered nodered 4096 Nov 25 07:12 lib\ndrwxr-xr-x 5 nodered nodered 4096 Nov 25 07:12 share\n</code></pre></p>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#3-install-nodered","title":"3. Install NodeRED","text":"<p>Now we are over the hump, there are a few commands left.</p> <ul> <li>Add NodeJS bin directory to PATH: <code>export PATH=$PATH:/opt/nodered/node-v7.2.0/bin/</code></li> <li>Set up npm prefix system env: <code>export NPM_CONFIG_PREFIX=/opt/nodered/node-v7.2.0/lib/node_modules/</code> Unless this environment npm will try to install node-red to /usr/local, but 'nodered' user has no write access to this.</li> <li>Run: <code>npm install -g node-red</code> This command will install Node-Red, and after this process you can start your Node-Red instance by typing <code>/opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red</code>.</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#31-run-node-red-in-the-background","title":"3.1. Run Node-Red In The Background","text":"<p>In order to run node-red in a the background we need one more package called <code>forever</code>.  ^3</p> <p>Do not forget to set up the system environments before install 'forever': <pre><code>export NPM_CONFIG_PREFIX=/opt/nodered/node-v7.2.0/lib/node_modules/ \nexport PATH=$PATH:/opt/nodered/node-v7.2.0/bin/\n</code></pre> After that simply run: <code>npm -g install forever</code> You can get help about forever by typing: <code>/opt/nodered/node-v7.2.0/lib/node_modules/bin/forever --help</code></p> <ul> <li>Start Node-Red in the Background: <code>/opt/nodered/node-v7.2.0/lib/node_modules/bin/forever start /opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red</code> You may want to specify <code>--userDir /opt/node/.node-red</code>.</li> <li>Check running 'forever' processes: <code>/opt/nodered/node-v7.2.0/lib/node_modules/bin/forever list</code></li> <li>Stop 'forever' process: <code>/opt/nodered/node-v7.2.0/lib/node_modules/bin/forever stop /opt/nodered/node-v7.2.0/lib/node_modules/bin/node-red</code> You can use <code>'Id|Uid|Pid|Index|Script'</code> to stop the application. (For example UID can be check with 'list' option.)</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/How_To_Install_Nodered_On_Raspberry_Pi/#32-add-extra-nodes-to-node-red","title":"3.2. Add Extra Nodes To Node-Red","text":"<p>You can browse more than 1000 extra nodes and flows on the http://flows.nodered.org.  For example: Install mysql node: <pre><code>nodered@raspberrypi:~/.node-red$ npm install node-red-node-mysql\n/opt/nodered/.node-red\n\u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u00ac node-red-node-mysql@0.0.11 \n  \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u00ac mysql@2.11.1 \n    \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac bignumber.js@2.3.0 \n    \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u00ac readable-stream@1.1.14 \n    \u00e2\u201d\u201a \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac core-util-is@1.0.2 \n    \u00e2\u201d\u201a \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac inherits@2.0.3 \n    \u00e2\u201d\u201a \u00e2\u201d\u0153\u00e2\u201d\u20ac\u00e2\u201d\u20ac isarray@0.0.1 \n    \u00e2\u201d\u201a \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac string_decoder@0.10.31 \n    \u00e2\u201d\u201d\u00e2\u201d\u20ac\u00e2\u201d\u20ac sqlstring@2.0.1 \n\nnpm WARN enoent ENOENT, open '/opt/nodered/.node-red/package.json'\nnpm WARN .node-red No description\nnpm WARN .node-red No repository field.\nnpm WARN .node-red No README data\nnpm WARN .node-red No license field.\n</code></pre></p> <p>**REFERENCES:**</p> <ul> <li>https://github.com/node-red/node-red</li> <li>https://github.com/nodejs/node</li> <li>http://nodered.org/</li> <li>https://nodejs.org/</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/","title":"Install Debian Jessie to Orange PI Plus 2","text":"<p>Caution</p> <p>This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>Maybe you heard about Orange PI, and maybe you are interested in it. I bought this little board because it's specification, I thought it has to be faster then Raspberry PI2, and it has built-in wifi chip, etc. I love all of my RPI very much and I wanted to try OPI as well, but buying OPI wasn't my best choice of my life.  To say the least Orange PI is a little bit neglected and lack of support compared with Raspberry which has a very great and big community. Of course there are a lot of article on Internet about OPI, but sometimes you can't find what you want and you have to do it on your own.  The whole story began with I wanted to install Jessie to my OPI.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#0-zero-step","title":"0. Zero-Step","text":"<p>The first thing I have to realize that Debian Jessie image cannot be downloaded from Orange PIs web site. If you don't want to spend (waste) time with build Jessie image, please visit OPI home page and choose from the ready-to-use distros. Maybe after a while Jessie will be also available for download. But now we have to build it manually. (Maybe it can be downloaded from another website, I didn't do detailed search for it.) Yes, it can be downloaded from here.</p> <p>I strongly recommend this forum for everyone who wants to deal with OrangePI.  In this post I will follow this guide.</p> <p>Important</p> <p>Most part of this post is by <code>ctrl+c</code> + <code>ctrl+v</code> from various other posts, I wrote this just for collecting things and give more help to others.</p> <p>First I installed a (clean) Debian (8.5) system on a Virtualbox  machine.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#1-installing-dependencies","title":"1. Installing dependencies","text":"<ul> <li>As in case of any other installation we start with updating the OS.</li> </ul> <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre> <ul> <li>Installing the necessary packages</li> </ul> <p><code>apt-get install gcc make debootstrap qemu-user-static git</code></p> <ul> <li>Clone the git repository</li> </ul> <pre><code>cd /usr/src\ngit clone https://github.com/loboris/OrangePi-BuildLinux\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#2-setting-up-paramsh-and-create-the-image","title":"2. Setting up param.sh and create the image","text":"<pre><code>cd /usr/src/OrangePi-BuildLinux\nvi vi params.sh\n</code></pre> <ul> <li>After the setting is done,my param.sh look like this (without commend and blank lines):</li> </ul> <pre><code>root@debian:/usr/src/OrangePi-BuildLinux# cat params.sh | grep -v \\# | grep -v ^$\nONLY_BASE=\"no\"\nHOSTNAME=\"OrangePI\"\nUSER=\"orangepi\"\nROOTPASS=\"orangepi\"\nUSERPASS=\"orangepi\"\n_timezone=\"Etc/UTC\"\nLANGUAGE=\"en\"\nLANG=\"en_US.UTF-8\"\nimage_name=\"jessie\"\n_format=\"ext4\"\nfatsize=64\nlinuxsize=1800\ndistro=\"jessie\"\nrepo=\"http://ftp.hu.debian.org/debian\"\nraspbian=\"no\"\n_compress=\"yes\"\n_boot_on_ext4=\"no\"\n</code></pre> <ul> <li>run create_image</li> </ul> <p>Sample output: https://drive.google.com/file/d/0B4xTxuaiVCZyV3dUVEppTnZQeWs</p> <ul> <li>After the script has been successfully finished we have jessie.img file:</li> </ul> <p><code>-rw-r--r-- 1 root root 1975517184 Sep  5 20:10 jessie.img</code></p>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#3-mount-and-set-up-the-new-image","title":"3. Mount and set up the new image","text":"<p>I want to show you multiple ways to properly set up these files.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#31-with-kpartx","title":"3.1 With kpartx","text":"<ul> <li>To mount the image file use the following commands:</li> </ul> <pre><code>losetup -f\nlosetup /dev/loop0 jessie.img\nkpartx -av /dev/loop0\n</code></pre> <p>For some reason the last command <code>kpartx</code> will fail, but the first partition can be mounted.</p> <pre><code>add map loop0p1 (254:0): 0 131072 linear /dev/loop0 40960\ndevice-mapper: resume ioctl on loop0p2 failed: Invalid argument\ncreate/reload failed on loop0p2\nadd map loop0p2 (0:0): 0 3686401 linear /dev/loop0 172032\n\nroot@debian:/usr/src/OrangePi-BuildLinux# blkid | grep  \"/dev/mapper/\"\n/dev/mapper/loop0p1: LABEL=\"BOOT\" UUID=\"DC73-B07C\" TYPE=\"vfat\" PARTUUID=\"8e7a5a1e-01\"\n\nmount -t vfat /dev/mapper/loop0p1 /mnt/\n</code></pre> <ul> <li>Copy uImage and script.bin according to your OPI version. In my case:</li> </ul> <pre><code>cd /mnt/\ncp uImage_OPI-PLUS uImage\ncp script.bin.OPI-PLUS_1080p60_hdmi uImage\n</code></pre> <ul> <li>Umount the partition and loop device:</li> </ul> <pre><code>sync\numount /mnt\nkpartx -dv /dev/loop0\nlosetup -d /dev/loop0\n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#32-without-kpartx","title":"3.2 Without kpartx","text":"<p>This way is only different from the first one in mounting the image.</p> <ul> <li>First determine  the start and and position of partitions inside the image file.  </li> <li>To do this run <code>parted jessie.img</code></li> <li>Type <code>u b</code>. This will change the unit type to <code>byte</code>.</li> <li>Type <code>print</code> </li> </ul> <p>If you get <code>Error: Can't have a partition outside the disk!</code> error, type Ignore and continue.</p> <p>You will show something like this:</p> <pre><code>Number  Start      End          Size         Type     File system  Flags\n1      20971520B  88080383B    67108864B    primary  fat32\n2      88080384B  1975517695B  1887437312B  primary  ext4\n</code></pre> <ul> <li>Create loop devices</li> </ul> <p><pre><code>losetup -f  jessie.img -o 20971520 --sizelimit 67108864\nlosetup -f  jessie.img -o 88080384 --sizelimit 1887437312\n</code></pre> * Check loop devices</p> <pre><code>root@debian:/usr/src/OrangePi-BuildLinux# blkid /dev/loop*\n/dev/loop0: LABEL=\"BOOT\" UUID=\"DC73-B07C\" TYPE=\"vfat\"\n/dev/loop1: LABEL=\"linux\" UUID=\"bb12e03a-254f-426a-be34-58fd5a9abb94\" TYPE=\"ext4\"\n</code></pre> <ul> <li>Mount them</li> </ul> <p><pre><code>mkdir /mnt/tmp_loop0\nmkdir /mnt/tmp_loop1\nmount /dev/loop0 /mnt/tmp_loop0\nmount /dev/loop1 /mnt/tmp_loop1\n</code></pre> You can check tho mounted partitions:</p> <ul> <li><code>ls -al /mnt/tmp_loop0</code></li> <li><code>ls -al /mnt/tmp_loop1</code></li> </ul> <p>Now you can modify uImage and script.bin.   </p> <ul> <li>After that unmount the image:</li> </ul> <pre><code>umount /mnt/tmp_loop0 /mnt/tmp_loop1\nlosetup -d /dev/loop0 /dev/loop1 \n</code></pre> <p>Now you can write this image to your SD card (or flashdrive), and boot you OPI. You can use Win32DiskImager or <code>dd</code> command to do this.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#33-do-it-on-your-orange-pi","title":"3.3 Do it on your Orange PI","text":"<p>The last method is the easiest. Without any modification on the image file just write it to your SD card, and boot your Orange PI.  In this case the board have to be connected to a keyboard and a monitor, because without properly configured uImage and script.bin it has a chance that ETH port won't work.  After the OPI boot the boot partition should be mounted on <code>/boot</code> or <code>/media/boot</code>, here you can set up the uImage and script.bin. If you are done with it reboot your OPI, and check if ETH port &amp; WIFI adapter working or not.</p> <p>Advantage of using the first and second method is that you shouldn't connect your OPI to display, after flashing the image everything should be working.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Debian_Jessie_To_Orange_Pi_Plus_2/#references-links","title":"REFERENCES &amp; LINKS","text":"<ul> <li>https://github.com/google/cloud-print-connector/wiki/Install</li> <li>https://github.com/google/cloud-print-connector/wiki/Build-from-source</li> <li>http://www.orangepi.cn/orangepibbsen/forum.php?mod=viewthread&amp;tid=342</li> <li>http://vosse.blogspot.hu/2015/10/installing-linux-img-files-on-orange-pi.html</li> <li>http://www.orangepi.org/Docs/Building.html</li> <li>https://linux-sunxi.org/Main_Page</li> <li>https://github.com/allwinner-zh/linux-3.4-sunxi</li> <li>https://github.com/loboris/OrangePI-Kernel</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/","title":"Install Openalpr On Raspberry Pi 3 part 2","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#install-openalpr-on-raspberry-pi-3-part-2","title":"Install OpenALPR on Raspberry PI 3 (Part 2)","text":"<p>I'm writing this post because it was reported that there are some issues with installing OpenALPR and its dependencies.  You can check the comments to my old post about this topic here: Install OpenALPR on Raspberry PI 3</p> <p>After the installation of OpenALPR you can get this error message:</p> <pre><code>Error in fopenReadStream: file not found\nError in pixaRead: stream not opened\nWarning in pixaGetFont: pixa of char bitmaps not found\nInfo in bmfCreate: Generating pixa of bitmap fonts\nError in fopenReadStream: file not found\nError in pixRead: image file not found: /usr/local/src/openalpr/src/build/chars-14.tif\nError in pixaGenerateFont: pixs not all defined\nError in bmfCreate: font pixa not made\n</code></pre> <p>I think the main problem with my first post that some people try to follow the steps without checking the dependencies. I tried to install OpenALPR by following my post, and  I successfully reproduced the issue. Unfortunately I didn't have enough time to try this on my RPI3, so I created a VM with much more RAM and CPU than RPI have to decrease the compiles time.  If you follow my old post you will install the latest packages which is not too good.</p> <p>So as the first step you have to always check the dependencies!</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#1-openalpr-dependencies-link","title":"1. OpenALPR dependencies: Link","text":"<ul> <li>Tesseract OCR v3.0.4 (https://github.com/tesseract-ocr/tesseract)</li> <li>OpenCV v2.4.8+ (http://opencv.org/)</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#2-tesseract-dependencies-link","title":"2. Tesseract dependencies: Link","text":"<p>First Note:</p> <p>OpenALPR does NOT need the newest Tesseract!  Tesseract needs Leptonica, and it can be download from here.</p> <p>BUT! At the moment the newest version is:</p> <p>Latest version: 1.74.1 (1/3/17)</p> <p>So we can notice that:</p> <ul> <li>OpenALRP needs Tesseract 3.04</li> <li>Tesseract needs Leptonica 1.71</li> </ul> <p>Just for testing purpose I installed the latest Leptonica, Tesseract &amp; OpenCV.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#3-opencv-dependencies-link","title":"3. OpenCV dependencies: Link","text":"<p>The Latest OpenCV version:</p> <p></p> <p>Second Note: We DO NOT need this version. </p> <p>If you get this error: <pre><code>root@opanalpr-tst02:/usr/local/src/opencv-3.2.0/release# opencv_version\nlibdc1394 error: Failed to initialize libdc1394\n</code></pre> Please run this:  <code>ln /dev/null /dev/raw1394</code></p> <p>And / or install these packages: <pre><code>apt-get install libdc1394-22-dev\napt-get install libdc1394-22 libdc1394-utils\n</code></pre></p>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#4-conclusion","title":"4. Conclusion","text":"<p>If you install the newest version of all dependencies you will get this error message: <pre><code>Error in fopenReadStream: file not found\nError in pixaRead: stream not opened\nWarning in pixaGetFont: pixa of char bitmaps not found\nInfo in bmfCreate: Generating pixa of bitmap fonts\nError in fopenReadStream: file not found\nError in pixRead: image file not found: /usr/local/src/openalpr/src/build/chars-14.tif\nError in pixaGenerateFont: pixs not all defined\nError in bmfCreate: font pixa not made\n</code></pre></p> <p>Third Note:</p> <p>I suggest to everyone to try installing only the requires version of dependencies, not always the latest one.</p> <p>Maybe there are some ways to use OpenALPR with the latest OpenCV and Tesseract but I failed to find it.</p> <p>Now here is a brief summary on how to install OpenALPR, including only the key steps with some explanation and suggestion.</p>"},{"location":"Old_Blog_Contents/raspberry/Install_Openalpr_On_Raspberry_Pi_3_part_2/#5-install-openalpr","title":"5. Install OpenALPR","text":"<ul> <li> <p>Install the dependencies <pre><code>apt-get install autoconf automake libtool libleptonica-dev libicu-dev libpango1.0-dev libcairo2-dev cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev python-dev python-numpy libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev virtualenvwrapper liblog4cplus-dev libcurl4-openssl-dev\n</code></pre> This part comes from my old post: Install OpenALPR on Raspberry PI 3</p> </li> <li> <p>Install Leptonica</p> </li> </ul> <pre><code>cd /usr/src\nwget http://www.leptonica.org/source/leptonica-1.71.tar.gz\ntar xf leptonica-1.71.tar.gz\n</code></pre> <p>You may need to install these packages: <pre><code>apt-get install libjpeg-dev libtiff5-dev libpng12-dev gcc make\n</code></pre></p> <p>Compile: <pre><code>/usr/src/leptonica-1.71\n./configure\nmake\nmake install\n</code></pre></p> <ul> <li>Install Tesseract</li> </ul> <p>You also may need to install these packages: <pre><code>apt-get install ca-certificates git\napt-get install autoconf automake libtool\napt-get install autoconf-archive\napt-get install pkg-config\n</code></pre></p> <p>If you plan to install the training tools, you also need the following libraries: <pre><code>apt-get install libicu-dev\napt-get install libpango1.0-dev\napt-get install libcairo2-dev\n</code></pre></p> <p>Clone From GIT <pre><code>cd /usr/src\ngit clone https://github.com/tesseract-ocr/tesseract.git\n</code></pre></p> <p>Check available versions (tags) <pre><code>cd /usr/src/tesseract\ngit tag\n</code></pre> Checkout the version which we need: <pre><code>git checkout 3.04.01\n</code></pre></p> <p>Run these commands: <pre><code>cd /usr/src/tesseract\n./autogen.sh\n./configure --enable-debug\nmake\nmake install\n</code></pre></p> <p>You will get the appropriate version: <pre><code>root@openalpr-tst01:/usr/src/tesseract# tesseract -v\ntesseract 3.04.01\n leptonica-1.71\n  libjpeg 6b : libpng 1.2.50 : libtiff 4.0.3 : zlib 1.2.8\n</code></pre></p> <ul> <li>Install OpenCV</li> </ul> <p>Download and extract: <pre><code>cd /usr/src\nwget https://github.com/opencv/opencv/archive/2.4.13.zip\nunzip  2.4.13.zip\n</code></pre></p> <p>Compile: <pre><code>cd opencv-2.4.13\nmkdir release\ncd release\ncmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local ..\nmake\nmake install\n</code></pre></p> <ul> <li>Install OpenALPR</li> </ul> <p>Download <pre><code>cd /usr/src\ngit clone https://github.com/openalpr/openalpr.git\n</code></pre></p> <p>Build: <pre><code>cd openalpr/src\nmkdir build\ncd build\ncmake -DCMAKE_INSTALL_PREFIX:PATH=/usr -DCMAKE_INSTALL_SYSCONFDIR:PATH=/etc ..\nmake\nmake install\n</code></pre></p> <p>If you experience some errors please try to install these packages: <pre><code>apt-get install cmake\napt-get install liblog4cplus-dev libcurl3-dev\nsudo apt-get install beanstalkd\napt-get install openjdk-7-jdk\nexport JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64/\n</code></pre></p> <p>Test: <pre><code>wget http://plates.openalpr.com/h786poj.jpg -O lp.jpg\nalpr lp.jpg\n</code></pre> The result must be something like this (Without any errors): <pre><code>plate0: 8 results\n    - 786P0      confidence: 90.1703\n    - 786PO      confidence: 85.579\n    - 786PQ      confidence: 85.3442\n    - 786PD      confidence: 84.4616\n    - 7B6P0      confidence: 69.4531\n    - 7B6PO      confidence: 64.8618\n    - 7B6PQ      confidence: 64.627\n    - 7B6PD      confidence: 63.7444\n</code></pre></p> <p>If you get any type of missing library error at any steps, run <code>ldconfig</code> command. </p> <p>I hope this post will be useful for you, and you will be able to install OpenALPR. If you have any further question or note you can leave a Disqus comment below.</p>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/","title":"Mount SD card image (partitioned image) w/o kpartx","text":"<p>Caution</p> <p>This page hasn't recently updated. Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>If you are working with SD card image I pretty sure that you were in the situation when you had to mount the image before write it to the SD card.  Particular example when you want to modify <code>cmdline.txt</code> in a Raspberry PI image (because you want to use different partition for booting).</p>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#with-kpartx-utility","title":"With kpartx utility","text":"<p>If your system doesn't have <code>kpartx</code> utility install it for example with <code>apt-get install kpartx</code> command in case of Debian-Like systems.</p> <p>For demonstration I use <code>2016-05-27-raspbian-jessie-lite.img</code> image. </p> <ol> <li> <p>set up loop devices <code>losetup -f 2016-05-27-raspbian-jessie-lite.img</code></p> </li> <li> <p>(optional) Check the loop device <pre><code>losetup\nNAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE\n/dev/loop0         0      0         0  0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img\n</code></pre></p> </li> <li> <p>Create device maps from partition tables <pre><code>kpartx -av /dev/loop0\nadd map loop0p1 (254:0): 0 129024 linear /dev/loop0 8192\nadd map loop0p2 (254:1): 0 2572288 linear /dev/loop0 137216\n</code></pre></p> </li> <li> <p>(optional) Check mapped partitions with <code>blkid</code> <pre><code>Simpliy type `blkid` and hit enter. Output:  \n...\n/dev/mapper/loop0p1: SEC_TYPE=\"msdos\" LABEL=\"boot\" UUID=\"22E0-C711\" TYPE=\"vfat\" PARTUUID=\"6fcf21f3-01\"\n/dev/mapper/loop0p2: UUID=\"202638e1-4ce4-45df-9a00-ad725c2537bb\" TYPE=\"ext4\" PARTUUID=\"6fcf21f3-02\"\n...\n</code></pre></p> </li> <li> <p>Mount these partitions <pre><code>mkdir /mnt/tmp1  \nmkdir /mnt/tmp2\nmount -t vfat /dev/mapper/loop0p1 /mnt/tmp1  \nmount -t ext4 /dev/mapper/loop0p2 /mnt/tmp2\n</code></pre> After you are done with the neccessary modifications you can dismount everything.</p> </li> <li> <p>DisMount <pre><code>umount /mnt/tmp1\numount /mnt/tmp2\nkpartx -dv /dev/loop0\nlosetup -d /dev/loop0\n</code></pre> You can check it everything is unmounted by running <code>losetup</code> command, it should return with \"nothing\".</p> </li> </ol>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#without-kpartx-utility","title":"Without kpartx utility","text":"<p>It is possible to mount partitions inside an image without kpart utility as well, but I think this way a little bit more complicated.</p> <p>Follow these steps:</p>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#determine-the-size-partitions-where-is-it-started-and-ended","title":"Determine the size partitions (where is it started and ended)","text":"<p>1.1. Run <code>parted 2016-05-27-raspbian-jessie-lite.img</code>   1.2. Type <code>u b</code> and enter. --&gt; This will change the display unit to bytes.   1.3. Type <code>print</code> to display partition layout. Sample output:  </p> <pre><code>parted 2016-05-27-raspbian-jessie-lite.img \nGNU Parted 3.2\nUsing /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img\nWelcome to GNU Parted! Type 'help' to view a list of commands.\n(parted) u b                                                              \n(parted) print                                                            \nModel:  (file)\nDisk /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img: 1387266048B\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start      End          Size         Type     File system  Flags\n 1      4194304B   70254591B    66060288B    primary  fat16        lba\n 2      70254592B  1387266047B  1317011456B  primary  ext4\n\n(parted) q                                                                \n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#set-up-loopbak-devices","title":"Set up loopbak devices","text":"<p>As you can see there are two partition inside the image file. We need to create two loopback device for them:</p> <pre><code>losetup -f -o 4194304 --sizelimit 66060288 2016-05-27-raspbian-jessie-lite.img \nlosetup -f -o 70254592 --sizelimit 1317011456 2016-05-27-raspbian-jessie-lite.img \n</code></pre> <p>Warning</p> <p>Please double check the offset and sizelimit parameters!</p> <p>Check with <code>losetup</code> and <code>blkid</code> commands: losetup: <pre><code>NAME        SIZELIMIT   OFFSET AUTOCLEAR RO BACK-FILE\n/dev/loop0   66060288  4194304         0  0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img\n/dev/loop1 1317011456 70254592         0  0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img\n</code></pre> blkid: <pre><code>/dev/loop0: SEC_TYPE=\"msdos\" LABEL=\"boot\" UUID=\"22E0-C711\" TYPE=\"vfat\"\n/dev/loop1: UUID=\"202638e1-4ce4-45df-9a00-ad725c2537bb\" TYPE=\"ext4\"\n</code></pre></p>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#mount-the-partitions","title":"Mount the partitions","text":"<pre><code>mount /dev/loop0 /mnt/tmp1\nmount /dev/loop1 /mnt/tmp2\n</code></pre> <p>Check: <pre><code>df -hT /dev/loop*\nFilesystem     Type      Size  Used Avail Use% Mounted on\n/dev/loop0     vfat       63M   21M   43M  33% /mnt/tmp1\n/dev/loop1     ext4      1.2G  738M  389M  66% /mnt/tmp2\nudev           devtmpfs   10M     0   10M   0% /dev\nudev           devtmpfs   10M     0   10M   0% /dev\nudev           devtmpfs   10M     0   10M   0% /dev\nudev           devtmpfs   10M     0   10M   0% /dev\nudev           devtmpfs   10M     0   10M   0% /dev\nudev           devtmpfs   10M     0   10M   0% /dev\nudev           devtmpfs   10M     0   10M   0% /dev\n</code></pre></p> <p>After you are done with you work unmount everything.</p>"},{"location":"Old_Blog_Contents/raspberry/Mount_Sd_Card_Image_partitioned_Image_Wo_Kpartx/#unmount-the-partitions","title":"Unmount the partitions","text":"<pre><code>root@debian:~# umount /mnt/tmp1 /mnt/tmp2/\nroot@debian:~# losetup \nNAME        SIZELIMIT   OFFSET AUTOCLEAR RO BACK-FILE\n/dev/loop0   66060288  4194304         0  0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img\n/dev/loop1 1317011456 70254592         0  0 /home/vinyo/Downloads/2016-05-27-raspbian-jessie-lite.img\nroot@debian:~# losetup -d /dev/loop0 /dev/loop1\nroot@debian:~# losetup \nroot@debian:~# \n</code></pre>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/","title":"Move root file system to USB storage (RPI2 &amp; RPI3)","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>UPDATE: 2018-11-06</p> <p>Maybe this post has lost its purpose because nowadays all modern distros support moving root file system to external device, for examples:</p> <ul> <li>Raspberry PIs: With help of noobs you can install the whole OS to external device.</li> <li>Orange &amp; Banana PIs: Armbian has the feature of moving file system to USB (<code>armbian-config</code>).</li> </ul> <p>I don't think this article can be used as a complete guide, the only reason I let this post (a)live is the commands and links in it.</p> <p>There are a lot of very good and useful guide on the Internet which can help you to move your root file system to an external USB storage. Using external storage is highly recommended if you want to use your RPI as a server machine which runs 7/24. I had a lot of problems with SD cards, I think they are absolutely unsuitable for use in a raspberry and store the root file system on them. I pretty sure all SD card will die in some months, and you will lost all of your data. This happens to me and I decided not to use SD cards anymore.</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#you-can-find-some-guides-here","title":"You can find some guides here:","text":"<ul> <li>http://www.kupply.com/move-your-raspberry-pi-system-to-usb-in-10-steps</li> <li>https://www.raspberrypi.org/blog/pi-3-booting-part-i-usb-mass-storage-boot (PI3 only)</li> <li>https://liewdaryl.wordpress.com/2015/06/06/setting-up-raspberry-pi-2-including-moving-rootfs-to-usb-drive</li> <li>http://elinux.org/Transfer_system_disk_from_SD_card_to_hard_disk</li> </ul> <p>So why am I writing this post? Just because I spent a lot of time to do this on my own, and I want to write down my experiences.  These tips can be useful when you deal with SD card, SD card images and hard drives.</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#1-create-image-from-sd-card-with-dd","title":"1. Create Image From SD card with <code>dd</code>","text":"<p>You can create copy of your entire SD card. For example for backup.</p> <p><code>dd if=/dev/sdc of=RPI_sdcard.img bs=4096</code></p> <p>If something fails during working with the image try decrease \"bs\" to 1024.</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#2-extract-partitions-from-the-image-file","title":"2. Extract Partitions From The Image File","text":""},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#21-determine-the-size-of-partitions","title":"2.1. Determine the size of partitions","text":"<p>I will use the official raspberry image. Run this command: <code>parted 2016-09-23-raspbian-jessie.img</code> Inside the parted type:  <pre><code>(parted) u b\n(parted) print\nNumber  Start      End          Size         Type     File system  Flags\n 1      4194304B   70254591B    66060288B    primary  fat16        lba\n 2      70254592B  4348444671B  4278190080B  primary  ext4\n</code></pre> First one is the boot partition, the second one is the root partition.</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#21-extract-the-first-partition-from-the-image","title":"2.1. Extract the first partition from the image","text":"<p>Command: <code>dd if=2016-09-23-raspbian-jessie.img iflag=skip_bytes,count_bytes,fullblock bs=4096 count=70254591 of=boot_fs.img</code></p> <p>This will create the <code>boot_fs.img</code>, inside it the second partition is absolutely unnecessary, so we can delete it. </p> <p>To do this use the fdisk utility.  <code>Command: fdisk boot_fs.img</code> Inside the fdisk type these commands: <pre><code>Command (m for help): d\nPartition number (1,2, default 2): 2\n\nPartition 2 has been deleted.\n\nCommand (m for help): w\nThe partition table has been altered.\nSyncing disks.\n</code></pre></p> <p>The created image is about 70MB, and it is more than enough to boot your PI, but be aware that you have to modify the <code>cmdline.txt</code> to use an external hard disk for the root partition. </p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#21-extract-the-second-partition-from-the-image","title":"2.1. Extract the second partition from the image","text":"<p>Command: <code>dd if=2016-09-23-raspbian-jessie.img iflag=skip_bytes,count_bytes,fullblock bs=4096 skip=70254592 count=4278190080 of=root_fs.img</code></p> <p>The meaning of the options:</p> <ul> <li><code>if=2016-09-23-raspbian-jessie.img</code> --&gt; Input file</li> <li><code>iflag</code> --&gt; \"read as per the comma separated symbol list\"</li> <li><code>skip_bytes</code> --&gt; \"treat 'skip=N' as a byte count (iflag only)\"</li> <li><code>count_bytes</code> --&gt; \"treat 'count=N' as a byte count (iflag only)\"</li> <li><code>fullblock</code> --&gt; \"accumulate full blocks of input (iflag only)\"</li> <li><code>skip=70254592</code> --&gt; Skip the first 70254592 bytes from the image. The start position of the second partition inside the image file. </li> <li><code>count=4278190080</code> --&gt; The size of the second partition.</li> </ul> <p>That's all. At the end of this step we have two files: <pre><code>-rw-r--r-- 1 root root 4278190080 nov 19 16:39 root_fs.img\n-rw-r--r-- 1 root root 70254591 nov 19 16:36 boot_fs.img \n</code></pre></p> <p>We can check all of them by using fdisk/parted/gdisk/gparted utility.</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#3-prepare-hdds","title":"3. Prepare HDDs","text":""},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#31-remove-all-partition-entries-from-the-disk","title":"3.1. Remove all partition entries from the disk","text":"<p>Example: <code>wipefs --all /dev/sdb</code> IMPORTANT: This will destroy all data on your entire disk!</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#32-create-partition","title":"3.2. Create Partition","text":"<p>If you want to copy an image (for example the root_fs.img) to your Hard Drive, first create a partition on it.</p> <p>Run parted: <code>parted /dev/sdb</code> </p> <p>You will see an error message: <code>Error: /dev/sdb: unrecognised disk label</code> </p> <p>So we have to create the disk label first: Command (inside parted):  <pre><code>(parted) mktable msdos\nCreate a partition for the image. Its size have to be at least 4278190080 Byte, but we will create a 20GB partition.\n(parted) mkpart                                                           \nPartition type?  primary/extended? primary                                \nFile system type?  [ext2]? ext3\nStart? 0%\n(optional) Check the partition\n(parted) p                                                                \nModel: WDC WD25 00JB-98GVA0 (scsi)\nDisk /dev/sdb: 250GB\nSector size (logical/physical): 512B/512B\nPartition Table: msdos\nDisk Flags: \n\nNumber  Start   End     Size    Type     File system  Flags\n 1      1049kB  20,0GB  20,0GB  primary  ext3         lba\n</code></pre></p> <p>After that you can create as many partition as you want. To create partitions you can use <code>gparted</code> graphical utility, too, or any other partition utility.</p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#32-copy-image-to-the-partition","title":"3.2. Copy image to the partition","text":"<p>Command: <code>dd if=root_fs.img of=/dev/sdb1 bs=4096</code></p> <p>If you created larger partition than the image, you have to resize to partition. (Of course if you created smaller partition than the image, dd will fail with \"no space left on device\" error message.)  Before running resize2fs you have to check the partition: <code>e2fsck -f /dev/sdb1</code> Resize: <code>resize2fs /dev/sdb1</code></p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#3-configure-raspberry-to-use-root-fs-on-the-external-hard-drive","title":"3. Configure Raspberry to use root fs on the external hard drive","text":""},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#31-cmdlinetxt","title":"3.1. <code>cmdline.txt</code>","text":"<p>This file can be found on the boot partition, on your SD card.</p> <p>Modify from: <code>root=/dev/mmcblk0p2</code> To: <code>root=/dev/sda1</code></p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#32-etcfstab","title":"3.2. <code>etc/fstab</code>","text":"<p>This file must be located on your external hard disk root partition. If your SD card still have the root partition lest modify fstab on it, because it won't take effect.</p> <p>Modify from: <code>/dev/mmcblk0p2 / ext4 defaults,noatime 0 1</code> To: <code>/dev/sdaq / ext4 defaults,noatime 0 2</code></p>"},{"location":"Old_Blog_Contents/raspberry/Move_Root_File_System_To_Usb_Storage_%28rpi2_%26_Rpi3%29/#4-final-thoughts","title":"4. Final Thoughts","text":"<p>I think running OS from an external HDD (or SSD) is highly recommended. As this post doesn't give you a step-by-step guide, I share with you the method I usually move root fs to HDD:</p> <ol> <li>Download the image from https://www.raspberrypi.org/downloads/</li> <li>Write the image to an SD card. (Linux: dd)</li> <li>Start your Raspberry PI and do the initial steps. The most important the FS resize.</li> <li>Shutdown the Raspberry</li> <li>Copy the root partition to the HDD. (Linux: dd)</li> <li>Edit cmdline.txt and fstab. (Please aware that the cmdline.txt remains on the SD card, but the fstab must be edited on the HDD.)</li> <li>Take back the SD card to the Raspberry &amp; connect the external HDD.</li> <li>Boot &amp; Enjoy</li> </ol> <p>References:</p> <ul> <li>http://unix.stackexchange.com/questions/38164/create-partition-aligned-using-parted</li> <li>https://www.pantz.org/software/parted/parted_and_disk_alignment.html</li> <li>http://askubuntu.com/questions/201164/proper-alignment-of-partitions-on-an-advanced-format-hdd-using-parted </li> <li>http://rainbow.chard.org/2013/01/30/how-to-align-partitions-for-best-performance-using-parted/</li> <li>http://gparted.org/h2-fix-msdos-pt.php</li> <li>https://www.raspberrypi.org/documentation/hardware/raspberrypi/bootmodes/msd.md</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/","title":"Raspberry Pi 2 As Print Server Airprint","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p> <p>I have an old but working Samsung ML1510 printer, and everytime I want to print something I have to turn on my old desktop PC, copy my doc(s) to it and  start printing from it. So I have decided to make my old printer able to work over wireless. </p> <p>My goals:  </p> <ul> <li>To print from my Home network over Wifi connection. I have an iPhone and it supports AirPrint.  </li> <li>I have a Google account, and Google has \"Google Cloud Print\" service. It supports local printing and printing over the internet from anywhere. (I don't understand why people want to print from anywhere to home, but O.K. let's do it.)</li> </ul> <p>Of course I can use a Raspberry PI 2 for this project, but I recommend to use RPI3, because it has built-in WIFI. In my case RPI is connected with ETH cable to my existing network (, or you can use Wi-fi stick)</p> <p>Raspbian version: <code>Linux raspberrypi 4.4.13-v7+ #894 SMP Mon Jun 13 13:13:27 BST 2016 armv7l GNU/Linux</code></p> <p></p> <p>Download Link: latest</p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#1-samsung-driver","title":"1. Samsung Driver","text":"<p>If it is needed please run update &amp; upgrade before <code>apt-get install</code>: <pre><code>sudo apt-get update\nsudo apt-get upgrade\n</code></pre></p> <p>The next step is to find a driver for your printer. I have a Samsung ML_1510 printer, after some googling I found an article which said that \u00e2\u20ac\u0153splix\u00e2\u20ac package contains the driver for my printer <code>(printer-driver-splix)</code>. Unfortunately the official Samsung linux driver does not support ARM architecture.</p> <p>root@raspberrypi:~# <code>apt-cache search samsung</code> <pre><code>bitpim - utility to communicate with many CDMA phones\nbitpim-lib - architecture-dependent helper files for BitPim\nheimdall-flash - tool for flashing firmware on Samsung Galaxy S devices\nheimdall-flash-frontend - tool for flashing firmware on Samsung Galaxy S devices - Qt GUI\nlibimage-exiftool-perl - library and program to read and write meta information in multimedia files\nmadwimax - user-space driver for mWiMAX equipment based on Samsung CMC-730\nprinter-driver-splix - Driver for Samsung and Xerox SPL2 and SPLc laser printers\nskyeye - Embedded Hardware Simulation\nfirmware-samsung - Binary firmware for Samsung MFC video codecs\n</code></pre> Command: <code>sudo apt-get install printer-driver-splix</code></p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#2-installing-and-configuring-cups","title":"2. Installing and configuring CUPS","text":"<p>Configuring CUPS is very easy. It listens on TCP/631, and you can use your favorite  browser to access the configuration page. Because Raspberry has limited resources (mem, cpu), I use ssh tunnel to configure CUPS. (By default cups listen only on the loopback interface for security reason. Of course you can configure cups to listen on its private IP address, but I love tunnelling everything. \u2122\u201a If you prefer this way you should change <code>Listen localhost:631</code> to <code>Port 631</code> in <code>/etc/cups/cupsd.conf</code>.) So I created a local tunnel from another linux box in my network: <pre><code>ssh 172.16.0.210 -L 1631:localhost:631\n</code></pre></p> <p>Note</p> <p>I used port 1631 because non-root user can not bind port under 1025. </p> <ul> <li>Install CUPS: <pre><code>apt-get install cups cups-client cups-common cups-pdf\n</code></pre></li> <li>Before you start administrating you should add \u00e2\u20ac\u0153pi\u00e2\u20ac (or another) user to lpadmin group (or you can use root user): <pre><code>usermod -a -G lpadmin pi\n</code></pre></li> <li>OK. Open your browser and head to http://localhost:1631 (or http://localhost:631 without tunneling or http://[IP]:631 if you preconfigured cups to listen on all interface). </li> <li>Click on \u00e2\u20ac\u0153Administration\u00e2\u20ac. You will be prompted for a username and password. If you previously added your user to <code>lpadmin</code> group you can use it, otherwise use \u00e2\u20ac\u0153root\u00e2\u20ac. Click \u00e2\u20ac\u0153Add Printer Button\u00e2\u20ac: </li> <li>Select your printer from the list. (In my case: Samsung ML-1510_700 (Samsung ML-1510_700)) and click continue. </li> <li>Give a name to your printer, check \u00e2\u20ac\u0153Share This Printer\u00e2\u20ac and click continue.  </li> <li>Select your printer (in my case Samsung ML_1510) and click \u00e2\u20ac\u0153Add Printer\u00e2\u20ac If your printer is not listed its driver is not properly installed. You have to do some research for linux driver. </li> <li>Set the default values for this printer. (I did not created a screenshot because this step is different in case of each printer.</li> <li>On the main page click \u00e2\u20ac\u0153Printers\u00e2\u20ac and check your newly added printer status.  </li> <li>(Optional) You can print a test page. Click on the printer\u00e2\u20ac\u2122s name: Now you can see the printer's configuration page. Click on \u00e2\u20ac\u0153Maintenance\u00e2\u20ac drop-down list and choose \u00e2\u20ac\u0153Print Test Page\u00e2\u20ac:  At the bottom of the page you can see the status of the printing but only for a few seconds. Next we check Jobs: Click on \u00e2\u20ac\u0153Job\u00e2\u20ac on the horizontal main menu, and click \u00e2\u20ac\u0153Show Competed Jobs\u00e2\u20ac:  </li> <li>(Optional) On the \u00e2\u20ac\u0153Administration\u00e2\u20ac page check: \u00e2\u20ac\u201c Share printers connected to this system \u00e2\u20ac\u201c Allow remote administration \u00e2\u20ac\u201c Allow printing from the Internet </li> </ul> <p>This is a very basic setup of cups, but this is just enough at the beginning. If you want to know more about CUPS visit its official website or do some google (re)search. </p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#2-make-you-printer-available-for-air-print","title":"2. Make you printer available for Air print","text":"<p>This step is incredibly easy, just one command: <code>sudo apt-get install avahi-discover avahi-daemon</code></p> <p>I found some articles about \u00e2\u20ac\u0153cups airprint setup\u00e2\u20ac which are much complected, but somehow in my case it works with installing only the  avahi damon and discover. Links:</p> <ul> <li>http://iain.polevaultweb.com/2014/03/setting-raspberry-pi-print-server-airprint-support/</li> <li>http://www.lynsayshepherd.com/blog/2015/10/18/wireless-printingairprint-server-via-the-raspberry-pi-updated-guide/</li> <li>https://wiki.debian.org/AirPrint</li> </ul> <p>NOTE: Of course you have to connect your RasPI to the same network which used by your Wifi, and connect your phone to it in order to make everything work fine.</p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#3-add-your-printer-to-windows-10","title":"3. Add your printer to Windows 10","text":"<p> So I think if you configured a network printer you would like to use it from another system(s). If you don\u00e2\u20ac\u2122t share your printer via SAMBA you can use your printers URL to connect to it from Win10. You can check this URL by accessing the admin page of cups. Click on \u00e2\u20ac\u0153printers\u00e2\u20ac and select the printer you want to connect to:  Copy your Printer URL to clipboard from your browser Address Line! For example: <code>https://172.16.0.210:631/printers/rpi_samsung_ml-1510</code></p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_2_As_Print_Server_Airprint/#4-windows-setup","title":"4. Windows Setup:","text":"<ul> <li>On Windows system click \u00e2\u20ac\u0153Device and Printers\u00e2\u20ac in the control panel. </li> <li>Click Add printer</li> <li>Click The printer that I want isnt listed</li> <li>Click Select a shared printer by name\u00e2\u20ac and paste your printer URL: </li> <li>Click Next. If Windows can not connect to you printer, try without SSL (http).</li> <li>Select your printer\u00e2\u20ac\u2122s driver and click finish.</li> </ul>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/","title":"Raspberry Pi 3 As Wifi Range Extender","text":"<p>Caution</p> <p>This page has been updated a long time ago.  Information found here could be outdated and may lead to missconfiguration. Some of the links and references may be broken or lead to non existing pages. Please use this docs carefully. Most of the information here now is only for reference or example!</p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#raspberry-pi-3-as-wifi-range-extender","title":"Raspberry PI 3 As Wifi Range Extender","text":""},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#tldr","title":"TL;DR","text":"<p>I have a Workshop in our backyard, and there are some ESPs inside. Unfortunately my router is far away from them and the Wifi connection often breaks. This is a real problem for me because the ESPs are controlling my lighting in the garden and it's a bit irritating when I cannot turn on the lights. (The ESPs automatically turn off all relay channels when the Wifi disconnects, so turning off is not an issue.) As I have wired LAN access (almost) everywhere in my house and even in my Workshop I was thinking about how to extend my Wifi range. OK. I know. There are a lot of possibilities to do it, but I wanted to choose the best and the most reliable way, and I wanted to use something I have already have. I use a Mikrotik router with a lot of switches all around the house. Since I don't like Wifi networks I'm trying to connect as much devices as I can wired to my network, but the ESPs needs Wifi connection. There is a  Raspberry PI3 already running in the workshop so it is reasonable to use that. Of course the PI has Ethernet connection to my Mikrotik router. :)</p> <p>Update: I haven't published this post yet, and I've already had an update.  To make it work was much easier than I thought. --&gt; To make it work was much complicated then I've ever thought and took more days. :(</p> <p>As you will see I faced a lot of problems. But I want to believe that this post will be helpful. </p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#install-the-necessary-packages","title":"Install the necessary packages","text":"<pre><code>sudo apt-get install hostapd bridge-utils wicd wicd-cli wpasupplicant\n</code></pre> <p>During the installation the wicd-daemon will ask you for the list of users who can use the wicd client. </p> <p>Users who should be able to run wicd clients need to be added to the group \"netdev\".</p> <p>You can modify these settings later by running <code>dpkg-reconfigure wicd-daemon</code> command.</p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#configuring-bridge-interface","title":"Configuring Bridge Interface","text":"<p>My setup looks like that: <pre><code>auto lo\niface lo inet loopback\niface eth0 inet manual\niface wlan0 inet manual\nauto br0\niface br0 inet dhcp\n  bridge_ports eth0 \n  bridge_stp off\n  bridge_fd 0\n  bridge_maxwait 0\n  bridge_waitport 0\n</code></pre></p> <p>Now you should restart your PI, after that you can check your config:</p> <p>Command<pre><code>brctl show\n</code></pre> Output<pre><code>bridge name bridge id       STP enabled interfaces\nbr0     8000.b827eb26993d   no      eth0\n</code></pre></p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#check-wifi-device-configure-hostapd","title":"Check Wifi Device &amp; Configure Hostapd","text":"<p>First it is recommended to check if your Wireless device supports AP mode or not. It is only necessary for example if you are using an RPI which is older than PI3. (RPI3 has built-n WiFi chip, which supports AP mode.)</p> <ul> <li>Check your interface list Command<pre><code>iw dev  \n</code></pre> Output<pre><code>phy#0\n    Interface wlan0\n        ifindex 3\n        wdev 0x1\n        addr b8:27:eb:73:cc:68\n        type managed\n</code></pre></li> </ul> <p>You can see I have only one interface: <code>phy#0</code>. Here is an example when there are multiple interfaces:</p> <p>Command<pre><code>iw dev  \n</code></pre> Output<pre><code>phy#1\n    Interface wlan1\n        ifindex 4\n        wdev 0x100000001\n        addr 00:e0:32:00:00:8b\n        type managed\n        channel 8 (2447 MHz), width: 20 MHz, center1: 2447 MHz\nphy#0\n    Interface wlan0\n        ifindex 3\n        wdev 0x1\n        addr b8:27:eb:f9:dd:be\n        ssid Vinyo-Net\n        type AP\n        channel 2 (2417 MHz), width: 20 MHz, center1: 2417 MHz\n</code></pre></p> <ul> <li>Check \"Supported interface modes\" Command<pre><code>iw phy phy0 info\n</code></pre> Output<pre><code>Wiphy phy0\n    max # scan SSIDs: 10\n    max scan IEs length: 2048 bytes\n    Retry short limit: 7\n    Retry long limit: 4\n    Coverage class: 0 (up to 0m)\n    Device supports T-DLS.\n    Supported Ciphers:\n        * WEP40 (00-0f-ac:1)\n        * WEP104 (00-0f-ac:5)\n        * TKIP (00-0f-ac:2)\n        * CCMP (00-0f-ac:4)\n    Available Antennas: TX 0 RX 0\n    Supported interface modes:\n         * IBSS\n         * managed\n         * AP\n         * P2P-client\n         * P2P-GO\n         * P2P-device\n    Band 1:\n        Capabilities: 0x1020\n            HT20\n            Static SM Power Save\n            RX HT20 SGI\n            No RX STBC\n            Max AMSDU length: 3839 bytes\n            DSSS/CCK HT40\n...\n...\n</code></pre></li> </ul> <p>As you can see, our built-in Wifi device supports the AP mode. (By running this command (<code>iw phy phy0 info</code>) you get a lot of information about your Wifi device.)</p> <ul> <li>Configure Hostapd</li> </ul> <p>I don't know why, but the /etc/hostapd directory does not contain default/sample configuration file, but you can find sample configuration files here: <code>/usr/share/doc/hostapd/examples</code> I used this command to get examples:</p> <pre><code>gunzip -c /usr/share/doc/hostapd/examples/hostapd.conf.gz | less\n</code></pre> <p>I advise you to use a different SSID firstly then you already have to test a vanilla setup.</p> <p>Here is working example file:</p> <pre><code>ctrl_interface=/var/run/hostapd\nmacaddr_acl=0 \ndriver=nl80211\ninterface=wlan0\nbridge=br0\ncountry_code=HU\nhw_mode=g\nieee80211n=1\nchannel=2\nssid=WS-TST-01\nauth_algs=1\nignore_broadcast_ssid=0\nwpa=2\nwpa_passphrase=12345678\nwpa_key_mgmt=WPA-PSK\nwpa_pairwise=CCMP\nrsn_pairwise=CCMP\n</code></pre> <p>Warning</p> <p>Use a channel which is different from the one your primary device uses.</p> <p></p> <p>After the configuration file is created you can try to start hostapd.</p> <pre><code>hostapd /etc/hostapd/hostapd.conf \nConfiguration file: /etc/hostapd/hostapd.conf\nFailed to create interface mon.wlan0: -95 (Operation not supported)\nwlan0: interface state UNINITIALIZED-&gt;COUNTRY_UPDATE\nwlan0: Could not connect to kernel driver\nUsing interface wlan0 with hwaddr b8:27:eb:73:cc:68 and ssid \"WS-TST-01\"\nwlan0: interface state COUNTRY_UPDATE-&gt;ENABLED\nwlan0: AP-ENABLED \n</code></pre> <p>We get two error messages:</p> <ul> <li><code>Failed to create interface mon.wlan0: -95 (Operation not supported)</code></li> <li><code>wlan0: Could not connect to kernel driver</code></li> </ul> <p>I did a lot of Google searches, but failed to find any solution for this issue. Despite the failures the \"AP-ENABLED\" messages shows us that everything should work. Now you can try to connect your new Wifi AP: <code>WS-TST-01</code></p> <ul> <li>Enable Auto Start hostapd daemon</li> </ul> <p>By default the hostapd doesn't start at boot time. To enable it change this file:</p> <pre><code>--- hostapd_orig    2017-08-15 11:14:58.673575799 +0200\n+++ hostapd    2017-08-14 21:25:47.684546287 +0200\n@@ -7,7 +7,7 @@\n # file and hostapd will be started during system boot. An example configuration\n # file can be found at /usr/share/doc/hostapd/examples/hostapd.conf.gz\n #\n-DAEMON_CONF=\"\"\n+DAEMON_CONF=\"/etc/hostapd/hostapd.conf\"\n\n # Additional daemon options to be appended to hostapd command:-\n #     -d   show more debug messages (-dd for even more)\n</code></pre> <p>At this point I was facing a very strange and serious issue. :( It took days to find a working solution or only a workaround instead. </p> <p>The problem: After restarting my PI everything seemed fine, but none of devices could see the SSID. But it could be solved with restarting the hostapd daemon. At the bottom of this post in the references section you can find some article which discuss similar issues, but without any \"real\" solution (only workarounds). (Example: control the hostapd daemon from interface config, ifup.d). </p> <p>Now I sharing you my experiences I got during the investigation, It may (or may not) be useful. You have to know I made uncountable tries to configure hostapd, dhcpcd, dhcp-client, bridge, etc, but none of them led to success. </p> <p>1. Replace the original System V init service to systemd.</p> <p>By default the hostapd daemon is started by an init.d scripts  (/etc/init.d/hostapd). I wanted to move the service start to the end of the boot process to make sure that every necessary service started before hostapd. So I wrote a custom systemd config file and removed it from /etc/rc3.d.</p> <p>My SystemD script:</p> <pre><code>[Unit]\nDescription=HOSTAPD\nRequires=multi-user.target network-online.target avahi-daemon.service smbd.service\nAfter=avahi-daemon.service smbd.service multi-user.target\n\n[Service]\nType=forking\nGuessMainPID=yes  \nExecStart=/usr/sbin/hostapd -d -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf\nExecStop=/usr/bin/kill -SIGINT $MAINPID  \nPIDFile=/run/hostapd.pid\nRestart=always\nUser=root\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>You can check the boot order by issuing the command (after reboot):</p> <pre><code>systemd-analyze plot &gt;/tmp/plot3.svg\n</code></pre> <p>This .svg file can be opened with any type of browser. I saw that the hostapd service was started almost at the end of the boot process (I think it was the penultimate one before systemd-update-utmp-runlevel.service and after the multi-user.target).</p> <p>But It did not solved my problem.</p> <p>2. Restart hostapd daemon after the IP address bounded (br0)</p> <p>I was reading the log files a lot, and found that something happens after the boot process has been finished with the interfaces (br0,wlan0,eth0):</p> <p>In the log files you can see that the boot process has been finished at 11:09:11. <code>Aug 26 11:09:11 ws-rpi3 systemd[1]: Startup finished in 3.918s (kernel) + 29.036s (userspace) = 32.954s.</code></p> <p>But after some seconds: <pre><code>Aug 26 11:09:14 ws-rpi3 kernel: [   36.241794] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 26 11:09:14 ws-rpi3 kernel: [   36.242140] br0: port 1(eth0) entered disabled state\nAug 26 11:09:14 ws-rpi3 kernel: [   36.342138] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 26 11:09:14 ws-rpi3 kernel: [   36.342435] br0: port 1(eth0) entered blocking state\nAug 26 11:09:14 ws-rpi3 kernel: [   36.342446] br0: port 1(eth0) entered forwarding state\nAug 26 11:09:14 ws-rpi3 kernel: [   36.474142] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 26 11:09:14 ws-rpi3 kernel: [   36.474380] br0: port 1(eth0) entered disabled state\nAug 26 11:09:14 ws-rpi3 dhcpcd[1046]: dhcpcd not running\nAug 26 11:09:14 ws-rpi3 kernel: [   36.591789] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 26 11:09:14 ws-rpi3 kernel: [   36.592070] br0: port 1(eth0) entered blocking state\nAug 26 11:09:14 ws-rpi3 kernel: [   36.592074] br0: port 1(eth0) entered forwarding state\nAug 26 11:09:16 ws-rpi3 kernel: [   38.073773] smsc95xx 1-1.1:1.0 eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1\nAug 26 11:09:16 ws-rpi3 dhcpcd[1051]: version 6.7.1 starting\nAug 26 11:09:16 ws-rpi3 dhcpcd[1051]: eth0: interface not found or invalid\nAug 26 11:09:16 ws-rpi3 dhcpcd[1051]: exited\nAug 26 11:09:21 ws-rpi3 dhcpcd[1089]: dhcpcd not running\nAug 26 11:09:21 ws-rpi3 kernel: [   43.735165] brcmfmac: power management disabled\nAug 26 11:09:22 ws-rpi3 dhcpcd[1113]: dhcpcd not running\nAug 26 11:09:22 ws-rpi3 kernel: [   43.866789] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 26 11:09:22 ws-rpi3 kernel: [   43.866935] br0: port 1(eth0) entered disabled state\nAug 26 11:09:22 ws-rpi3 kernel: [   43.962974] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 26 11:09:22 ws-rpi3 kernel: [   43.963452] br0: port 1(eth0) entered blocking state\nAug 26 11:09:22 ws-rpi3 kernel: [   43.963460] br0: port 1(eth0) entered forwarding state\nAug 26 11:09:23 ws-rpi3 kernel: [   45.581772] smsc95xx 1-1.1:1.0 eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1\n</code></pre></p> <p>This is the only possible reason I could find. For some reason the interfaces changed to down and up again. I run the ping command from a remote machine and during this period the packages were lost. (When I was fast I could see the SSID on my phone for some seconds before it disappeared.)</p> <p>Here I was facing another big issue. I could not find any errors in the log files or anything else on the RPI side with which the restart could have been triggered. This means that everything seemed fine on the RPI side, but the SSID was not visible. </p> <p>First I thought it is a good idea to restart hostapd daemon when the br0 get the IP address from the DHCP server (BOUND), but it wasn't. :( The dhcp client \"only\" renews the IP address. </p> <p>Only for information I paste here my dhcp related config (<code>/etc/dhcp/dhclient-enter-hooks.d/jvincze_custom</code>):</p> <pre><code>#!/bin/sh\n\ncase \"$reason\" in\n\n    BOUND)\n        echo \"[ $(date +%F\\ %T ) ] - ($0) DHCP REASON: $reason ($new_ip_address , $interface)\" &gt;&gt;/var/log/jvinczedhcp.log\n        PID=$( ps aux|grep hostapd | grep -v grep | awk '{print $2}' )\n        if [ ! -z $PID ] \n        then\n          echo \"[ $(date +%F\\ %T ) ] - KILLING $PID pid\" &gt;&gt;/var/log/jvinczedhcp.log\n          sleep 30\n          kill $PID\n        fi\n        ;;\n    *)\n        echo \"[ $(date +%F\\ %T ) ] - ($0) DHCP REASON: $reason ($new_ip_address , $interface)\" &gt;&gt;/var/log/jvinczedhcp.log\n       ;; \n\nesac\n</code></pre> <p>First I tried this (instead of killing the process):</p> <p><pre><code>#[ ! -z $PID ] &amp;&amp; while kill $PID  2&gt; /dev/null;  do sleep 1 ;  done;\n#sleep 4\n#/usr/sbin/hostapd -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf\n</code></pre> I did not like this, because this file contains the command line parameter of hostapd. It is not a \"real\" problem, but with this way this file has to be synchronized with the .service file.</p> <p>After I slept a bit I found out that I can kill the process if I use <code>Restart=always</code> and <code>Type=forking</code> in the .service file (systemd will restart the process when it's dead/killed/etc.)</p> <p>But nothing changed. :(</p> <p>It could be a solution to modify the script to restart hostapd daemon when ip address is renewed, but in case of long lease time I could lose the wifi connection for hours. (And I did not want to change a network parameter which affects my entire network and devices.)</p> <p>NOTE: After finding my final WA, I did not remove this script to make sure if the interface br0 get a new ip address hostapd will be restarted. (I don't know if it is necessary or not, but think this won't cause further problems.)  If you have more than one interface using dhcp to assign IP address you can write an \"if\" condition to specify the interface to check. Example:</p> <pre><code>if [ ! -z $PID ] || [ \"$interface\" == \"br0\" ] ; then\n</code></pre> <p>3. Another tries</p> <p>Start hostapd from systemd with this command line options: <pre><code>ExecStart=/usr/sbin/hostapd -d -S -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf\n</code></pre></p> <p>After the RPI restarted, I stopped hostapd, backed-up/removed the hostapd.log files, and started the hostapd (again). Then I compared the two log files.</p> <p>You can download my diff file from here: LINK Example .svg file. LINK</p> <p>I tried to find something special to search in google, but all of my searches led to nothing. :( So I finally gave up searching in the log files. It is obvious that this behavior was caused by the interface changes after boot. Syslog: <pre><code>...\nAug 30 21:05:33 ws-rpi3 kernel: [   44.726888] br0: port 2(wlan0) entered disabled state\nAug 30 21:05:33 ws-rpi3 dhcpcd[1020]: dhcpcd not running\nAug 30 21:05:33 ws-rpi3 kernel: [   44.969033] smsc95xx 1-1.1:1.0 eth0: hardware isn't capable of remote wakeup\nAug 30 21:05:33 ws-rpi3 kernel: [   44.969368] br0: port 1(eth0) entered disabled state\n...\n</code></pre></p> <p>Of course I did a lot of search for syslog entries but I could not find any relating, or useful article. </p> <p>4. Final Solution (WA)</p> <p>I tried a lot of things in order to make this work, but the only way is the \"delayed start\". :(</p> <p>I completely disabled the hostapd service and restarted the PI, waited some time (~1min) and started the hostapd daemon. Everything was fine. Now the \"only\" thing was to figuring out how to start hostapd service delayed. </p> <p>My first solution was this (.service file): <pre><code>[Service]\nExecStartPre=/bin/sleep 30\n</code></pre> It is a working but ugly workaround. With this method the whole boot process is delayed. :( And you can see in the boot order (.svg) that the hostapd starting process take +30s compared with the \"normal\" case. :(</p> <p>Finally I set up a systemd timer:</p> <ul> <li>hostapd.timer</li> </ul> cat /lib/systemd/system/hostapd.timer<pre><code>[Unit]\nDescription=Runs hostapd\nAfter=multi-user.target\n\n[Timer]\nOnBootSec=1min\nUnit=hostapd.service\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>I think 1 minute enough delay after boot to start hostapd.  </p> <ul> <li>hostapd.service</li> </ul> <p><pre><code>cat /lib/systemd/system/hostapd.service \n[Unit]\nDescription=HOSTAPD\nRequires=multi-user.target network-online.target avahi-daemon.service smbd.service\nAfter=avahi-daemon.service smbd.service multi-user.target sockets.targee\n\n[Service]\nType=forking\nGuessMainPID=yes  \nExecStart=/usr/sbin/hostapd -d -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf\n#ExecStart=/usr/sbin/hostapd -d -S -t -B -P /run/hostapd.pid -f /var/log/hostapd.log /etc/hostapd/hostapd.conf\n#ExecStart=/usr/sbin/hostapd -t -B  -f /var/log/hostapd.log /etc/hostapd/hostapd.conf\nExecStop=/usr/bin/kill -SIGINT $MAINPID  \nPIDFile=/run/hostapd.pid\nRestart=always\nUser=root\n\n#[Install]\n#WantedBy=multi-user.target\n</code></pre> Commands: Command<pre><code>systemctl daemon-reload\nsystemctl disable hostapd.service\n</code></pre> Output<pre><code>Removed symlink /etc/systemd/system/multi-user.target.wants/hostapd.service.\nsystemctl enable hostapd.timer\nCreated symlink from /etc/systemd/system/multi-user.target.wants/hostapd.timer to /lib/systemd/system/hostapd.timer.\n</code></pre></p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#final-thoughts","title":"Final Thoughts","text":"<p>When I started to write this post I thought configuring hostapd must consist of some easy steps. Actually it is right because configuring hostapd is pretty easy, I had(/have) problems with autostart on boot time. </p> <p>I have been running this setup since weeks without any problem. Maybe later when I have time for this, I'll continue the investigation, or maybe in the future there will be a new kernel and/or hostapd daemon with which this problem won't occur.</p> <p>If you have any idea how to solve this please let me a post on Disqus.  :)</p>"},{"location":"Old_Blog_Contents/raspberry/Raspberry_Pi_3_As_Wifi_Range_Extender/#some-useful-commands","title":"Some Useful Commands","text":"<ul> <li>Check Connected Devices</li> </ul> <pre><code>hostapd_cli all_sta\n</code></pre> <ul> <li>Check Connected Devices (only MAC addresses)</li> </ul> <pre><code>hostapd_cli all_sta | grep  -E '^([0-9|a-f|A-F]{2,2}\\:?){6,6}'\n</code></pre> <ul> <li>Check Connected Devices &amp; Query MAC address on MIKROTIK router</li> </ul> <p>Command<pre><code>for MAC in $(hostapd_cli all_sta | grep  -E '^([0-9|a-f|A-F]{2,2}\\:?){6,6}'); do echo \"########## $MAC ##########\" ; ssh admin@172.16.0.1 \"ip dhcp-server lease print where mac-address=$MAC\" ; done\n</code></pre> Output<pre><code>########## 60:01:94:08:31:48 ##########\nFlags: X - disabled, R - radius, D - dynamic, B - blocked\n # ADDRESS                                      MAC-ADDRESS       HO SER.. RA\n 0   172.20.0.15                                  60:01:94:08:31:48 NO def..\n</code></pre></p> <ul> <li>Check bridge interface Command<pre><code>brctl show\n</code></pre> Output<pre><code>bridge name bridge id       STP enabled interfaces\nbr0     8000.b827eb26993d   no      eth0\n                            wlan0\n</code></pre></li> </ul> <p>NOTE: If the hostapd have been started successfully you have to see WLAN0 in the interface list.</p> <ul> <li>Analyze boot order</li> </ul> <pre><code>systemd-analyze plot &gt;/tmp/plot.svg\n</code></pre> <p>You can open the created file in your browser.</p> <ul> <li>Enable packet forwarding by kernel</li> </ul> <pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward\niptables --append FORWARD --in-interface eth1 -j ACCEPT\n</code></pre> <p>Or permanently, add this line to <code>/etc/sysctl.conf</code> <pre><code>net.ipv4.ip_forward=1\n</code></pre></p> <p>Alternatively:</p> <pre><code>sudo sysctl -w net.ipv4.ip_forward=1\n</code></pre> <ul> <li>Disable interface in dhcpcd.conf</li> </ul> <pre><code>denyinterfaces wlan0\ndenyinterfaces br0\n</code></pre> <ul> <li>Disable ipv6 (sysctl.conf)</li> </ul> <pre><code>net.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\n</code></pre> <ul> <li>Wifi Channel Setup (hostapd.conf)</li> </ul> <p>If you build a WiFi infrastructure you must use different channel in all devices. References:</p> <ul> <li>https://www.extremetech.com/computing/179344-how-to-boost-your-wifi-speed-by-choosing-the-right-channel</li> <li>https://www.hanselman.com/blog/ConfiguringTwoWirelessRoutersWithOneSSIDNetworkNameAtHomeForFreeRoaming.aspx</li> </ul> <p>From hostapd example conf:</p> <pre><code># Channel number (IEEE 802.11)\n# (default: 0, i.e., not set)\n# Please note that some drivers do not use this value from hostapd and the\n# channel will need to be configured separately with iwconfig.\n#\n# If CONFIG_ACS build option is enabled, the channel can be selected\n# automatically at run time by setting channel=acs_survey or channel=0, both of\n# which will enable the ACS survey based algorithm.\nchannel=1\n</code></pre> <ul> <li>Bonus - Connect To Existing WiFi Network</li> </ul> <p>It can be useful when you have two WiFi adapters, and you want to use your raspberry to extend WiFi signal range without ETH connection. I haven't tested this, but I think based on this post and the linked articles it can be done easily.</p> <ul> <li>Install necessary packages <pre><code>apt-get install wpasupplicant wicd wicd-cli\n</code></pre></li> <li>Search for Wifi Networks <pre><code>iwlist wlan1 scan  \n</code></pre></li> <li>Create Config File <pre><code>wpa_passphrase  Wifi-NetWork 12345678 &gt;wifi.config\n</code></pre></li> <li>Test you connection <pre><code>wpa_supplicant -i wlan1 -D nl80211 -c wifi.config  \n</code></pre></li> <li>Edit Interface Config (/etc/network/interfaces) <pre><code>auto wlan1  \nallow-hotplug wlan1  \niface wlan1 inet dhcp  \n    wpa-conf /root/wifi.config\n</code></pre></li> </ul> <p>References:</p> <ul> <li>https://frillip.com/using-your-raspberry-pi-3-as-a-wifi-access-point-with-hostapd/</li> <li>http://www.instructables.com/id/How-to-make-a-WiFi-Access-Point-out-of-a-Raspberry/</li> <li>http://www.catonrug.net/2016/07/use-phone-tablet-as-raspberry-pi-3-wireless-screen-part-2.html</li> <li>https://www.raspberrypi.org/forums/viewtopic.php?t=141807</li> <li>https://wiki.debian.org/BridgeNetworkConnections</li> <li>https://askubuntu.com/questions/617973/fatal-error-netlink-genl-genl-h-no-such-file-or-directory</li> <li>https://unix.stackexchange.com/questions/119209/hostapd-will-not-start-via-service-but-will-start-directly</li> <li>https://superuser.com/questions/676918/hostapd-requires-manual-restart-for-devices-to-connect</li> <li>https://learn.adafruit.com/setting-up-a-raspberry-pi-as-a-wifi-access-point/install-software</li> </ul>"}]}